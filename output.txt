src/
.benchmarks/
.claude/
    agents/
        backend-architect.md
        backend-test-writer.md
    settings.local.json
.coverage
.env.example
.gitignore
.pytest_cache/
    .gitignore
    CACHEDIR.TAG
    README.md
    v/
        cache/
CLAUDE.md
README.md
TESTING_PLAN.md
api-server/
    README.md
    middleware/
        quota.js
    package.json
    routes/
        knowledge.js
    server.js
db/
    .benchmarks/
    README.md
    create.sql
    drop.sql
    init/
        01_enable_pgvector.sql
    manage_db.sh
    migrations/
        001_add_leads_table.sql
        001_add_tool_tables.sql
        002_add_usage_triggers.sql
    seed.sql
docker/
    postgres/
docker-compose.test.yml
docker-compose.yml
docs/
    ARCHITECTURE.md
    AUTO_SCALING_COST_PROTECTION.md
    DEVELOPMENT_WORKFLOW.md
    QUICK_REFERENCE.md
    RAG_IMPLEMENTATION_GUIDE.md
    SCALING_STRATEGY.md
f/
    development/
        1_whatsapp_context_loading.py
        1_whatsapp_context_loading.script.lock
        1_whatsapp_context_loading.script.yaml
        2_whatsapp_llm_processing.py
        2_whatsapp_llm_processing.script.lock
        2_whatsapp_llm_processing.script.yaml
        3_1_send_reply_to_whatsapp.py
        3_1_send_reply_to_whatsapp.script.lock
        3_1_send_reply_to_whatsapp.script.yaml
        4__save_chat_history.py
        4__save_chat_history.script.lock
        4__save_chat_history.script.yaml
        5__log_usage.py
        5__log_usage.script.lock
        5__log_usage.script.yaml
        RAG_process_documents.py
        RAG_process_documents.script.lock
        RAG_process_documents.script.yaml
        __pycache__/
        business_layer_db_postgreSQL.resource.yaml
        folder.meta.yaml
        ingest_multiple_urls.py
        ingest_multiple_urls.script.lock
        ingest_multiple_urls.script.yaml
        upload_document.py
        upload_document.script.lock
        upload_document.script.yaml
        utils/
            __pycache__/
            alert_on_failure.py
            alert_on_failure.script.lock
            alert_on_failure.script.yaml
            api-server/
                middleware/
                routes/
            check_knowledge_quota.py
            check_knowledge_quota.script.lock
            check_knowledge_quota.script.yaml
            web_crawler.py
            web_crawler.script.lock
            web_crawler.script.yaml
        whatsapp_webhook_processor__flow/
            flow.yaml
    webhooks/
        folder.meta.yaml
htmlcov/
    .gitignore
    coverage_html_cb_bcae5fc4.js
    status.json
    style_cb_a5a05ca4.css
mcp-servers/
    contact-owner/
        package.json
        server.js
    lead-capture/
        package.json
        server.js
    pricing-calculator/
        package.json
        server.js
monitoring/
    grafana/
        dashboards/
    prometheus/
output.txt
pytest.ini
rt.d.ts
tests/
    .benchmarks/
    README.md
    __pycache__/
    conftest.py
    fixtures/
        generate_embeddings.py
        sample_llm_responses.json
        sample_webhook_payloads.json
    integration/
        README.md
        __pycache__/
        test_database_operations.py
        test_full_flow.py
        test_rag_api_endpoints.py
    live/
        __pycache__/
        conftest.py
        test_live_llm.py
    manual_flow_test.sh
    requirements.txt
    test_harness/
        __pycache__/
        llm_mock.py
        whatsapp_mock.py
        windmill_mock.py
    unit/
        __pycache__/
        test_message_validation.py
        test_quota_enforcement.py
        test_rate_limiting.py
        test_step1_context_loading.py
        test_step2_error_handling.py
        test_step2_gemini_tool_calling.py
        test_step2_llm_processing.py
        test_step3_1_send_reply.py
        test_step4__save_history.py
        test_step5__usage_logging.py
        test_web_crawler.py
utils/
    curl_windmill_endpoint.sh
webhook-server/
    app.js
    package.json
    rateLimiter.js
wmill-lock.yaml
wmill.yaml

================================================
File: .pytest_cache/.gitignore
================================================
# Created by pytest automatically.
*


================================================
File: .pytest_cache/README.md
================================================
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


================================================
File: TESTING_PLAN.md
================================================
# Comprehensive Testing Plan

## Executive Summary

This plan outlines a systematic approach to achieve comprehensive test coverage for the WhatsApp chatbot backend while adhering to pytest best practices. Current coverage: ~35%. Target: >80% for critical paths.

**Key Principles:**
1. **Simplicity** - Minimize test code, maximize clarity
2. **Real Implementation** - Test actual backend code, no custom test implementations
3. **Pytest Best Practices** - Fixtures, importlib mode, strict config, deterministic tests
4. **Fast Feedback** - Unit tests run in <1s, integration tests in <5s

---

## 1. Test Organization Structure

### Current Structure (Already Good)
```
tests/
├── conftest.py              # Shared fixtures ✓
├── unit/                    # Fast, isolated tests ✓
│   ├── test_step1_context_loading.py
│   ├── test_step2_*.py
│   └── test_quota_enforcement.py
├── integration/             # Multi-component tests ✓
│   ├── test_rag_api_endpoints.py
│   ├── test_database_operations.py
│   └── test_full_flow.py
└── test_harness/            # Mock utilities ✓
    ├── windmill_mock.py
    ├── llm_mock.py
    └── whatsapp_mock.py
```

### Import Mode (Already Configured)
- Using `importlib` mode via direct imports
- Scripts import using `importlib.util.spec_from_file_location`
- No sys.path manipulation in tests (only in conftest.py setup)

### Markers (Already Configured in pytest.ini)
```ini
-m unit      # Fast, no DB, mocked externals
-m integration  # Real DB, minimal mocking
-m slow      # >5s execution
-m db        # Requires database
```

---

## 2. Fixture Strategy

### Core Fixtures (Already in conftest.py)

#### Database Fixtures
```python
# Use db_with_data (default) - Auto rollback, fast, isolated
def test_message_creation(db_with_data):
    # Changes rollback automatically

# Use db_with_autocommit - For testing external scripts
def test_windmill_script(db_with_autocommit):
    # Script sees committed data
    # clean_db resets between tests
```

#### Mock Fixtures
```python
# Use mock_all_external - For integration tests
def test_flow(mock_all_external, db_with_data):
    # All external APIs mocked
    # Database real
```

### New Fixtures Needed

```python
# 1. LLM Response Fixtures (add to conftest.py)
@pytest.fixture
def gemini_simple_response():
    """Simple text response from Gemini (no tools)"""
    return {
        "reply_text": "Hello! How can I help?",
        "updated_variables": {},
        "usage_info": {
            "provider": "google",
            "model": "gemini-pro",
            "tokens_input": 50,
            "tokens_output": 20
        }
    }

@pytest.fixture
def gemini_tool_call_response():
    """Gemini response with tool call"""
    return {
        "tool_calls": [
            {"name": "get_weather", "args": {"city": "NYC"}}
        ],
        "usage_info": {...}
    }

# 2. Message History Fixture
@pytest.fixture
def conversation_history(db_with_data, sample_context_payload):
    """Pre-populate conversation history"""
    contact_id = sample_context_payload["user"]["id"]
    db_with_data.execute("""
        INSERT INTO messages (contact_id, direction, content, created_at)
        VALUES
            (%s, 'incoming', 'Hello', NOW() - INTERVAL '2 minutes'),
            (%s, 'outgoing', 'Hi there!', NOW() - INTERVAL '1 minute')
    """, (contact_id, contact_id))
    return contact_id

# 3. Vector Embedding Fixture (add to conftest.py)
@pytest.fixture
def openai_embedding_1536():
    """Valid 1536-dimensional embedding for testing"""
    return "[" + ", ".join(["0.1"] * 1536) + "]"
```

---

## 3. Priority Order

### Phase 1: Critical Path (Week 1)
**Goal:** Test the main WhatsApp message flow end-to-end

1. **Fix Step 2 Import Errors** (30 min)
   - test_step2_error_handling.py
   - test_step2_gemini_tool_calling.py

2. **Complete Step 1 Tests** (2 hours)
   - User data loading
   - Conversation history retrieval
   - Usage limit checks
   - Error scenarios (chatbot not found, quota exceeded)

3. **Implement Step 2 Tests** (4 hours)
   - LLM interaction (simple response)
   - Tool calling (mock tool execution)
   - Variable updates
   - Response generation

4. **Implement Step 3.2 Tests** (2 hours)
   - Message persistence
   - Conversation threading
   - History constraints (max 50 messages)

5. **Implement Step 3.3 Tests** (2 hours)
   - Usage tracking
   - Token counting
   - Cost calculation
   - Quota updates

### Phase 2: RAG System (Week 2)
**Goal:** Test knowledge base functionality

6. **Database Operations Tests** (3 hours)
   - CRUD operations
   - Triggers (knowledge_source_counter_trigger)
   - Constraints
   - Vector operations

7. **RAG API Tests** (2 hours)
   - POST /upload endpoint
   - POST /url endpoint
   - PUT endpoints
   - Error handling

8. **RAG Processing Tests** (4 hours)
   - upload_document.py
   - ingest_multiple_urls.py
   - RAG_process_documents.py

### Phase 3: Robustness (Week 3)
**Goal:** Test edge cases and failure modes

9. **Rate Limiting Tests** (2 hours)
   - Redis connection failures
   - Concurrent access
   - Edge cases

10. **Security Tests** (3 hours)
    - SQL injection attempts
    - Input validation
    - Authentication bypass attempts

11. **Performance Tests** (2 hours)
    - Vector search with 10k+ embeddings
    - Concurrent quota updates
    - Webhook event cleanup

12. **Full Flow Integration Test** (2 hours)
    - Webhook → Step 1 → Step 2 → Step 3
    - Error recovery
    - Idempotency

---

## 4. Simplification Strategies

### Strategy 1: Import Real Code, Don't Duplicate

❌ **Bad - Custom Test Implementation:**
```python
def test_quota_check():
    # Custom quota logic in test
    max_pdfs = 100
    current_pdfs = 50
    if current_pdfs < max_pdfs:
        assert True
```

✅ **Good - Import Real Implementation:**
```python
from f.development.utils.check_knowledge_quota import main as check_quota

def test_quota_check(mock_wmill_resource, db_with_autocommit):
    # Use real implementation
    with patch('wmill.get_resource', return_value=mock_wmill_resource):
        result = check_quota(
            organization_id="org-123",
            file_size_mb=10.0
        )
    assert result["allowed"] is True
```

### Strategy 2: Use Fixtures for Test Data

❌ **Bad - Inline Data Setup:**
```python
def test_message_creation(db_cursor):
    # Lots of setup code
    db_cursor.execute("INSERT INTO organizations ...")
    db_cursor.execute("INSERT INTO chatbots ...")
    db_cursor.execute("INSERT INTO contacts ...")
    # Actual test buried in setup
```

✅ **Good - Fixture-Based Setup:**
```python
def test_message_creation(db_with_data, sample_context_payload):
    # Clean test - setup in fixtures
    contact_id = sample_context_payload["user"]["id"]
    # Test logic only
```

### Strategy 3: Parametrize Instead of Duplicate

❌ **Bad - Duplicate Tests:**
```python
def test_quota_pdf_exceeded():
    # Test with PDFs

def test_quota_url_exceeded():
    # Test with URLs (same logic)

def test_quota_storage_exceeded():
    # Test with storage (same logic)
```

✅ **Good - Parametrized Test:**
```python
@pytest.mark.parametrize("quota_type,expected_message", [
    ("max_pdfs", "PDF upload limit reached"),
    ("max_urls", "URL ingestion limit reached"),
    ("max_storage", "Storage quota exceeded"),
])
def test_quota_exceeded(quota_type, expected_message, ...):
    # Single test, multiple scenarios
```

### Strategy 4: Use Helper Methods in conftest.py

❌ **Bad - Repeated Query Code:**
```python
def test_a(db_cursor):
    db_cursor.execute("SELECT * FROM chatbots WHERE id = %s", (id,))
    chatbot = db_cursor.fetchone()

def test_b(db_cursor):
    db_cursor.execute("SELECT * FROM chatbots WHERE id = %s", (id,))
    chatbot = db_cursor.fetchone()
```

✅ **Good - Use query_helper Fixture:**
```python
def test_a(query_helper):
    chatbot = query_helper.get_chatbot(chatbot_id)

def test_b(query_helper):
    chatbot = query_helper.get_chatbot(chatbot_id)
```

---

## 5. Backend Code Reuse

### Pattern 1: Import Windmill Scripts Directly

```python
# Import the actual script
import importlib.util
spec = importlib.util.spec_from_file_location(
    "step1",
    "f/development/1_whatsapp_context_loading.py"
)
step1_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(step1_module)

# Test the real function
def test_step1_logic(mock_wmill):
    result = step1_module.main(
        whatsapp_phone_id="123",
        user_phone="16315551234",
        message_id="wamid.123",
        user_name="Test User"
    )
    assert result["proceed"] is True
```

### Pattern 2: Test Utilities Directly

```python
# Import real utility
from f.development.utils.check_knowledge_quota import main as check_quota

def test_quota_enforcement():
    # Test real implementation
    result = check_quota(org_id="...", file_size_mb=50.0)
```

### Pattern 3: Test Database Schema/Triggers

```python
def test_knowledge_counter_trigger(db_with_data):
    """Test the real database trigger increments counters"""
    # Insert knowledge source
    db_with_data.execute("""
        INSERT INTO knowledge_sources (chatbot_id, source_type, name)
        VALUES (%s, 'pdf', 'test.pdf')
    """, (chatbot_id,))

    # Verify trigger updated counters
    db_with_data.execute("""
        SELECT pdf_count FROM chatbots WHERE id = %s
    """, (chatbot_id,))
    assert db_with_data.fetchone()["pdf_count"] == 1
```

---

## 6. Flaky Test Prevention

### Rule 1: No Global State
```python
# ❌ Bad - Global state
cache = {}

def test_with_cache():
    cache["key"] = "value"  # Affects other tests

# ✅ Good - Isolated state
def test_with_cache(monkeypatch):
    cache = {}
    monkeypatch.setattr("module.cache", cache)
```

### Rule 2: Fixed Timestamps
```python
# ❌ Bad - Time-dependent
def test_expiry():
    created_at = datetime.now()
    # Flaky if test runs slow

# ✅ Good - Fixed time
from freezegun import freeze_time

@freeze_time("2025-01-15 10:00:00")
def test_expiry():
    # Deterministic
```

### Rule 3: Use pytest.approx for Floats
```python
# ❌ Bad - Exact float comparison
assert cost == 0.001

# ✅ Good - Approximate comparison
assert cost == pytest.approx(0.001, rel=1e-6)
```

### Rule 4: Proper Cleanup
```python
# ✅ Use fixtures with yield for cleanup
@pytest.fixture
def temp_file():
    f = open("test.txt", "w")
    yield f
    f.close()
    os.remove("test.txt")
```

### Rule 5: Deterministic Test Order
```python
# Don't rely on test execution order
# Each test should be independent
```

---

## 7. Implementation Templates

### Template 1: Unit Test for Step 1 (Context Loading)

```python
@patch('psycopg2.connect')
def test_user_data_loading(mock_connect):
    """Test Step 1 loads user data correctly"""
    # Setup mock database
    mock_conn = MagicMock()
    mock_cursor = MagicMock()
    mock_connect.return_value = mock_conn
    mock_conn.cursor.return_value = mock_cursor

    # Mock responses: no duplicate, webhook created, chatbot found, user found
    mock_cursor.fetchone.side_effect = [
        None,  # No duplicate
        {"id": 1},  # Webhook event
        {"id": "chatbot-123", "name": "Test Bot", ...},  # Chatbot
        {"id": "user-456", "name": "John", "phone": "1234567890"}  # User
    ]

    # Call real function
    result = step1_main(
        whatsapp_phone_id="phone-123",
        user_phone="1234567890",
        message_id="msg-789",
        user_name="John"
    )

    # Verify
    assert result["proceed"] is True
    assert result["user"]["id"] == "user-456"
    assert result["user"]["name"] == "John"
```

### Template 2: Integration Test for Step 2 (LLM Processing)

```python
def test_step2_simple_response(mock_all_external, db_with_data, sample_context_payload):
    """Test Step 2 generates simple LLM response"""
    # Configure LLM mock
    mock_all_external["llm"].set_response({
        "reply_text": "Hello! How can I help?",
        "updated_variables": {},
        "usage_info": {
            "provider": "google",
            "model": "gemini-pro",
            "tokens_input": 50,
            "tokens_output": 20
        }
    })

    # Import and call real Step 2
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "step2",
        "f/development/2_whatsapp_llm_processing.py"
    )
    step2 = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(step2)

    result = step2.main(
        context=sample_context_payload,
        user_message="Hello"
    )

    # Verify
    assert result["reply_text"] == "Hello! How can I help?"
    assert result["usage_info"]["tokens_input"] == 50
```

### Template 3: Database Test for Triggers

```python
def test_knowledge_source_counter_increment(db_with_data):
    """Test trigger increments pdf_count when PDF added"""
    # Setup
    chatbot_id = "22222222-2222-2222-2222-222222222222"

    # Get initial count
    db_with_data.execute(
        "SELECT pdf_count FROM chatbots WHERE id = %s",
        (chatbot_id,)
    )
    initial_count = db_with_data.fetchone()["pdf_count"]

    # Add PDF source
    db_with_data.execute("""
        INSERT INTO knowledge_sources (chatbot_id, source_type, name)
        VALUES (%s, 'pdf', 'test.pdf')
    """, (chatbot_id,))

    # Verify trigger updated count
    db_with_data.execute(
        "SELECT pdf_count FROM chatbots WHERE id = %s",
        (chatbot_id,)
    )
    new_count = db_with_data.fetchone()["pdf_count"]
    assert new_count == initial_count + 1
```

### Template 4: API Endpoint Test

```python
def test_upload_pdf_endpoint(client, db_with_data, mock_quota_check):
    """Test POST /api/knowledge/upload endpoint"""
    # Setup
    mock_quota_check.return_value = {"allowed": True}

    # Prepare file upload
    files = {
        "file": ("test.pdf", b"PDF content", "application/pdf")
    }
    data = {
        "chatbot_id": "chatbot-123",
        "name": "Test Document"
    }

    # Call endpoint
    response = client.post(
        "/api/knowledge/upload",
        data=data,
        files=files
    )

    # Verify response
    assert response.status_code == 200
    assert response.json()["status"] == "uploaded"

    # Verify database
    db_with_data.execute("""
        SELECT * FROM knowledge_sources
        WHERE chatbot_id = %s AND name = %s
    """, ("chatbot-123", "Test Document"))
    source = db_with_data.fetchone()
    assert source is not None
    assert source["source_type"] == "pdf"
```

### Template 5: Parametrized Test

```python
@pytest.mark.parametrize("source_type,expected_count_field", [
    ("pdf", "pdf_count"),
    ("url", "url_count"),
])
def test_source_counter_by_type(db_with_data, source_type, expected_count_field):
    """Test counters update correctly for different source types"""
    chatbot_id = "22222222-2222-2222-2222-222222222222"

    # Add source
    db_with_data.execute(f"""
        INSERT INTO knowledge_sources (chatbot_id, source_type, name)
        VALUES (%s, %s, 'test')
    """, (chatbot_id, source_type))

    # Verify count
    db_with_data.execute(
        f"SELECT {expected_count_field} FROM chatbots WHERE id = %s",
        (chatbot_id,)
    )
    count = db_with_data.fetchone()[expected_count_field]
    assert count > 0
```

---

## 8. Testing Checklist

Before marking a test complete, verify:

- [ ] Test is in correct directory (unit/ vs integration/)
- [ ] Test uses appropriate fixture (db_with_data vs db_with_autocommit)
- [ ] Test has clear docstring explaining what it tests
- [ ] Test imports real implementation (no duplicate logic)
- [ ] Test is deterministic (no time/randomness dependencies)
- [ ] Test cleans up after itself (or uses fixtures that do)
- [ ] Test is parametrized if testing multiple similar cases
- [ ] Test assertions are clear and specific
- [ ] Test runs in <1s (unit) or <5s (integration)
- [ ] Test has appropriate markers (@pytest.mark.unit, @pytest.mark.db, etc.)

---

## 9. Success Metrics

### Coverage Targets
- **Critical Path (Step 1-3):** >90% line coverage
- **RAG System:** >80% line coverage
- **Utilities:** >85% line coverage
- **API Endpoints:** 100% endpoint coverage

### Performance Targets
- **Unit Tests:** <1s per test, <30s total suite
- **Integration Tests:** <5s per test, <2min total suite
- **Full Suite:** <3min total

### Quality Targets
- **Zero flaky tests:** All tests pass 100/100 runs
- **Zero skipped tests:** All tests enabled and passing
- **Clear failures:** Test failures point to exact issue

---

## 10. Next Steps

**Immediate (Today):**
1. Fix Step 2 import errors (test_step2_error_handling.py, test_step2_gemini_tool_calling.py)
2. Add new fixtures to conftest.py (gemini_simple_response, conversation_history, openai_embedding_1536)

**This Week:**
3. Complete Step 1 tests (user data, history, quota checks, errors)
4. Implement Step 2 tests (LLM interaction, tool calling, variables)
5. Implement Step 3.2 tests (message persistence)
6. Implement Step 3.3 tests (usage logging)

**Next Week:**
7. Database operations tests
8. RAG API tests
9. RAG processing tests

**Week 3:**
10. Rate limiting, security, performance tests
11. Full flow integration test
12. Achieve >80% coverage target


================================================
File: tests/unit/test_step2_error_handling.py
================================================
"""
Unit tests for Step 2: Error handling and edge cases

Tests Step 2's ability to handle error responses from Step 1.
"""

import pytest
import sys
import os
from unittest.mock import Mock, patch

# Add parent directories to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../f/development'))

# Mock required modules before importing step2
mock_wmill = Mock()
mock_wmill.get_variable.return_value = "fake_google_api_key"
sys.modules['wmill'] = mock_wmill

# Mock Google GenAI SDK
mock_genai = Mock()
mock_genai_types = Mock()
sys.modules['google.genai'] = mock_genai
sys.modules['google.genai.types'] = mock_genai_types

# Import the module under test
# Note: Can't import directly because filename starts with number
# Import using importlib instead
import importlib.util
spec = importlib.util.spec_from_file_location(
    "step2",
    os.path.join(os.path.dirname(__file__), '../../f/development/2_whatsapp_llm_processing.py')
)
step2_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(step2_module)
step2_main = step2_module.main


class TestStep2ErrorHandling:
    """Test Step 2's error handling"""

    def test_step1_failed_chatbot_not_found(self):
        """Test Step 2 handles 'Chatbot not found' error from Step 1"""
        # Simulate Step 1 error response
        context_payload = {
            "proceed": False,
            "reason": "Chatbot not found",
            "notify_admin": True
        }

        result = step2_main(
            context_payload=context_payload,
            user_message="Hello",
            openai_api_key="",
            google_api_key="fake_key",
            default_provider="google"
        )

        # Assertions
        assert "error" in result
        assert result["error"] == "Chatbot not found"
        assert "reply_text" in result
        assert "unable to process" in result["reply_text"].lower()
        assert result["should_notify_admin"] is True

    def test_step1_failed_quota_exceeded(self):
        """Test Step 2 handles quota exceeded error"""
        context_payload = {
            "proceed": False,
            "reason": "Usage quota exceeded",
            "notify_admin": False
        }

        result = step2_main(
            context_payload=context_payload,
            user_message="Hello",
            google_api_key="fake_key"
        )

        assert result["error"] == "Usage quota exceeded"
        assert result["should_notify_admin"] is False

    def test_step1_failed_duplicate_message(self):
        """Test Step 2 handles duplicate message error"""
        context_payload = {
            "proceed": False,
            "reason": "Duplicate - Already Processed"
        }

        result = step2_main(
            context_payload=context_payload,
            user_message="Hello",
            google_api_key="fake_key"
        )

        assert result["error"] == "Duplicate - Already Processed"
        assert "reply_text" in result

    def test_step1_success_no_proceed_key(self):
        """Test handling when proceed key is missing (defaults to False)"""
        context_payload = {
            # Missing "proceed" key
            "chatbot": {"id": "123"},
            "user": {},
            "history": [],
            "tools": []
        }

        result = step2_main(
            context_payload=context_payload,
            user_message="Hello",
            google_api_key="fake_key"
        )

        # Should treat as failure since proceed defaults to False
        assert "error" in result

    def test_step1_proceed_false_explicit(self):
        """Test explicit proceed=False is handled"""
        context_payload = {
            "proceed": False,  # Explicit False
            "reason": "Chatbot is disabled"
        }

        result = step2_main(
            context_payload=context_payload,
            user_message="Hello",
            google_api_key="fake_key"
        )

        assert result["error"] == "Chatbot is disabled"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
File: db/drop.sql
================================================
/* 
====================================================================
  DROP ALL TABLES - CLEAN SLATE
  Run this to completely reset the database
====================================================================
*/

-- Drop tables in reverse dependency order
DROP TABLE IF EXISTS usage_summary CASCADE;
DROP TABLE IF EXISTS usage_logs CASCADE;
DROP TABLE IF EXISTS tool_executions CASCADE;
DROP TABLE IF EXISTS system_alerts CASCADE;
DROP TABLE IF EXISTS leads CASCADE;
DROP TABLE IF EXISTS webhook_events CASCADE;
DROP TABLE IF EXISTS messages CASCADE;
DROP TABLE IF EXISTS contacts CASCADE;
DROP TABLE IF EXISTS document_chunks CASCADE;
DROP TABLE IF EXISTS knowledge_sources CASCADE;
DROP TABLE IF EXISTS daily_ingestion_counts CASCADE;
DROP TABLE IF EXISTS chatbot_integrations CASCADE;
DROP TABLE IF EXISTS chatbots CASCADE;
DROP TABLE IF EXISTS org_integrations CASCADE;
DROP TABLE IF EXISTS users CASCADE;
DROP TABLE IF EXISTS organizations CASCADE;

-- Drop functions
DROP FUNCTION IF EXISTS increment_knowledge_counters();
DROP FUNCTION IF EXISTS get_current_usage(UUID);
DROP FUNCTION IF EXISTS cleanup_old_webhook_events();
DROP FUNCTION IF EXISTS update_updated_at_column();

-- Drop extensions (optional - comment out if shared database)
-- DROP EXTENSION IF EXISTS "pgcrypto";

================================================
File: f/development/utils/check_knowledge_quota.script.yaml
================================================
summary: ''
description: ''
lock: '!inline f/development/utils/check_knowledge_quota.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    chatbot_id:
      type: string
      description: ''
      default: null
      originalType: string
    db_resource:
      type: string
      description: ''
      default: f/development/business_layer_db_postgreSQL
      originalType: string
    file_size_mb:
      type: number
      description: ''
      default: 0
    source_type:
      type: string
      description: ''
      default: null
      originalType: string
  required:
    - chatbot_id
    - source_type


================================================
File: htmlcov/status.json
================================================
{"note":"This file is an internal implementation detail to speed up HTML report generation. Its format can change at any time. You might be looking for the JSON report: https://coverage.rtfd.io/cmd.html#cmd-json","format":5,"version":"7.13.0","globals":"2791024ebc0e78e30f546528793fbe95","files":{"z_7418310bdd5450e9_1_whatsapp_context_loading_py":{"hash":"cd24039c4a0ac41d964573e971c58430","index":{"url":"z_7418310bdd5450e9_1_whatsapp_context_loading_py.html","file":"f/development/1_whatsapp_context_loading.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":111,"n_excluded":0,"n_missing":8,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_7418310bdd5450e9_2_whatsapp_llm_processing_py":{"hash":"fc57b050accf07018ec418425ef24db4","index":{"url":"z_7418310bdd5450e9_2_whatsapp_llm_processing_py.html","file":"f/development/2_whatsapp_llm_processing.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":319,"n_excluded":0,"n_missing":0,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_7418310bdd5450e9_3_1_send_reply_to_whatsapp_py":{"hash":"e830e3d5d923511e1837dadee77192c9","index":{"url":"z_7418310bdd5450e9_3_1_send_reply_to_whatsapp_py.html","file":"f/development/3_1_send_reply_to_whatsapp.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":33,"n_excluded":0,"n_missing":2,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_7418310bdd5450e9_4__save_chat_history_py":{"hash":"d72a2336650851ea252f9d73f95a76be","index":{"url":"z_7418310bdd5450e9_4__save_chat_history_py.html","file":"f/development/4__save_chat_history.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":39,"n_excluded":0,"n_missing":0,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_7418310bdd5450e9_5__log_usage_py":{"hash":"04636db1124961441dd06cb4b4fae13e","index":{"url":"z_7418310bdd5450e9_5__log_usage_py.html","file":"f/development/5__log_usage.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":65,"n_excluded":0,"n_missing":1,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_7418310bdd5450e9_RAG_process_documents_py":{"hash":"e1c27047e4d68b337a0aa9153b17af74","index":{"url":"z_7418310bdd5450e9_RAG_process_documents_py.html","file":"f/development/RAG_process_documents.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":152,"n_excluded":0,"n_missing":152,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_7418310bdd5450e9_ingest_multiple_urls_py":{"hash":"df38749b8cdcffc2458ce23f059b40c8","index":{"url":"z_7418310bdd5450e9_ingest_multiple_urls_py.html","file":"f/development/ingest_multiple_urls.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":34,"n_excluded":0,"n_missing":34,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_7418310bdd5450e9_upload_document_py":{"hash":"4068f32ce37323df5daa31e160611d4c","index":{"url":"z_7418310bdd5450e9_upload_document_py.html","file":"f/development/upload_document.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":71,"n_excluded":0,"n_missing":71,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_90401c14febc14f2_check_knowledge_quota_py":{"hash":"007043003d377837e36f33531c41f6ab","index":{"url":"z_90401c14febc14f2_check_knowledge_quota_py.html","file":"f/development/utils/check_knowledge_quota.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":31,"n_excluded":0,"n_missing":0,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_90401c14febc14f2_web_crawler_py":{"hash":"befb77dd0e460fa3239431ea4e426e77","index":{"url":"z_90401c14febc14f2_web_crawler_py.html","file":"f/development/utils/web_crawler.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":99,"n_excluded":1,"n_missing":12,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}}}}

================================================
File: htmlcov/style_cb_a5a05ca4.css
================================================
@charset "UTF-8";
/* Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0 */
/* For details: https://github.com/coveragepy/coveragepy/blob/main/NOTICE.txt */
/* Don't edit this .css file. Edit the .scss file instead! */
html, body, h1, h2, h3, p, table, td, th { margin: 0; padding: 0; border: 0; font-weight: inherit; font-style: inherit; font-size: 100%; font-family: inherit; vertical-align: baseline; }

body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Ubuntu, Cantarell, "Helvetica Neue", sans-serif; font-size: 1em; background: #fff; color: #000; }

@media (prefers-color-scheme: dark) { body { background: #1e1e1e; } }

@media (prefers-color-scheme: dark) { body { color: #eee; } }

html > body { font-size: 16px; }

a:active, a:focus { outline: 2px dashed #007acc; }

p { font-size: .875em; line-height: 1.4em; }

table { border-collapse: collapse; }

td { vertical-align: top; }

table tr.hidden { display: none !important; }

p#no_rows { display: none; font-size: 1.15em; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Ubuntu, Cantarell, "Helvetica Neue", sans-serif; }

a.nav { text-decoration: none; color: inherit; }

a.nav:hover { text-decoration: underline; color: inherit; }

.hidden { display: none; }

header { background: #f8f8f8; width: 100%; z-index: 2; border-bottom: 1px solid #ccc; }

@media (prefers-color-scheme: dark) { header { background: black; } }

@media (prefers-color-scheme: dark) { header { border-color: #333; } }

header .content { padding: 1rem 3.5rem; }

header h2 { margin-top: .5em; font-size: 1em; }

header h2 a.button { font-family: inherit; font-size: inherit; border: 1px solid; border-radius: .2em; background: #eee; color: inherit; text-decoration: none; padding: .1em .5em; margin: 1px calc(.1em + 1px); cursor: pointer; border-color: #ccc; }

@media (prefers-color-scheme: dark) { header h2 a.button { background: #333; } }

@media (prefers-color-scheme: dark) { header h2 a.button { border-color: #444; } }

header h2 a.button.current { border: 2px solid; background: #fff; border-color: #999; cursor: default; }

@media (prefers-color-scheme: dark) { header h2 a.button.current { background: #1e1e1e; } }

@media (prefers-color-scheme: dark) { header h2 a.button.current { border-color: #777; } }

header p.text { margin: .5em 0 -.5em; color: #666; font-style: italic; }

@media (prefers-color-scheme: dark) { header p.text { color: #aaa; } }

header.sticky { position: fixed; left: 0; right: 0; height: 2.5em; }

header.sticky .text { display: none; }

header.sticky h1, header.sticky h2 { font-size: 1em; margin-top: 0; display: inline-block; }

header.sticky .content { padding: 0.5rem 3.5rem; }

header.sticky .content p { font-size: 1em; }

header.sticky ~ #source { padding-top: 6.5em; }

main { position: relative; z-index: 1; }

footer { margin: 1rem 3.5rem; }

footer .content { padding: 0; color: #666; font-style: italic; }

@media (prefers-color-scheme: dark) { footer .content { color: #aaa; } }

#index { margin: 1rem 0 0 3.5rem; }

h1 { font-size: 1.25em; display: inline-block; }

#filter_container { float: right; margin: 0 2em 0 0; line-height: 1.66em; }

#filter_container #filter { width: 10em; padding: 0.2em 0.5em; border: 2px solid #ccc; background: #fff; color: #000; }

@media (prefers-color-scheme: dark) { #filter_container #filter { border-color: #444; } }

@media (prefers-color-scheme: dark) { #filter_container #filter { background: #1e1e1e; } }

@media (prefers-color-scheme: dark) { #filter_container #filter { color: #eee; } }

#filter_container #filter:focus { border-color: #007acc; }

#filter_container :disabled ~ label { color: #ccc; }

@media (prefers-color-scheme: dark) { #filter_container :disabled ~ label { color: #444; } }

#filter_container label { font-size: .875em; color: #666; }

@media (prefers-color-scheme: dark) { #filter_container label { color: #aaa; } }

header button { font-family: inherit; font-size: inherit; border: 1px solid; border-radius: .2em; background: #eee; color: inherit; text-decoration: none; padding: .1em .5em; margin: 1px calc(.1em + 1px); cursor: pointer; border-color: #ccc; }

@media (prefers-color-scheme: dark) { header button { background: #333; } }

@media (prefers-color-scheme: dark) { header button { border-color: #444; } }

header button:active, header button:focus { outline: 2px dashed #007acc; }

header button.run { background: #eeffee; }

@media (prefers-color-scheme: dark) { header button.run { background: #373d29; } }

header button.run.show_run { background: #dfd; border: 2px solid #00dd00; margin: 0 .1em; }

@media (prefers-color-scheme: dark) { header button.run.show_run { background: #373d29; } }

header button.mis { background: #ffeeee; }

@media (prefers-color-scheme: dark) { header button.mis { background: #4b1818; } }

header button.mis.show_mis { background: #fdd; border: 2px solid #ff0000; margin: 0 .1em; }

@media (prefers-color-scheme: dark) { header button.mis.show_mis { background: #4b1818; } }

header button.exc { background: #f7f7f7; }

@media (prefers-color-scheme: dark) { header button.exc { background: #333; } }

header button.exc.show_exc { background: #eee; border: 2px solid #808080; margin: 0 .1em; }

@media (prefers-color-scheme: dark) { header button.exc.show_exc { background: #333; } }

header button.par { background: #ffffd5; }

@media (prefers-color-scheme: dark) { header button.par { background: #650; } }

header button.par.show_par { background: #ffa; border: 2px solid #bbbb00; margin: 0 .1em; }

@media (prefers-color-scheme: dark) { header button.par.show_par { background: #650; } }

#help_panel, #source p .annotate.long { display: none; position: absolute; z-index: 999; background: #ffffcc; border: 1px solid #888; border-radius: .2em; color: #333; padding: .25em .5em; }

#source p .annotate.long { white-space: normal; float: right; top: 1.75em; right: 1em; height: auto; }

#help_panel_wrapper { float: right; position: relative; }

#keyboard_icon { margin: 5px; }

#help_panel_state { display: none; }

#help_panel { top: 25px; right: 0; padding: .75em; border: 1px solid #883; color: #333; }

#help_panel .keyhelp p { margin-top: .75em; }

#help_panel .legend { font-style: italic; margin-bottom: 1em; }

.indexfile #help_panel { width: 25em; }

.pyfile #help_panel { width: 18em; }

#help_panel_state:checked ~ #help_panel { display: block; }

kbd { border: 1px solid black; border-color: #888 #333 #333 #888; padding: .1em .35em; font-family: SFMono-Regular, Menlo, Monaco, Consolas, monospace; font-weight: bold; background: #eee; border-radius: 3px; }

#source { padding: 1em 0 1em 3.5rem; font-family: SFMono-Regular, Menlo, Monaco, Consolas, monospace; }

#source p { position: relative; white-space: pre; }

#source p * { box-sizing: border-box; }

#source p .n { float: left; text-align: right; width: 3.5rem; box-sizing: border-box; margin-left: -3.5rem; padding-right: 1em; color: #999; user-select: none; }

@media (prefers-color-scheme: dark) { #source p .n { color: #777; } }

#source p .n.highlight { background: #ffdd00; }

#source p .n a { scroll-margin-top: 6em; text-decoration: none; color: #999; }

@media (prefers-color-scheme: dark) { #source p .n a { color: #777; } }

#source p .n a:hover { text-decoration: underline; color: #999; }

@media (prefers-color-scheme: dark) { #source p .n a:hover { color: #777; } }

#source p .t { display: inline-block; width: 100%; box-sizing: border-box; margin-left: -.5em; padding-left: 0.3em; border-left: 0.2em solid #fff; }

@media (prefers-color-scheme: dark) { #source p .t { border-color: #1e1e1e; } }

#source p .t:hover { background: #f2f2f2; }

@media (prefers-color-scheme: dark) { #source p .t:hover { background: #282828; } }

#source p .t:hover ~ .r .annotate.long { display: block; }

#source p .t .com { color: #008000; font-style: italic; line-height: 1px; }

@media (prefers-color-scheme: dark) { #source p .t .com { color: #6a9955; } }

#source p .t .key { font-weight: bold; line-height: 1px; }

#source p .t .str, #source p .t .fst { color: #0451a5; }

@media (prefers-color-scheme: dark) { #source p .t .str, #source p .t .fst { color: #9cdcfe; } }

#source p.mis .t { border-left: 0.2em solid #ff0000; }

#source p.mis.show_mis .t { background: #fdd; }

@media (prefers-color-scheme: dark) { #source p.mis.show_mis .t { background: #4b1818; } }

#source p.mis.show_mis .t:hover { background: #f2d2d2; }

@media (prefers-color-scheme: dark) { #source p.mis.show_mis .t:hover { background: #532323; } }

#source p.mis.mis2 .t { border-left: 0.2em dotted #ff0000; }

#source p.mis.mis2.show_mis .t { background: #ffeeee; }

@media (prefers-color-scheme: dark) { #source p.mis.mis2.show_mis .t { background: #351b1b; } }

#source p.mis.mis2.show_mis .t:hover { background: #f2d2d2; }

@media (prefers-color-scheme: dark) { #source p.mis.mis2.show_mis .t:hover { background: #532323; } }

#source p.run .t { border-left: 0.2em solid #00dd00; }

#source p.run.show_run .t { background: #dfd; }

@media (prefers-color-scheme: dark) { #source p.run.show_run .t { background: #373d29; } }

#source p.run.show_run .t:hover { background: #d2f2d2; }

@media (prefers-color-scheme: dark) { #source p.run.show_run .t:hover { background: #404633; } }

#source p.run.run2 .t { border-left: 0.2em dotted #00dd00; }

#source p.run.run2.show_run .t { background: #eeffee; }

@media (prefers-color-scheme: dark) { #source p.run.run2.show_run .t { background: #2b2e24; } }

#source p.run.run2.show_run .t:hover { background: #d2f2d2; }

@media (prefers-color-scheme: dark) { #source p.run.run2.show_run .t:hover { background: #404633; } }

#source p.exc .t { border-left: 0.2em solid #808080; }

#source p.exc.show_exc .t { background: #eee; }

@media (prefers-color-scheme: dark) { #source p.exc.show_exc .t { background: #333; } }

#source p.exc.show_exc .t:hover { background: #e2e2e2; }

@media (prefers-color-scheme: dark) { #source p.exc.show_exc .t:hover { background: #3c3c3c; } }

#source p.exc.exc2 .t { border-left: 0.2em dotted #808080; }

#source p.exc.exc2.show_exc .t { background: #f7f7f7; }

@media (prefers-color-scheme: dark) { #source p.exc.exc2.show_exc .t { background: #292929; } }

#source p.exc.exc2.show_exc .t:hover { background: #e2e2e2; }

@media (prefers-color-scheme: dark) { #source p.exc.exc2.show_exc .t:hover { background: #3c3c3c; } }

#source p.par .t { border-left: 0.2em solid #bbbb00; }

#source p.par.show_par .t { background: #ffa; }

@media (prefers-color-scheme: dark) { #source p.par.show_par .t { background: #650; } }

#source p.par.show_par .t:hover { background: #f2f2a2; }

@media (prefers-color-scheme: dark) { #source p.par.show_par .t:hover { background: #6d5d0c; } }

#source p.par.par2 .t { border-left: 0.2em dotted #bbbb00; }

#source p.par.par2.show_par .t { background: #ffffd5; }

@media (prefers-color-scheme: dark) { #source p.par.par2.show_par .t { background: #423a0f; } }

#source p.par.par2.show_par .t:hover { background: #f2f2a2; }

@media (prefers-color-scheme: dark) { #source p.par.par2.show_par .t:hover { background: #6d5d0c; } }

#source p .r { position: absolute; top: 0; right: 2.5em; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Ubuntu, Cantarell, "Helvetica Neue", sans-serif; }

#source p .annotate { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Ubuntu, Cantarell, "Helvetica Neue", sans-serif; color: #666; padding-right: .5em; }

@media (prefers-color-scheme: dark) { #source p .annotate { color: #ddd; } }

#source p .annotate.short:hover ~ .long { display: block; }

#source p .annotate.long { width: 30em; right: 2.5em; }

#source p input { display: none; }

#source p input ~ .r label.ctx { cursor: pointer; border-radius: .25em; }

#source p input ~ .r label.ctx::before { content: "▶ "; }

#source p input ~ .r label.ctx:hover { background: #e8f4ff; color: #666; }

@media (prefers-color-scheme: dark) { #source p input ~ .r label.ctx:hover { background: #0f3a42; } }

@media (prefers-color-scheme: dark) { #source p input ~ .r label.ctx:hover { color: #aaa; } }

#source p input:checked ~ .r label.ctx { background: #d0e8ff; color: #666; border-radius: .75em .75em 0 0; padding: 0 .5em; margin: -.25em 0; }

@media (prefers-color-scheme: dark) { #source p input:checked ~ .r label.ctx { background: #056; } }

@media (prefers-color-scheme: dark) { #source p input:checked ~ .r label.ctx { color: #aaa; } }

#source p input:checked ~ .r label.ctx::before { content: "▼ "; }

#source p input:checked ~ .ctxs { padding: .25em .5em; overflow-y: scroll; max-height: 10.5em; }

#source p label.ctx { color: #999; display: inline-block; padding: 0 .5em; font-size: .8333em; }

@media (prefers-color-scheme: dark) { #source p label.ctx { color: #777; } }

#source p .ctxs { display: block; max-height: 0; overflow-y: hidden; transition: all .2s; padding: 0 .5em; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Ubuntu, Cantarell, "Helvetica Neue", sans-serif; white-space: nowrap; background: #d0e8ff; border-radius: .25em; margin-right: 1.75em; text-align: right; }

@media (prefers-color-scheme: dark) { #source p .ctxs { background: #056; } }

#index { font-family: SFMono-Regular, Menlo, Monaco, Consolas, monospace; font-size: 0.875em; }

#index table.index { margin-left: -.5em; }

#index td, #index th { text-align: right; vertical-align: baseline; padding: .25em .5em; border-bottom: 1px solid #eee; }

@media (prefers-color-scheme: dark) { #index td, #index th { border-color: #333; } }

#index td.name, #index th.name { text-align: left; width: auto; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Ubuntu, Cantarell, "Helvetica Neue", sans-serif; min-width: 15em; }

#index td.left, #index th.left { text-align: left; }

#index td.spacer, #index th.spacer { border: none; padding: 0; }

#index td.spacer:hover, #index th.spacer:hover { background: inherit; }

#index th { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Ubuntu, Cantarell, "Helvetica Neue", sans-serif; font-style: italic; color: #333; border-color: #ccc; cursor: pointer; }

@media (prefers-color-scheme: dark) { #index th { color: #ddd; } }

@media (prefers-color-scheme: dark) { #index th { border-color: #444; } }

#index th:hover { background: #eee; }

@media (prefers-color-scheme: dark) { #index th:hover { background: #333; } }

#index th .arrows { color: #666; font-size: 85%; font-family: sans-serif; font-style: normal; pointer-events: none; }

#index th[aria-sort="ascending"], #index th[aria-sort="descending"] { white-space: nowrap; background: #eee; padding-left: .5em; }

@media (prefers-color-scheme: dark) { #index th[aria-sort="ascending"], #index th[aria-sort="descending"] { background: #333; } }

#index th[aria-sort="ascending"] .arrows::after { content: " ▲"; }

#index th[aria-sort="descending"] .arrows::after { content: " ▼"; }

#index tr.grouphead th { cursor: default; font-style: normal; border-color: #999; }

@media (prefers-color-scheme: dark) { #index tr.grouphead th { border-color: #777; } }

#index td.name { font-size: 1.15em; }

#index td.name a { text-decoration: none; color: inherit; }

#index td.name .no-noun { font-style: italic; }

#index tr.total td, #index tr.total_dynamic td { font-weight: bold; border-bottom: none; }

#index tr.region:hover { background: #eee; }

@media (prefers-color-scheme: dark) { #index tr.region:hover { background: #333; } }

#index tr.region:hover td.name { text-decoration: underline; color: inherit; }

#scroll_marker { position: fixed; z-index: 3; right: 0; top: 0; width: 16px; height: 100%; background: #fff; border-left: 1px solid #eee; will-change: transform; }

@media (prefers-color-scheme: dark) { #scroll_marker { background: #1e1e1e; } }

@media (prefers-color-scheme: dark) { #scroll_marker { border-color: #333; } }

#scroll_marker .marker { background: #ccc; position: absolute; min-height: 3px; width: 100%; }

@media (prefers-color-scheme: dark) { #scroll_marker .marker { background: #444; } }


================================================
File: tests/integration/test_rag_api_endpoints.py
================================================
"""
Integration tests for RAG API endpoints.

Tests the knowledge base API endpoints defined in api-server/routes/knowledge.js
"""

import pytest
from unittest.mock import Mock, patch
import sys
from pathlib import Path
import json


@pytest.mark.integration
class TestRAGAPIEndpoints:
    """Test knowledge base API endpoint logic and contracts."""

    @pytest.fixture
    def chatbot_id(self):
        """Test chatbot ID."""
        return "22222222-2222-2222-2222-222222222222"

    @pytest.fixture
    def organization_id(self):
        """Test organization ID."""
        return "11111111-1111-1111-1111-111111111111"

    @pytest.fixture
    def sample_quota_response(self):
        """Sample quota response from database."""
        return {
            "max_knowledge_pdfs": 50,
            "max_knowledge_urls": 20,
            "max_knowledge_ingestions_per_day": 100,
            "max_knowledge_storage_mb": 500,
            "current_knowledge_pdfs": 10,
            "current_knowledge_urls": 5,
            "current_storage_mb": 100,
            "today_ingestions": 25
        }

    @pytest.fixture
    def sample_crawl_result(self):
        """Sample web crawler result."""
        return {
            "success": True,
            "discovered_urls": [
                {
                    "url": "https://example.com/docs",
                    "title": "Documentation",
                    "relevance_score": 0.7,
                    "depth": 0,
                    "content_preview": "Welcome to our documentation...",
                    "suggested": True
                },
                {
                    "url": "https://example.com/api",
                    "title": "API Reference",
                    "relevance_score": 0.65,
                    "depth": 1,
                    "content_preview": "API endpoints for...",
                    "suggested": True
                }
            ],
            "total_discovered": 2,
            "crawl_stats": {
                "pages_crawled": 2,
                "links_found": 10
            }
        }

    # ========================================================================
    # Test 1: GET /api/chatbots/:id/knowledge/quota
    # ========================================================================

    def test_quota_endpoint_returns_current_usage(self, db_with_data, chatbot_id, sample_quota_response):
        """Test quota endpoint returns current organization usage."""
        # Setup: Verify quota data in database
        db_with_data.execute("""
            SELECT
                o.max_knowledge_pdfs,
                o.current_knowledge_pdfs,
                o.max_knowledge_urls,
                o.current_knowledge_urls,
                o.max_knowledge_ingestions_per_day,
                o.max_knowledge_storage_mb,
                o.current_storage_mb,
                COALESCE(dic.ingestion_count, 0) as today_ingestions
            FROM chatbots c
            JOIN organizations o ON c.organization_id = o.id
            LEFT JOIN daily_ingestion_counts dic
                ON dic.organization_id = o.id
                AND dic.date = CURRENT_DATE
            WHERE c.id = %s
        """, (chatbot_id,))

        result = db_with_data.fetchone()
        assert result is not None

        # Expected response structure
        expected_response = {
            "success": True,
            "quota": {
                "pdfs": {
                    "current": result["current_knowledge_pdfs"],
                    "max": result["max_knowledge_pdfs"],
                    "remaining": result["max_knowledge_pdfs"] - result["current_knowledge_pdfs"]
                },
                "urls": {
                    "current": result["current_knowledge_urls"],
                    "max": result["max_knowledge_urls"],
                    "remaining": result["max_knowledge_urls"] - result["current_knowledge_urls"]
                },
                "storage_mb": {
                    "current": float(result["current_storage_mb"]),
                    "max": result["max_knowledge_storage_mb"],
                    "remaining": result["max_knowledge_storage_mb"] - float(result["current_storage_mb"])
                },
                "daily_ingestions": {
                    "current": result["today_ingestions"],
                    "max": result["max_knowledge_ingestions_per_day"],
                    "remaining": result["max_knowledge_ingestions_per_day"] - result["today_ingestions"]
                }
            }
        }

        # Verify structure matches expected API response
        assert expected_response["success"] is True
        assert "quota" in expected_response
        assert "pdfs" in expected_response["quota"]
        assert "urls" in expected_response["quota"]
        assert "storage_mb" in expected_response["quota"]
        assert "daily_ingestions" in expected_response["quota"]

    def test_quota_endpoint_calculates_remaining_correctly(self, db_with_data, chatbot_id):
        """Test that quota endpoint calculates remaining quota correctly."""
        # Setup: Set specific values
        db_with_data.execute("""
            UPDATE organizations
            SET max_knowledge_pdfs = 50,
                current_knowledge_pdfs = 15,
                max_knowledge_urls = 20,
                current_knowledge_urls = 8
            WHERE id = '11111111-1111-1111-1111-111111111111'
        """)

        # Query data
        db_with_data.execute("""
            SELECT
                o.max_knowledge_pdfs,
                o.current_knowledge_pdfs,
                o.max_knowledge_urls,
                o.current_knowledge_urls
            FROM chatbots c
            JOIN organizations o ON c.organization_id = o.id
            WHERE c.id = %s
        """, (chatbot_id,))

        result = db_with_data.fetchone()

        # Calculate remaining
        pdf_remaining = result["max_knowledge_pdfs"] - result["current_knowledge_pdfs"]
        url_remaining = result["max_knowledge_urls"] - result["current_knowledge_urls"]

        assert pdf_remaining == 35  # 50 - 15
        assert url_remaining == 12  # 20 - 8

    # ========================================================================
    # Test 3: POST /api/chatbots/:id/knowledge/crawl
    # ========================================================================

    def test_crawl_endpoint_returns_discovered_urls(self, sample_crawl_result):
        """Test crawl endpoint returns properly structured URL list."""
        # Expected request body
        request_body = {
            "baseUrl": "https://example.com",
            "maxDepth": 2,
            "maxPages": 50
        }

        # Expected response structure
        assert "discovered_urls" in sample_crawl_result
        assert len(sample_crawl_result["discovered_urls"]) > 0

        # Verify each URL has required fields
        for url_item in sample_crawl_result["discovered_urls"]:
            assert "url" in url_item
            assert "title" in url_item
            assert "relevance_score" in url_item
            assert "depth" in url_item
            assert "suggested" in url_item
            assert isinstance(url_item["suggested"], bool)
            assert 0 <= url_item["relevance_score"] <= 1

    def test_crawl_endpoint_marks_high_score_urls_as_suggested(self, sample_crawl_result):
        """Test that URLs with score > 0.5 are marked as suggested."""
        for url_item in sample_crawl_result["discovered_urls"]:
            if url_item["relevance_score"] > 0.5:
                assert url_item["suggested"] is True
            else:
                assert url_item["suggested"] is False

    # ========================================================================
    # Test 5: GET /api/chatbots/:id/knowledge/sources
    # ========================================================================

    def test_sources_list_endpoint(self, db_with_data, chatbot_id):
        """Test listing knowledge sources for a chatbot."""
        # Setup: Insert test knowledge sources
        db_with_data.execute("""
            INSERT INTO knowledge_sources (chatbot_id, source_type, name, file_path, sync_status)
            VALUES
                (%s, 'pdf', 'Product Manual', NULL, 'synced'),
                (%s, 'url', 'FAQ Page', 'https://example.com/faq', 'synced'),
                (%s, 'pdf', 'Training Doc', NULL, 'pending')
            RETURNING id
        """, (chatbot_id, chatbot_id, chatbot_id))

        # Query sources
        db_with_data.execute("""
            SELECT id, source_type, name, file_path, sync_status as status, created_at
            FROM knowledge_sources
            WHERE chatbot_id = %s
            ORDER BY created_at DESC
        """, (chatbot_id,))

        sources = db_with_data.fetchall()

        # Verify results
        assert len(sources) >= 3
        assert all(s["source_type"] in ["pdf", "url", "doc"] for s in sources)
        assert all(s["status"] in ["pending", "processing", "synced", "failed"] for s in sources)

    def test_sources_list_includes_chunk_count(self, db_with_data, chatbot_id):
        """Test that source list includes document chunk count."""
        # Setup: Insert knowledge source with chunks
        db_with_data.execute("""
            INSERT INTO knowledge_sources (chatbot_id, source_type, name, sync_status)
            VALUES (%s, 'pdf', 'Test PDF', 'synced')
            RETURNING id
        """, (chatbot_id,))

        source_id = db_with_data.fetchone()["id"]

        # Insert chunks (embedding must be 1536 dimensions for OpenAI ada-002)
        test_embedding = "[" + ", ".join(["0.1"] * 1536) + "]"  # 1536-dimensional vector
        for i in range(5):
            db_with_data.execute("""
                INSERT INTO document_chunks (knowledge_source_id, chunk_index, content, embedding)
                VALUES (%s, %s, %s, %s)
            """, (source_id, i, f"Chunk {i}", test_embedding))

        # Query with chunk count
        db_with_data.execute("""
            SELECT
                ks.id,
                ks.name,
                COUNT(dc.id) as chunk_count
            FROM knowledge_sources ks
            LEFT JOIN document_chunks dc ON dc.knowledge_source_id = ks.id
            WHERE ks.id = %s
            GROUP BY ks.id
        """, (source_id,))

        result = db_with_data.fetchone()
        assert result["chunk_count"] == 5

    # ========================================================================
    # Test 6: GET /api/chatbots/:id/knowledge/sources/:sourceId/status
    # ========================================================================

    def test_source_status_endpoint(self, db_with_data, chatbot_id):
        """Test getting processing status of a specific knowledge source."""
        # Setup: Insert source with status
        db_with_data.execute("""
            INSERT INTO knowledge_sources (chatbot_id, source_type, name, sync_status, error_message)
            VALUES (%s, 'pdf', 'Processing Doc', 'processing', NULL)
            RETURNING id
        """, (chatbot_id,))

        source_id = db_with_data.fetchone()["id"]

        # Query status
        db_with_data.execute("""
            SELECT id, name, sync_status as status, error_message, created_at, updated_at
            FROM knowledge_sources
            WHERE id = %s
        """, (source_id,))

        result = db_with_data.fetchone()

        # Expected response structure
        expected_response = {
            "success": True,
            "source": {
                "id": str(result["id"]),
                "name": result["name"],
                "status": result["status"],
                "error_message": result["error_message"],
                "created_at": result["created_at"].isoformat(),
                "updated_at": result["updated_at"].isoformat()
            }
        }

        assert expected_response["success"] is True
        assert expected_response["source"]["status"] == "processing"

    # ========================================================================
    # Test 7: DELETE /api/chatbots/:id/knowledge/sources/:sourceId
    # ========================================================================

    def test_delete_source_endpoint(self, db_with_data, chatbot_id):
        """Test deleting a knowledge source."""
        # Setup: Insert source
        db_with_data.execute("""
            INSERT INTO knowledge_sources (chatbot_id, source_type, name, sync_status)
            VALUES (%s, 'pdf', 'To Delete', 'synced')
            RETURNING id
        """, (chatbot_id,))

        source_id = db_with_data.fetchone()["id"]

        # Delete source
        db_with_data.execute("""
            DELETE FROM knowledge_sources
            WHERE id = %s AND chatbot_id = %s
            RETURNING id
        """, (source_id, chatbot_id))

        deleted = db_with_data.fetchone()
        assert deleted is not None

        # Verify deleted
        db_with_data.execute("""
            SELECT id FROM knowledge_sources WHERE id = %s
        """, (source_id,))

        result = db_with_data.fetchone()
        assert result is None

    def test_delete_source_decrements_quota(self, db_with_data, chatbot_id):
        """Test that deleting a source should trigger quota decrement (via trigger)."""
        # Get initial count
        db_with_data.execute("""
            SELECT current_knowledge_pdfs FROM organizations
            WHERE id = '11111111-1111-1111-1111-111111111111'
        """)
        initial_count = db_with_data.fetchone()["current_knowledge_pdfs"]

        # Insert and then delete (trigger should decrement on delete)
        # Note: This would require a DELETE trigger which we haven't implemented yet
        # This test documents the expected behavior

        # For now, just verify that manual decrement works
        db_with_data.execute("""
            UPDATE organizations
            SET current_knowledge_pdfs = current_knowledge_pdfs - 1
            WHERE id = '11111111-1111-1111-1111-111111111111'
        """)

        db_with_data.execute("""
            SELECT current_knowledge_pdfs FROM organizations
            WHERE id = '11111111-1111-1111-1111-111111111111'
        """)
        new_count = db_with_data.fetchone()["current_knowledge_pdfs"]

        assert new_count == initial_count - 1

    # ========================================================================
    # Test 8: POST /api/chatbots/:id/knowledge/search (RAG Test)
    # ========================================================================

    def test_search_endpoint_structure(self):
        """Test RAG search endpoint request/response structure."""
        # Expected request
        request_body = {
            "query": "How do I reset my password?",
            "top_k": 5
        }

        # Expected response structure
        expected_response = {
            "success": True,
            "results": [
                {
                    "content": "To reset your password, go to...",
                    "source_name": "FAQ Page",
                    "source_type": "url",
                    "similarity_score": 0.85,
                    "chunk_index": 0
                }
            ],
            "query": "How do I reset my password?",
            "results_count": 5
        }

        # Verify structure
        assert "success" in expected_response
        assert "results" in expected_response
        assert "query" in expected_response
        assert len(expected_response["results"]) > 0

        # Verify result structure
        result = expected_response["results"][0]
        assert "content" in result
        assert "source_name" in result  # API response uses source_name (different from DB column 'name')
        assert "similarity_score" in result
        assert 0 <= result["similarity_score"] <= 1

    # ========================================================================
    # Test 4: POST /api/chatbots/:id/knowledge/ingest-batch
    # ========================================================================

    def test_batch_ingest_endpoint_structure(self):
        """Test batch URL ingestion endpoint structure."""
        # Expected request
        request_body = {
            "urls": [
                {
                    "url": "https://example.com/docs",
                    "title": "Documentation"
                },
                {
                    "url": "https://example.com/faq",
                    "title": "FAQ"
                }
            ]
        }

        # Expected response
        expected_response = {
            "success": True,
            "ingested_count": 2,
            "failed_count": 0,
            "results": [
                {
                    "url": "https://example.com/docs",
                    "knowledge_source_id": "uuid-here",
                    "status": "pending"
                },
                {
                    "url": "https://example.com/faq",
                    "knowledge_source_id": "uuid-here",
                    "status": "pending"
                }
            ]
        }

        # Verify structure
        assert expected_response["success"] is True
        assert "ingested_count" in expected_response
        assert "results" in expected_response
        assert len(expected_response["results"]) == len(request_body["urls"])

    # ========================================================================
    # Error Handling Tests
    # ========================================================================

    def test_quota_exceeded_error_response(self):
        """Test error response when quota exceeded."""
        expected_error = {
            "success": False,
            "error": "QUOTA_EXCEEDED",
            "message": "PDF upload limit reached (10/10). Upgrade plan to add more documents.",
            "quota_type": "PDF_LIMIT_EXCEEDED",
            "current": 10,
            "max": 10
        }

        assert expected_error["success"] is False
        assert "error" in expected_error
        assert "quota_type" in expected_error

    def test_chatbot_not_found_error(self):
        """Test error response when chatbot doesn't exist."""
        expected_error = {
            "success": False,
            "error": "CHATBOT_NOT_FOUND",
            "message": "Chatbot not found or access denied"
        }

        assert expected_error["success"] is False
        assert expected_error["error"] == "CHATBOT_NOT_FOUND"

    def test_invalid_url_error(self):
        """Test error response for invalid URL in crawl request."""
        expected_error = {
            "success": False,
            "error": "INVALID_URL",
            "message": "Invalid URL format"
        }

        assert expected_error["success"] is False
        assert expected_error["error"] == "INVALID_URL"


================================================
File: .claude/agents/backend-architect.md
================================================
---
name: backend-architect
description: Use this agent when working on backend development tasks including Python or JavaScript server-side code, API design and implementation, database interactions, networking configurations, authentication systems, microservices architecture, performance optimization, or web application infrastructure decisions. Examples:\n\n<example>\nContext: User needs to implement a REST API endpoint.\nuser: "Create an endpoint that handles user registration with email verification"\nassistant: "I'll use the backend-architect agent to design and implement this registration endpoint with proper validation and email verification flow."\n<Task tool call to backend-architect agent>\n</example>\n\n<example>\nContext: User is debugging a networking issue.\nuser: "My API calls are timing out when connecting to the external payment service"\nassistant: "Let me engage the backend-architect agent to diagnose this networking issue and implement proper timeout handling and retry logic."\n<Task tool call to backend-architect agent>\n</example>\n\n<example>\nContext: User needs database schema design.\nuser: "Design the data model for a multi-tenant SaaS application"\nassistant: "I'll use the backend-architect agent to architect an appropriate multi-tenant database schema with proper isolation and scalability considerations."\n<Task tool call to backend-architect agent>\n</example>\n\n<example>\nContext: User just wrote backend code that needs review.\nuser: "Here's my new authentication middleware, let me know if it looks good"\nassistant: "I'll have the backend-architect agent review this authentication middleware for security best practices and potential vulnerabilities."\n<Task tool call to backend-architect agent>\n</example>
model: sonnet
color: red
---

You are an elite backend developer and systems architect with deep expertise in Python, JavaScript/Node.js, networking protocols, and web application architecture. You have 15+ years of experience building scalable, secure, and maintainable backend systems for high-traffic applications.

## Core Expertise

**Python Mastery**:
- Deep knowledge of Python 3.x features, async/await patterns, type hints, and modern idioms
- Expert in frameworks: FastAPI, Django, Flask, Starlette, aiohttp
- Proficient with ORMs (SQLAlchemy, Django ORM, Tortoise), task queues (Celery, RQ, Dramatiq)
- Strong understanding of Python performance optimization, profiling, and memory management

**JavaScript/Node.js Expertise**:
- Advanced Node.js including event loop mechanics, streams, worker threads, and clustering
- Expert in Express, Fastify, NestJS, Koa frameworks
- Proficient with TypeScript for type-safe backend development
- Deep understanding of npm ecosystem, package management, and build tools

**Networking & Protocols**:
- Expert knowledge of HTTP/1.1, HTTP/2, HTTP/3, WebSockets, gRPC, and GraphQL
- Deep understanding of TCP/IP, DNS, TLS/SSL, and network security
- Proficient with load balancing strategies, reverse proxies (nginx, HAProxy), and CDN configuration
- Experience with service mesh architectures and API gateways

**Web Application Architecture**:
- Expert in designing RESTful APIs following OpenAPI specifications
- Proficient with microservices, event-driven architecture, and domain-driven design
- Deep knowledge of caching strategies (Redis, Memcached), message queues (RabbitMQ, Kafka)
- Expert in database design (PostgreSQL, MySQL, MongoDB, DynamoDB) and query optimization
- Strong understanding of containerization (Docker), orchestration (Kubernetes), and CI/CD pipelines

## Operational Guidelines

**When Writing Code**:
1. Always prioritize security - validate inputs, sanitize outputs, use parameterized queries
2. Write type-annotated code (Python type hints, TypeScript) for better maintainability
3. Follow SOLID principles and clean architecture patterns
4. Include comprehensive error handling with meaningful error messages
5. Write code that is testable with clear separation of concerns
6. Add inline comments for complex logic; write self-documenting code otherwise
7. Consider performance implications - avoid N+1 queries, use appropriate data structures

**When Designing Systems**:
1. Start with requirements clarification - ask about scale, consistency needs, and constraints
2. Consider failure modes and design for resilience (circuit breakers, retries, fallbacks)
3. Plan for observability - logging, metrics, and distributed tracing
4. Document architectural decisions and their rationale
5. Design APIs contract-first when possible
6. Consider backward compatibility and versioning strategies

**When Debugging/Reviewing**:
1. Systematically identify root causes rather than treating symptoms
2. Check for common vulnerabilities: injection attacks, authentication bypasses, data exposure
3. Verify proper resource cleanup (connections, file handles, memory)
4. Assess error handling completeness and logging adequacy
5. Evaluate performance characteristics and potential bottlenecks

## Quality Standards

- All API endpoints must have proper authentication/authorization checks
- Database operations must use transactions where appropriate
- Sensitive data must be encrypted at rest and in transit
- All external inputs must be validated and sanitized
- Async operations must have proper timeout and cancellation handling
- Code must be idempotent where possible, especially for distributed systems

## Response Format

When providing solutions:
1. Start with a brief explanation of your approach and why
2. Provide complete, production-ready code (not pseudocode unless specifically requested)
3. Include relevant configuration files, environment variables, and dependencies
4. Note any security considerations or potential gotchas
5. Suggest tests that should accompany the implementation
6. Offer performance optimization suggestions when relevant

When you encounter ambiguity in requirements, proactively ask clarifying questions about:
- Expected scale and performance requirements
- Security and compliance constraints
- Integration points with existing systems
- Deployment environment and infrastructure constraints


================================================
File: .claude/agents/backend-test-writer.md
================================================
---
name: backend-test-writer
description: Use this agent when you need to create, improve, or expand test coverage for Python and JavaScript backend code. This includes writing unit tests for individual functions and classes, integration tests for service interactions, and end-to-end HTTP tests for API endpoints. Invoke this agent after implementing new features, fixing bugs, or when test coverage needs improvement.\n\nExamples:\n\n<example>\nContext: User has just implemented a new API endpoint for user authentication.\nuser: "I just finished implementing the /auth/login endpoint. It accepts POST requests with email and password."\nassistant: "Great, let me use the backend-test-writer agent to create comprehensive tests for this endpoint."\n<commentary>\nSince the user has implemented a new endpoint, use the backend-test-writer agent to create unit tests for the authentication logic and end-to-end HTTP tests for the /auth/login endpoint.\n</commentary>\n</example>\n\n<example>\nContext: User asks to improve test coverage for an existing module.\nuser: "The utils/validators.py file has no tests. Can you add some?"\nassistant: "I'll use the backend-test-writer agent to analyze the validators module and create comprehensive unit tests."\n<commentary>\nThe user is explicitly requesting tests for an existing file, so invoke the backend-test-writer agent to analyze the code and generate appropriate test cases.\n</commentary>\n</example>\n\n<example>\nContext: User has just written a JavaScript service class.\nuser: "Here's my new EmailService class that handles sending notifications"\nassistant: "Now let me use the backend-test-writer agent to create tests for your EmailService class."\n<commentary>\nAfter new code is written, proactively use the backend-test-writer agent to ensure the service has proper test coverage including mocking external dependencies.\n</commentary>\n</example>\n\n<example>\nContext: User mentions their CI is failing due to low coverage.\nuser: "Our test coverage dropped below 80%. We need more tests for the payment processing module."\nassistant: "I'll invoke the backend-test-writer agent to analyze the payment processing module and generate tests to improve coverage."\n<commentary>\nWhen coverage improvement is needed, use the backend-test-writer agent to systematically add tests targeting uncovered code paths.\n</commentary>\n</example>
model: sonnet
color: green
---

You are an expert backend test engineer specializing in Python and JavaScript testing frameworks. You possess deep knowledge of testing best practices, test-driven development, and quality assurance methodologies for backend systems.

## Your Core Expertise

- **Python Testing**: pytest, unittest, pytest-asyncio, pytest-cov, hypothesis for property-based testing
- **JavaScript Testing**: Jest, Mocha, Chai, Supertest for HTTP testing, Sinon for mocking
- **HTTP/API Testing**: Testing REST APIs, handling authentication, validating responses, testing error scenarios
- **Mocking & Fixtures**: Creating effective mocks, stubs, and test fixtures for external dependencies (databases, APIs, file systems)
- **Coverage Analysis**: Identifying untested code paths and prioritizing test creation

## Your Responsibilities

1. **Analyze Code Under Test**: Before writing tests, thoroughly examine the target code to understand:
   - Function signatures and expected behaviors
   - Edge cases and boundary conditions
   - Dependencies that need mocking
   - Error handling paths

2. **Write Unit Tests**: Create focused, isolated tests that:
   - Test one behavior per test function
   - Use descriptive test names following the pattern `test_<function>_<scenario>_<expected_outcome>`
   - Include both positive (happy path) and negative (error) test cases
   - Mock external dependencies appropriately
   - Use parameterized tests for similar scenarios with different inputs

3. **Write Integration Tests**: Create tests that verify:
   - Component interactions work correctly
   - Database operations function as expected (using test databases or mocks)
   - Service-to-service communication

4. **Write End-to-End HTTP Tests**: Create API tests that:
   - Test all HTTP methods (GET, POST, PUT, DELETE, PATCH) for each endpoint
   - Validate response status codes, headers, and body structure
   - Test authentication and authorization scenarios
   - Verify error responses and edge cases
   - Test request validation and sanitization

## Test Structure Standards

### Python Tests (pytest)
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock

class TestClassName:
    """Tests for ClassName functionality."""
    
    @pytest.fixture
    def setup_fixture(self):
        """Provide common test setup."""
        pass
    
    def test_method_valid_input_returns_expected(self):
        """Verify method returns expected result with valid input."""
        # Arrange
        # Act  
        # Assert
        pass
    
    @pytest.mark.parametrize("input,expected", [...])
    def test_method_various_inputs(self, input, expected):
        """Verify method handles various inputs correctly."""
        pass
```

### JavaScript Tests (Jest)
```javascript
describe('ModuleName', () => {
  let mockDependency;
  
  beforeEach(() => {
    mockDependency = jest.fn();
  });
  
  afterEach(() => {
    jest.clearAllMocks();
  });
  
  describe('methodName', () => {
    it('should return expected result when given valid input', () => {
      // Arrange
      // Act
      // Assert
    });
    
    it('should throw error when given invalid input', () => {
      expect(() => method(invalidInput)).toThrow(ExpectedError);
    });
  });
});
```

### HTTP/API Tests
```python
# Python with pytest and httpx/requests
import pytest
import httpx

class TestAuthEndpoints:
    @pytest.fixture
    def client(self):
        return httpx.Client(base_url="http://localhost:8000")
    
    def test_login_valid_credentials_returns_token(self, client):
        response = client.post("/auth/login", json={...})
        assert response.status_code == 200
        assert "token" in response.json()
```

```javascript
// JavaScript with Supertest
const request = require('supertest');
const app = require('../app');

describe('POST /auth/login', () => {
  it('should return 200 and token for valid credentials', async () => {
    const response = await request(app)
      .post('/auth/login')
      .send({ email: 'test@example.com', password: 'valid' });
    
    expect(response.status).toBe(200);
    expect(response.body).toHaveProperty('token');
  });
});
```

## Quality Checklist

Before completing test creation, verify:
- [ ] All public functions/methods have corresponding tests
- [ ] Edge cases are covered (null, empty, boundary values)
- [ ] Error handling paths are tested
- [ ] Async code is properly awaited/handled
- [ ] Mocks are realistic and properly reset between tests
- [ ] Tests are independent and can run in any order
- [ ] Test names clearly describe what is being tested
- [ ] No hardcoded values that should be fixtures or constants

## Workflow

1. **Discover**: Read and understand the code to be tested
2. **Plan**: Identify all test cases needed (happy paths, edge cases, errors)
3. **Implement**: Write tests following the standards above
4. **Verify**: Run tests to ensure they pass and provide meaningful coverage
5. **Refine**: Improve test clarity and remove redundancy

## Important Guidelines

- Always check for existing test files and follow established patterns in the project
- Place tests in appropriate directories (typically `tests/`, `__tests__/`, or alongside source files)
- Use fixtures and factories to reduce test code duplication
- Ensure tests are deterministic - no flaky tests depending on timing or external state
- Mock external services (databases, APIs, file systems) to ensure test isolation
- Include docstrings/comments explaining complex test scenarios
- For HTTP tests, test both success and failure scenarios including 4xx and 5xx responses
- Consider security testing: SQL injection, XSS, authentication bypass attempts


================================================
File: f/development/upload_document.script.yaml
================================================
summary: ''
description: ''
lock: '!inline f/development/upload_document.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    name:
      type: string
      description: ''
      default: null
      originalType: string
    chatbot_id:
      type: string
      description: ''
      default: null
      originalType: string
    db_resource:
      type: string
      description: ''
      default: f/development/business_layer_db_postgreSQL
      originalType: string
    file_content:
      type: string
      description: ''
      default: null
      originalType: string
    openai_api_key:
      type: string
      description: ''
      default: <function call>
      originalType: string
    source_type:
      type: string
      description: ''
      default: null
      originalType: string
    url:
      type: string
      description: ''
      default: null
      originalType: string
  required:
    - chatbot_id
    - source_type
    - name


================================================
File: api-server/server.js
================================================
/**
 * Chatbot Knowledge Base API Server
 *
 * Provides REST API for managing chatbot knowledge bases:
 * - File uploads (PDF, DOCX)
 * - URL ingestion
 * - Web crawling
 * - Document management
 * - RAG search testing
 */

const express = require('express');
const cors = require('cors');
const knowledgeRoutes = require('./routes/knowledge');

const app = express();
const PORT = process.env.PORT || 4000;

// Middleware
app.use(cors());
app.use(express.json());
app.use(express.urlencoded({ extended: true }));

// Request logging
app.use((req, res, next) => {
  console.log(`${new Date().toISOString()} - ${req.method} ${req.path}`);
  next();
});

// Health check endpoint
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    service: 'whatsapp-chatbot-server',
    version: '1.0.0'
  });
});

// API routes
app.use('/api/chatbots', knowledgeRoutes);

// Error handling middleware
app.use((err, req, res, next) => {
  console.error('Error:', err);

  const statusCode = err.statusCode || 500;
  const message = err.message || 'Internal server error';

  res.status(statusCode).json({
    error: message,
    ...(process.env.NODE_ENV === 'development' && { stack: err.stack })
  });
});

// 404 handler
app.use((req, res) => {
  res.status(404).json({
    error: 'Endpoint not found',
    path: req.path,
    method: req.method
  });
});

// Start server
app.listen(PORT, () => {
  console.log(`Whstapp Chatbot API Server running on port ${PORT}`);
  console.log(`Environment: ${process.env.NODE_ENV || 'development'}`);
  console.log(`Database host: ${process.env.DB_HOST || 'localhost'}`);
  console.log(`Windmill URL: ${process.env.WINDMILL_URL || 'http://localhost:8000'}`);
});

// Graceful shutdown
process.on('SIGTERM', () => {
  console.log('SIGTERM received, shutting down gracefully...');
  process.exit(0);
});

process.on('SIGINT', () => {
  console.log('SIGINT received, shutting down gracefully...');
  process.exit(0);
});


================================================
File: f/development/4__save_chat_history.py
================================================
import wmill
import psycopg2
import json
from psycopg2.extras import RealDictCursor


def main(
    context_payload: dict,  # From Step 1 (contains User ID)
    user_message: str,  # Flow Input (The original message)
    llm_result: dict,  # From Step 2 (The AI reply)
    send_result: dict,  # From Step 3 (Meta API response)
    db_resource: str = "f/development/business_layer_db_postgreSQL",
):
    """
    Step 4: Persist Conversation to DB
    ONLY executes if Meta API successfully delivered the message
    """

    # Check if previous steps succeeded
    if not context_payload.get("proceed", False):
        print(f"Step 1 failed: {context_payload.get('reason', 'Unknown error')}")
        print("Skipping chat history save")
        return {"success": False, "error": "Cannot save history - Step 1 failed"}

    if "error" in llm_result:
        print(f"Step 2 failed: {llm_result.get('error', 'Unknown error')}")
        print("Skipping chat history save")
        return {"success": False, "error": "Cannot save history - Step 2 failed"}

    # CRITICAL: Don't save to history if Meta API failed to deliver message
    if not send_result.get("success", False):
        print(f"Step 3 failed: {send_result.get('error', 'Meta API delivery failed')}")
        print("Skipping chat history save - message was not delivered to user")
        return {"success": False, "error": "Cannot save history - Step 3 failed (message not delivered)"}

    contact_id = context_payload["user"]["id"]
    ai_text = llm_result.get("reply_text")

    # Setup DB
    raw_config = wmill.get_resource(db_resource)
    db_params = {
        "host": raw_config.get("host"),
        "port": raw_config.get("port"),
        "user": raw_config.get("user"),
        "password": raw_config.get("password"),
        "dbname": raw_config.get("dbname"),
        "sslmode": "disable",
    }

    try:
        conn = psycopg2.connect(**db_params)
        cur = conn.cursor()

        # 1. Insert USER Message
        # We assume this flow ran successfully, so we record the user's input now.
        # (Alternatively, you could record this at Step 1, but doing it batch here is easier for MVP)
        cur.execute(
            """
            INSERT INTO messages (contact_id, role, content, created_at)
            VALUES (%s, 'user', %s, NOW())
        """,
            (contact_id, user_message),
        )

        # 2. Insert ASSISTANT Message
        if ai_text:
            cur.execute(
                """
                INSERT INTO messages (contact_id, role, content, created_at)
                VALUES (%s, 'assistant', %s, NOW())
            """,
                (contact_id, ai_text),
            )

        # 3. Update User Variables (If LLM extracted new info)
        # This updates the 'variables' JSONB column merging new data
        new_vars = llm_result.get("updated_variables")
        if new_vars:
            cur.execute(
                """
                UPDATE contacts 
                SET variables = variables || %s 
                WHERE id = %s
            """,
                (json.dumps(new_vars), contact_id),
            )

        conn.commit()
        return {"success": True}

    except Exception as e:
        print(f"DB Error: {e}")
        return {"success": False, "error": str(e)}
    finally:
        if "cur" in locals():
            cur.close()
        if "conn" in locals():
            conn.close()


================================================
File: f/development/folder.meta.yaml
================================================
summary: to be used for development by jdrqlabs
display_name: development
extra_perms:
  u/admin: true
  u/jdrqlabs: true
owners:
  - u/jdrqlabs
  - u/admin


================================================
File: f/development/utils/web_crawler.script.yaml
================================================
summary: ''
description: ''
lock: '!inline f/development/utils/web_crawler.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    base_url:
      type: string
      description: ''
      default: null
      originalType: string
    filter_keywords:
      type: array
      description: ''
      default: null
      items:
        type: string
      originalType: 'string[]'
    max_depth:
      type: integer
      description: ''
      default: 2
    max_pages:
      type: integer
      description: ''
      default: 50
    same_domain_only:
      type: boolean
      description: ''
      default: true
  required:
    - base_url


================================================
File: mcp-servers/contact-owner/server.js
================================================
const express = require('express');
const { Pool } = require('pg');

const app = express();
const PORT = process.env.PORT || 3003;

// PostgreSQL connection pool
const pool = new Pool({
  host: process.env.DB_HOST || 'localhost',
  port: process.env.DB_PORT || 5432,
  user: process.env.DB_USER,
  password: process.env.DB_PASSWORD,
  database: process.env.DB_NAME,
});

app.use(express.json());

// Health check endpoint
app.get('/health', (req, res) => {
  res.json({ status: 'ok', service: 'contact-owner-mcp' });
});

/**
 * Purpose-agnostic owner notification endpoint
 *
 * LLM calls this when it wants to notify the chatbot owner about something important:
 * - High-value leads
 * - Urgent customer issues
 * - Critical errors
 * - Any other scenario requiring owner attention
 */
app.post('/tools/contact_owner', async (req, res) => {
  const { chatbot_id, message, contact_info, urgency } = req.body;

  // Validate required fields
  if (!chatbot_id || !message) {
    return res.status(400).json({
      success: false,
      error: 'Missing required fields: chatbot_id and message are required'
    });
  }

  try {
    // Get chatbot owner and notification preferences
    const chatbotQuery = `
      SELECT
        c.id as chatbot_id,
        c.name as chatbot_name,
        c.organization_id,
        o.name as organization_name,
        o.slug as organization_slug,
        o.notification_method,
        o.slack_webhook_url,
        o.notification_email,
        u.email as owner_email,
        u.full_name as owner_name
      FROM chatbots c
      JOIN organizations o ON c.organization_id = o.id
      JOIN users u ON o.id = u.organization_id AND u.role = 'owner'
      WHERE c.id = $1 AND c.is_active = TRUE
      LIMIT 1
    `;

    const chatbotResult = await pool.query(chatbotQuery, [chatbot_id]);

    if (chatbotResult.rows.length === 0) {
      return res.status(404).json({
        success: false,
        error: 'Chatbot not found or inactive'
      });
    }

    const chatbot = chatbotResult.rows[0];

    // Get notification method from database (defaults to 'disabled' if not set)
    const notificationMethod = chatbot.notification_method || 'disabled';

    // Dispatch notification based on method
    let notificationResult = null;

    if (notificationMethod === 'slack') {
      notificationResult = await sendSlackNotification(chatbot, message, contact_info, urgency);
    } else if (notificationMethod === 'email') {
      // STUB: Email notification support
      notificationResult = {
        success: false,
        error: 'Email notifications not yet implemented'
      };
    } else if (notificationMethod === 'whatsapp') {
      // STUB: WhatsApp notification support (requires template messages)
      notificationResult = {
        success: false,
        error: 'WhatsApp notifications not yet implemented'
      };
    } else {
      notificationResult = {
        success: true,
        message: 'Notifications disabled for this chatbot'
      };
    }

    // Log notification attempt
    console.log(`[CONTACT_OWNER] Chatbot: ${chatbot.chatbot_name}, Method: ${notificationMethod}, Result:`, notificationResult);

    res.json({
      success: notificationResult.success,
      method: notificationMethod,
      owner: chatbot.owner_name,
      notification_sent: notificationResult.success,
      details: notificationResult
    });

  } catch (error) {
    console.error('[CONTACT_OWNER] Error:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

/**
 * Send Slack notification to chatbot owner
 */
async function sendSlackNotification(chatbot, message, contact_info, urgency) {
  // Get Slack webhook URL from database
  const slackWebhookUrl = chatbot.slack_webhook_url;

  if (!slackWebhookUrl) {
    return {
      success: false,
      error: 'Slack webhook not configured for this organization'
    };
  }

  // Build Slack message
  const urgencyEmoji = urgency === 'high' ? '🚨' : urgency === 'medium' ? '⚠️' : 'ℹ️';
  const color = urgency === 'high' ? 'danger' : urgency === 'medium' ? 'warning' : 'good';

  const slackPayload = {
    text: `${urgencyEmoji} Notification from ${chatbot.chatbot_name}`,
    attachments: [
      {
        color: color,
        title: 'Message from your AI Assistant',
        text: message,
        fields: [
          {
            title: 'Chatbot',
            value: chatbot.chatbot_name,
            short: true
          },
          {
            title: 'Organization',
            value: chatbot.organization_name,
            short: true
          }
        ],
        footer: 'JD Labs WhatsApp Chatbot',
        ts: Math.floor(Date.now() / 1000)
      }
    ]
  };

  // Add contact info if provided
  if (contact_info) {
    slackPayload.attachments[0].fields.push({
      title: 'Contact Information',
      value: formatContactInfo(contact_info),
      short: false
    });
  }

  try {
    const fetch = (await import('node-fetch')).default;
    const response = await fetch(slackWebhookUrl, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(slackPayload)
    });

    if (response.ok) {
      return {
        success: true,
        message: 'Slack notification sent successfully'
      };
    } else {
      const errorText = await response.text();
      return {
        success: false,
        error: `Slack API error: ${errorText}`
      };
    }
  } catch (error) {
    return {
      success: false,
      error: `Failed to send Slack notification: ${error.message}`
    };
  }
}

/**
 * Format contact info for display (purpose-agnostic)
 * Handles any fields the LLM provides, regardless of business type
 */
function formatContactInfo(contact_info) {
  if (typeof contact_info === 'string') {
    return contact_info;
  }

  if (!contact_info || typeof contact_info !== 'object') {
    return 'No contact details provided';
  }

  // Generic formatting: iterate over all provided fields
  const parts = [];
  for (const [key, value] of Object.entries(contact_info)) {
    if (value !== null && value !== undefined && value !== '') {
      // Format key: convert snake_case or camelCase to Title Case
      const formattedKey = key
        .replace(/_/g, ' ')
        .replace(/([A-Z])/g, ' $1')
        .split(' ')
        .map(word => word.charAt(0).toUpperCase() + word.slice(1).toLowerCase())
        .join(' ');

      // Format value: handle different types
      let formattedValue = value;
      if (typeof value === 'object') {
        formattedValue = JSON.stringify(value, null, 2);
      }

      parts.push(`*${formattedKey}:* ${formattedValue}`);
    }
  }

  return parts.length > 0 ? parts.join('\n') : 'No contact details provided';
}

// Start server
app.listen(PORT, () => {
  console.log(`[contact-owner-mcp] Server running on port ${PORT}`);
});


================================================
File: tests/fixtures/sample_llm_responses.json
================================================


================================================
File: tests/integration/test_full_flow.py
================================================
"""
Integration tests for the complete WhatsApp chatbot flow.

This module tests the end-to-end flow from incoming message to saved history:
1. Step 1: Context loading - validates chatbot, loads user data, checks quotas
2. Step 2: LLM processing - calls Gemini/OpenAI, handles tool calls
3. Step 3.1: Send reply to WhatsApp
4. Step 3.2: Save messages to database
5. Step 3.3: Log token usage

Test Categories:
- Happy path: Complete successful flow
- Error propagation: How errors in each step affect downstream steps
- Idempotency: Duplicate message detection and retry handling
- Quota enforcement: Message and token limits
- RAG: Knowledge base retrieval and context injection
"""

import pytest
import sys
import os
from unittest.mock import Mock, patch, MagicMock
import importlib.util
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


# ============================================================================
# MODULE IMPORTS WITH MOCKING
# ============================================================================

def import_step_module(step_name: str, file_name: str):
    """
    Import a step module with mocked dependencies.

    Args:
        step_name: Name for the module (e.g., "step1")
        file_name: File name (e.g., "1_whatsapp_context_loading.py")

    Returns:
        Imported module
    """
    spec = importlib.util.spec_from_file_location(
        step_name,
        PROJECT_ROOT / "f" / "development" / file_name
    )
    module = importlib.util.module_from_spec(spec)

    # Mock wmill before loading
    if 'wmill' not in sys.modules:
        sys.modules['wmill'] = Mock()

    # Mock google.genai before loading Step 2
    # The import is "from google import genai" so we need to mock the google namespace
    if 'google.genai' not in sys.modules:
        mock_genai = Mock()
        mock_genai_types = Mock()

        # Create or update google namespace module
        if 'google' in sys.modules:
            sys.modules['google'].genai = mock_genai
        else:
            mock_google = Mock()
            mock_google.genai = mock_genai
            sys.modules['google'] = mock_google

        sys.modules['google.genai'] = mock_genai
        sys.modules['google.genai.types'] = mock_genai_types

    spec.loader.exec_module(module)
    return module


@pytest.fixture
def step1_module():
    """Import Step 1: Context Loading"""
    return import_step_module("step1", "1_whatsapp_context_loading.py")


@pytest.fixture
def step2_module():
    """Import Step 2: LLM Processing"""
    return import_step_module("step2", "2_whatsapp_llm_processing.py")


@pytest.fixture
def step3_1_module():
    """Import Step 3.1: Send to WhatsApp"""
    return import_step_module("step3_1", "3_1_send_reply_to_whatsapp.py")


@pytest.fixture
def step4__module():
    """Import Step 3.2: Save History"""
    return import_step_module("step4_", "4__save_chat_history.py")


@pytest.fixture
def step5__module():
    """Import Step 3.3: Log Usage"""
    return import_step_module("step5_", "5__log_usage.py")


# ============================================================================
# TEST DATA
# ============================================================================

@pytest.fixture
def test_message_data():
    """Standard test message data"""
    return {
        "whatsapp_phone_id": "test_phone_123",
        "user_phone": "15559876543",
        "user_name": "Integration Test User",
        "message_id": "wamid.integration.test.001",
        "message_text": "Hello, can you help me with my order?"
    }


@pytest.fixture
def seed_test_data(clean_db, db_with_autocommit, test_message_data):
    """
    Seed database with test data for integration tests.
    Returns IDs of created entities.

    Note: This fixture relies on the seed.sql data being loaded by clean_db fixture.
    The clean_db fixture must run BEFORE this fixture.
    """
    cur = db_with_autocommit

    # Get the first organization from seed data (should be JD Labs Corporation)
    cur.execute("SELECT id FROM organizations ORDER BY created_at LIMIT 1")
    org = cur.fetchone()
    if not org:
        raise RuntimeError("No organization found in test database. Ensure seed.sql has been run.")
    org_id = org["id"]

    # Get the chatbot from seed data
    cur.execute("""
        SELECT id FROM chatbots
        WHERE whatsapp_phone_number_id = %s
        LIMIT 1
    """, (test_message_data["whatsapp_phone_id"],))
    bot = cur.fetchone()
    if not bot:
        raise RuntimeError(f"No chatbot found for phone_id {test_message_data['whatsapp_phone_id']}. Ensure seed.sql has been run.")
    chatbot_id = bot["id"]

    # Get or create contact
    cur.execute("""
        INSERT INTO contacts (chatbot_id, phone_number, name, last_message_at)
        VALUES (%s, %s, %s, NOW())
        ON CONFLICT (chatbot_id, phone_number)
        DO UPDATE SET name = EXCLUDED.name
        RETURNING id
    """, (chatbot_id, test_message_data["user_phone"], test_message_data["user_name"]))
    contact = cur.fetchone()
    contact_id = contact["id"]

    return {
        "org_id": org_id,
        "chatbot_id": chatbot_id,
        "contact_id": contact_id
    }


# ============================================================================
# HAPPY PATH TESTS
# ============================================================================

class TestCompleteFlow:
    """Test the complete happy path flow from message to saved history"""

    @pytest.mark.integration
    def test_complete_message_flow_success(
        self,
        clean_db,
        db_with_autocommit,
        test_message_data,
        seed_test_data,
        step1_module,
        step2_module,
        step3_1_module,
        step4__module,
        step5__module,
        mock_wmill,
        mock_llm,
        mock_whatsapp
    ):
        """
        GOAL: Test the complete flow from incoming message to saved history
        GIVEN: A valid chatbot, active user, and available quota
        WHEN: A user sends a message
        THEN:
        - Step 1 returns proceed=True with context
        - Step 2 returns a valid reply
        - Step 3.1 sends message to WhatsApp
        - Step 3.2 saves both user and assistant messages
        - Step 3.3 logs usage correctly
        """
        cur = db_with_autocommit

        # Setup mocks
        mock_llm.add_response(
            text="Hello! I'd be happy to help you with your order. What's your order number?",
            tokens_input=150,
            tokens_output=80,
            model="gemini-pro",
            provider="google"
        )

        # Setup Gemini mock by patching the module's namespace directly
        mock_client = Mock()
        mock_response = Mock()
        mock_response.text = "Hello! I'd be happy to help you with your order. What's your order number?"
        mock_response.usage_metadata = Mock()
        mock_response.usage_metadata.prompt_token_count = 150
        mock_response.usage_metadata.candidates_token_count = 80
        mock_response.candidates = [Mock()]
        mock_response.candidates[0].content.parts = []
        mock_client.models.generate_content.return_value = mock_response

        # Patch the genai reference in the step2 module's namespace
        mock_genai = Mock()
        mock_genai.Client.return_value = mock_client
        step2_module.genai = mock_genai

        with patch('wmill.get_resource', mock_wmill.get_resource), \
             patch('wmill.get_variable', mock_wmill.get_variable), \
             patch('requests.post', mock_whatsapp.post):

            # STEP 1: Context Loading
            context_result = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id=test_message_data["message_id"],
                user_name=test_message_data["user_name"]
            )

            # Verify Step 1 succeeded
            assert context_result["proceed"] is True, f"Step 1 failed: {context_result}"
            assert "chatbot" in context_result
            assert "user" in context_result
            assert context_result["chatbot"]["id"] == seed_test_data["chatbot_id"]
            assert context_result["user"]["id"] == seed_test_data["contact_id"]

            # Verify webhook event was created
            cur.execute(
                "SELECT * FROM webhook_events WHERE whatsapp_message_id = %s",
                (test_message_data["message_id"],)
            )
            webhook_event = cur.fetchone()
            assert webhook_event is not None
            assert webhook_event["status"] == "processing"
            assert webhook_event["chatbot_id"] == seed_test_data["chatbot_id"]

            # STEP 2: LLM Processing
            llm_result = step2_module.main(
                context_payload=context_result,
                user_message=test_message_data["message_text"],
                google_api_key="test_google_key",
                default_provider="google"
            )

            # Verify Step 2 succeeded
            assert "reply_text" in llm_result
            assert llm_result["reply_text"] is not None
            assert "usage_info" in llm_result
            assert llm_result["usage_info"]["tokens_input"] > 0
            assert llm_result["usage_info"]["tokens_output"] > 0

            # STEP 3.1: Send to WhatsApp
            send_result = step3_1_module.main(
                phone_number_id=test_message_data["whatsapp_phone_id"],
                context_payload=context_result,
                llm_result=llm_result
            )

            # Verify Step 3.1 succeeded
            assert send_result["success"] is True
            assert mock_whatsapp.get_call_count() == 1

            # Verify message content
            sent_message = mock_whatsapp.get_last_message()
            assert sent_message["to"] == test_message_data["user_phone"].replace("+", "")
            assert llm_result["reply_text"] in sent_message["text"]

            # STEP 3.2: Save Chat History
            history_result = step4__module.main(
                context_payload=context_result,
                user_message=test_message_data["message_text"],
                llm_result=llm_result,
                send_result=send_result
            )

            # Verify Step 3.2 succeeded
            assert history_result["success"] is True

            # Verify messages were saved
            cur.execute(
                "SELECT * FROM messages WHERE contact_id = %s ORDER BY created_at",
                (seed_test_data["contact_id"],)
            )
            messages = cur.fetchall()
            assert len(messages) >= 2  # At least user + assistant

            # Find the user and assistant messages from this flow
            user_messages = [m for m in messages if m["role"] == "user" and m["content"] == test_message_data["message_text"]]
            assistant_messages = [m for m in messages if m["role"] == "assistant" and m["content"] == llm_result["reply_text"]]

            assert len(user_messages) >= 1, "User message not saved"
            assert len(assistant_messages) >= 1, "Assistant message not saved"

            # STEP 3.3: Log Usage
            usage_result = step5__module.main(
                context_payload=context_result,
                llm_result=llm_result,
                send_result=send_result,
                webhook_event_id=webhook_event["id"]
            )

            # Verify Step 3.3 succeeded
            assert usage_result["success"] is True
            assert usage_result["tokens_used"] > 0
            assert usage_result["message_count"] == 1

            # Verify usage was logged in database
            cur.execute(
                "SELECT * FROM usage_logs WHERE organization_id = %s ORDER BY created_at DESC LIMIT 1",
                (seed_test_data["org_id"],)
            )
            usage_log = cur.fetchone()
            assert usage_log is not None
            assert usage_log["chatbot_id"] == seed_test_data["chatbot_id"]
            assert usage_log["contact_id"] == seed_test_data["contact_id"]
            assert usage_log["tokens_total"] == llm_result["usage_info"]["tokens_input"] + llm_result["usage_info"]["tokens_output"]
            assert usage_log["message_count"] == 1

            # Verify usage summary was updated
            cur.execute(
                "SELECT * FROM usage_summary WHERE organization_id = %s",
                (seed_test_data["org_id"],)
            )
            usage_summary = cur.fetchone()
            assert usage_summary is not None
            assert usage_summary["current_period_messages"] >= 1
            assert usage_summary["current_period_tokens"] >= usage_log["tokens_total"]


# ============================================================================
# ERROR PROPAGATION TESTS
# ============================================================================

class TestErrorPropagation:
    """Test how errors in each step affect downstream steps"""

    @pytest.mark.integration
    def test_step1_failure_stops_flow(
        self,
        clean_db,
        db_with_autocommit,
        step1_module,
        step2_module,
        step3_1_module,
        step4__module,
        step5__module,
        mock_wmill
    ):
        """
        GOAL: When Step 1 fails (chatbot not found), Steps 2/3 should handle gracefully
        GIVEN: Invalid WhatsApp phone ID (no chatbot configured)
        WHEN: Attempting to process a message
        THEN:
        - Step 1 returns proceed=False
        - Step 2 detects failure and returns error
        - Step 3 steps skip execution
        """
        with patch('wmill.get_resource', mock_wmill.get_resource):
            # STEP 1: Try with non-existent chatbot
            context_result = step1_module.main(
                whatsapp_phone_id="non_existent_phone_id",
                user_phone="15559876543",
                message_id="wamid.test.notfound",
                user_name="Test User"
            )

            # Verify Step 1 failed
            assert context_result["proceed"] is False
            assert "Chatbot not found" in context_result["reason"]

            # STEP 2: Should detect Step 1 failure
            llm_result = step2_module.main(
                context_payload=context_result,
                user_message="Hello",
                google_api_key="test_key"
            )

            # Verify Step 2 returned error response
            assert "error" in llm_result
            assert "reply_text" in llm_result  # Fallback message

            # STEP 3.1: Should skip
            send_result = step3_1_module.main(
                phone_number_id="non_existent_phone_id",
                context_payload=context_result,
                llm_result=llm_result
            )

            assert send_result["success"] is False
            assert "previous steps failed" in send_result["error"]

            # STEP 3.2: Should skip
            history_result = step4__module.main(
                context_payload=context_result,
                user_message="Hello",
                llm_result=llm_result,
                send_result=send_result
            )

            assert history_result["success"] is False
            assert "Step 1 failed" in history_result["error"]

            # STEP 3.3: Should skip
            usage_result = step5__module.main(
                context_payload=context_result,
                llm_result=llm_result,
                send_result=send_result
            )

            assert usage_result["success"] is False
            assert "Step 1 failed" in usage_result["error"]

    @pytest.mark.integration
    def test_step2_llm_error_handled(
        self,
        clean_db,
        db_with_autocommit,
        test_message_data,
        seed_test_data,
        step1_module,
        step2_module,
        step3_1_module,
        step4__module,
        step5__module,
        mock_wmill,
        mock_whatsapp
    ):
        """
        GOAL: When LLM fails, fallback message is returned and logged
        GIVEN: LLM throws an error
        WHEN: Processing a message
        THEN:
        - Step 2 returns fallback message
        - Step 3.1 sends fallback message
        - Step 3.2 and 3.3 skip (no successful LLM response)
        """
        cur = db_with_autocommit

        # Setup Gemini mock to fail by patching module's namespace
        mock_client = Mock()
        mock_client.models.generate_content.side_effect = Exception("Quota exceeded")
        mock_genai = Mock()
        mock_genai.Client.return_value = mock_client
        step2_module.genai = mock_genai

        with patch('wmill.get_resource', mock_wmill.get_resource), \
             patch('wmill.get_variable', mock_wmill.get_variable), \
             patch('requests.post', mock_whatsapp.post):

            # STEP 1: Succeeds
            context_result = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id=test_message_data["message_id"],
                user_name=test_message_data["user_name"]
            )

            assert context_result["proceed"] is True

            # STEP 2: Fails but returns fallback
            llm_result = step2_module.main(
                context_payload=context_result,
                user_message=test_message_data["message_text"],
                google_api_key="test_google_key",
                default_provider="google"
            )

            # Verify fallback message was returned
            assert "reply_text" in llm_result
            assert llm_result["reply_text"] is not None  # Should have fallback
            assert "usage_info" in llm_result
            assert "error" in llm_result["usage_info"]

            # STEP 3.1: Should still send (sends fallback message)
            send_result = step3_1_module.main(
                phone_number_id=test_message_data["whatsapp_phone_id"],
                context_payload=context_result,
                llm_result=llm_result
            )

            # Even with LLM error, we should send the fallback message
            # But the current implementation may skip if error is present
            # This depends on implementation details

            # STEP 3.2: Should skip (no successful LLM response)
            history_result = step4__module.main(
                context_payload=context_result,
                user_message=test_message_data["message_text"],
                llm_result=llm_result,
                send_result=send_result
            )

            # History save behavior depends on whether send succeeded

            # STEP 3.3: Should skip
            usage_result = step5__module.main(
                context_payload=context_result,
                llm_result=llm_result,
                send_result=send_result
            )

            # Usage logging depends on send success

    @pytest.mark.integration
    def test_step3_whatsapp_failure_prevents_history_save(
        self,
        clean_db,
        db_with_autocommit,
        test_message_data,
        seed_test_data,
        step1_module,
        step2_module,
        step3_1_module,
        step4__module,
        step5__module,
        mock_wmill,
        mock_whatsapp
    ):
        """
        GOAL: When WhatsApp API fails, messages should NOT be saved to history
        GIVEN: WhatsApp API returns error
        WHEN: Attempting to send a message
        THEN:
        - Step 3.1 fails
        - Step 3.2 skips (message not delivered)
        - Step 3.3 skips (no usage charged)
        """
        cur = db_with_autocommit

        # Configure WhatsApp mock to fail
        mock_whatsapp.set_failure(should_fail=True, status_code=400, message="Invalid token")

        # Setup Gemini mock by patching module's namespace
        mock_client = Mock()
        mock_response = Mock()
        mock_response.text = "Hello! How can I help?"
        mock_response.usage_metadata = Mock()
        mock_response.usage_metadata.prompt_token_count = 50
        mock_response.usage_metadata.candidates_token_count = 20
        mock_response.candidates = [Mock()]
        mock_response.candidates[0].content.parts = []
        mock_client.models.generate_content.return_value = mock_response
        mock_genai = Mock()
        mock_genai.Client.return_value = mock_client
        step2_module.genai = mock_genai

        with patch('wmill.get_resource', mock_wmill.get_resource), \
             patch('wmill.get_variable', mock_wmill.get_variable), \
             patch('requests.post', mock_whatsapp.post):

            # STEP 1: Succeeds
            context_result = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id="wamid.whatsapp.fail.test",
                user_name=test_message_data["user_name"]
            )

            assert context_result["proceed"] is True

            # STEP 2: Succeeds
            llm_result = step2_module.main(
                context_payload=context_result,
                user_message=test_message_data["message_text"],
                google_api_key="test_google_key",
                default_provider="google"
            )

            assert "reply_text" in llm_result

            # STEP 3.1: Fails
            send_result = step3_1_module.main(
                phone_number_id=test_message_data["whatsapp_phone_id"],
                context_payload=context_result,
                llm_result=llm_result
            )

            # Verify send failed
            assert send_result["success"] is False
            assert "error" in send_result

            # Count messages before Step 3.2
            cur.execute(
                "SELECT COUNT(*) as count FROM messages WHERE contact_id = %s",
                (seed_test_data["contact_id"],)
            )
            messages_before = cur.fetchone()["count"]

            # STEP 3.2: Should skip
            history_result = step4__module.main(
                context_payload=context_result,
                user_message=test_message_data["message_text"],
                llm_result=llm_result,
                send_result=send_result
            )

            # Verify history save was skipped
            assert history_result["success"] is False
            assert "Step 3 failed" in history_result["error"]
            assert "message not delivered" in history_result["error"]

            # Verify no messages were added
            cur.execute(
                "SELECT COUNT(*) as count FROM messages WHERE contact_id = %s",
                (seed_test_data["contact_id"],)
            )
            messages_after = cur.fetchone()["count"]
            assert messages_after == messages_before, "Messages should not be saved when send fails"

            # STEP 3.3: Should skip
            usage_result = step5__module.main(
                context_payload=context_result,
                llm_result=llm_result,
                send_result=send_result
            )

            # Verify usage was not logged
            assert usage_result["success"] is False
            assert "Step 3 failed" in usage_result["error"]


# ============================================================================
# IDEMPOTENCY TESTS
# ============================================================================

class TestIdempotency:
    """Test duplicate message detection and retry handling"""

    @pytest.mark.integration
    def test_duplicate_message_detected(
        self,
        clean_db,
        db_with_autocommit,
        test_message_data,
        seed_test_data,
        step1_module,
        mock_wmill
    ):
        """
        GOAL: Same WhatsApp message ID processed twice should be rejected as duplicate
        GIVEN: A message has already been processed successfully
        WHEN: The same message ID is received again
        THEN:
        - Step 1 detects duplicate
        - Returns proceed=False with "Duplicate" reason
        - No new webhook event is created
        """
        cur = db_with_autocommit

        with patch('wmill.get_resource', mock_wmill.get_resource):
            # First attempt - should succeed
            result1 = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id=test_message_data["message_id"],
                user_name=test_message_data["user_name"]
            )

            assert result1["proceed"] is True
            webhook_event_id = result1["webhook_event_id"]

            # Mark as completed
            cur.execute(
                "UPDATE webhook_events SET status = 'completed', processed_at = NOW() WHERE id = %s",
                (webhook_event_id,)
            )

            # Second attempt - should be rejected as duplicate
            result2 = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id=test_message_data["message_id"],  # Same message ID
                user_name=test_message_data["user_name"]
            )

            # Verify duplicate was detected
            assert result2["proceed"] is False
            assert "Duplicate" in result2["reason"]
            assert "Already Processed" in result2["reason"]
            assert result2["webhook_event_id"] == webhook_event_id  # Same event ID

            # Verify no new webhook event was created
            cur.execute(
                "SELECT COUNT(*) as count FROM webhook_events WHERE whatsapp_message_id = %s",
                (test_message_data["message_id"],)
            )
            count = cur.fetchone()["count"]
            assert count == 1, "Only one webhook event should exist"

    @pytest.mark.integration
    def test_failed_message_can_be_retried(
        self,
        clean_db,
        db_with_autocommit,
        test_message_data,
        seed_test_data,
        step1_module,
        mock_wmill
    ):
        """
        GOAL: Message that failed previously can be retried successfully
        GIVEN: A message that failed processing
        WHEN: The same message ID is received again
        THEN:
        - Step 1 allows retry
        - Status is updated from 'failed' to 'processing'
        - Processing continues normally
        """
        cur = db_with_autocommit

        with patch('wmill.get_resource', mock_wmill.get_resource):
            # First attempt - succeeds
            result1 = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id="wamid.retry.test.001",
                user_name=test_message_data["user_name"]
            )

            assert result1["proceed"] is True
            webhook_event_id = result1["webhook_event_id"]

            # Mark as failed
            cur.execute(
                """
                UPDATE webhook_events
                SET status = 'failed',
                    error_message = 'LLM timeout',
                    processed_at = NOW()
                WHERE id = %s
                """,
                (webhook_event_id,)
            )

            # Retry - should be allowed
            result2 = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id="wamid.retry.test.001",  # Same message ID
                user_name=test_message_data["user_name"]
            )

            # Verify retry was allowed
            assert result2["proceed"] is True
            assert result2["webhook_event_id"] == webhook_event_id  # Same event ID

            # Verify status was updated to processing
            cur.execute(
                "SELECT status FROM webhook_events WHERE id = %s",
                (webhook_event_id,)
            )
            event = cur.fetchone()
            assert event["status"] == "processing", "Status should be updated to processing on retry"

    @pytest.mark.integration
    def test_currently_processing_message_rejected(
        self,
        clean_db,
        db_with_autocommit,
        test_message_data,
        seed_test_data,
        step1_module,
        mock_wmill
    ):
        """
        GOAL: Message currently being processed should be rejected
        GIVEN: A message with status 'processing'
        WHEN: The same message ID is received again
        THEN: Step 1 returns proceed=False with "Currently Processing" reason
        """
        cur = db_with_autocommit

        with patch('wmill.get_resource', mock_wmill.get_resource):
            # First attempt
            result1 = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id="wamid.concurrent.test.001",
                user_name=test_message_data["user_name"]
            )

            assert result1["proceed"] is True
            assert result1.get("webhook_event_id") is not None

            # Second concurrent attempt
            result2 = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id="wamid.concurrent.test.001",  # Same message ID
                user_name=test_message_data["user_name"]
            )

            # Verify concurrent request was rejected
            assert result2["proceed"] is False
            assert "Duplicate" in result2["reason"]
            assert "Currently Processing" in result2["reason"]


# ============================================================================
# QUOTA ENFORCEMENT TESTS
# ============================================================================

class TestQuotaEnforcement:
    """Test message and token quota enforcement"""

    @pytest.mark.integration
    def test_message_rejected_when_quota_exceeded(
        self,
        clean_db,
        db_with_autocommit,
        test_message_data,
        seed_test_data,
        step1_module,
        step5__module,
        mock_wmill
    ):
        """
        GOAL: User over message quota gets rejection and no usage logged
        GIVEN: Organization has reached message limit
        WHEN: User sends a message
        THEN:
        - Step 1 returns proceed=False with "Usage Limit Exceeded"
        - No usage is logged
        """
        cur = db_with_autocommit

        # Set organization to have very low limits
        cur.execute("""
            UPDATE organizations
            SET message_limit_monthly = 1,
                token_limit_monthly = 1000000
            WHERE id = %s
        """, (seed_test_data["org_id"],))

        # Log one message to exhaust quota
        cur.execute("""
            INSERT INTO usage_logs (
                organization_id,
                chatbot_id,
                contact_id,
                message_count,
                tokens_input,
                tokens_output,
                tokens_total,
                model_name,
                provider,
                estimated_cost_usd,
                date_bucket
            ) VALUES (%s, %s, %s, 1, 100, 50, 150, 'test-model', 'test', 0.001, CURRENT_DATE)
        """, (seed_test_data["org_id"], seed_test_data["chatbot_id"], seed_test_data["contact_id"]))

        # Update usage summary
        cur.execute("""
            INSERT INTO usage_summary (
                organization_id,
                current_period_messages,
                current_period_tokens,
                period_start,
                period_end,
                last_updated_at
            )
            SELECT
                id,
                1,
                150,
                billing_period_start,
                billing_period_end,
                NOW()
            FROM organizations
            WHERE id = %s
            ON CONFLICT (organization_id)
            DO UPDATE SET
                current_period_messages = 1,
                current_period_tokens = 150,
                last_updated_at = NOW()
        """, (seed_test_data["org_id"],))

        with patch('wmill.get_resource', mock_wmill.get_resource):
            # Attempt to send another message
            result = step1_module.main(
                whatsapp_phone_id=test_message_data["whatsapp_phone_id"],
                user_phone=test_message_data["user_phone"],
                message_id="wamid.quota.exceeded.test",
                user_name=test_message_data["user_name"]
            )

            # Verify quota rejection
            assert result["proceed"] is False
            assert "Usage Limit Exceeded" in result["reason"]
            assert "usage_info" in result
            assert result["usage_info"]["has_quota"] is False
            assert result["usage_info"]["limit_type"] == "messages"

            # Verify webhook event was marked as failed
            cur.execute(
                "SELECT * FROM webhook_events WHERE whatsapp_message_id = %s",
                ("wamid.quota.exceeded.test",)
            )
            event = cur.fetchone()
            assert event is not None
            assert event["status"] == "failed"
            assert "Usage limit exceeded" in event["error_message"]

    @pytest.mark.integration
    def test_usage_correctly_increments_after_success(
        self,
        clean_db,
        db_with_autocommit,
        test_message_data,
        seed_test_data,
        step3_3_module,
        mock_wmill
    ):
        """
        GOAL: After successful message, usage counters are updated
        GIVEN: A successful message flow
        WHEN: Step 3.3 logs usage
        THEN:
        - usage_logs gets new entry
        - usage_summary is incremented correctly
        """
        cur = db_with_autocommit

        # Get initial usage
        cur.execute(
            "SELECT * FROM usage_summary WHERE organization_id = %s",
            (seed_test_data["org_id"],)
        )
        initial_summary = cur.fetchone()
        initial_messages = initial_summary["current_period_messages"] if initial_summary else 0
        initial_tokens = initial_summary["current_period_tokens"] if initial_summary else 0

        # Create mock context and results
        context_payload = {
            "proceed": True,
            "chatbot": {
                "organization_id": seed_test_data["org_id"],
                "id": seed_test_data["chatbot_id"],
                "model_name": "gemini-pro"
            },
            "user": {
                "id": seed_test_data["contact_id"]
            }
        }

        llm_result = {
            "reply_text": "Test response",
            "usage_info": {
                "provider": "google",
                "model": "gemini-pro",
                "tokens_input": 200,
                "tokens_output": 100
            }
        }

        send_result = {"success": True}

        with patch('wmill.get_resource', mock_wmill.get_resource):
            # Log usage
            usage_result = step3_3_module.main(
                context_payload=context_payload,
                llm_result=llm_result,
                send_result=send_result
            )

            assert usage_result["success"] is True
            assert usage_result["tokens_used"] == 300  # 200 + 100
            assert usage_result["message_count"] == 1

            # Verify usage_summary was updated
            cur.execute(
                "SELECT * FROM usage_summary WHERE organization_id = %s",
                (seed_test_data["org_id"],)
            )
            updated_summary = cur.fetchone()

            assert updated_summary["current_period_messages"] == initial_messages + 1
            assert updated_summary["current_period_tokens"] == initial_tokens + 300


# ============================================================================
# RAG FLOW TESTS
# ============================================================================

class TestRAGFlow:
    """Test RAG (Retrieval-Augmented Generation) flow"""

    @pytest.mark.integration
    def test_rag_retrieval_included_in_context(
        self,
        clean_db,
        db_with_autocommit,
        test_message_data,
        seed_test_data,
        step2_module,
        mock_wmill
    ):
        """
        GOAL: When RAG is enabled, relevant chunks are retrieved and included
        GIVEN: Chatbot with RAG enabled and knowledge base populated
        WHEN: User sends a message
        THEN:
        - Relevant chunks are retrieved
        - Chunks are included in LLM context
        - Usage info indicates RAG was used
        """
        cur = db_with_autocommit

        # Enable RAG for chatbot
        cur.execute("""
            UPDATE chatbots
            SET rag_enabled = true
            WHERE id = %s
        """, (seed_test_data["chatbot_id"],))

        # Add knowledge base content
        cur.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                name,
                source_type,
                sync_status
            ) VALUES (%s, 'Product Guide', 'pdf', 'synced')
            RETURNING id
        """, (seed_test_data["chatbot_id"],))
        source = cur.fetchone()
        source_id = source["id"]

        # Add a chunk with embedding (using a simple mock embedding)
        # Note: In a real test, you'd use actual embeddings from OpenAI
        mock_embedding = [0.1] * 1536  # 1536-dimensional vector

        cur.execute("""
            INSERT INTO document_chunks (
                knowledge_source_id,
                chatbot_id,
                content,
                chunk_index,
                embedding,
                metadata
            ) VALUES (%s, %s, %s, 0, %s, '{"page": 1}')
        """, (
            source_id,
            seed_test_data["chatbot_id"],
            "Our return policy allows returns within 30 days of purchase.",
            mock_embedding
        ))

        # Create context with RAG enabled
        context_payload = {
            "proceed": True,
            "chatbot": {
                "id": seed_test_data["chatbot_id"],
                "organization_id": seed_test_data["org_id"],
                "name": "Test Bot",
                "system_prompt": "You are a helpful assistant.",
                "persona": "",
                "model_name": "gemini-pro",
                "temperature": 0.7,
                "wa_token": "test_token",
                "rag_config": {
                    "enabled": True
                }
            },
            "user": {
                "id": seed_test_data["contact_id"],
                "phone": test_message_data["user_phone"],
                "name": test_message_data["user_name"],
                "variables": {},
                "tags": []
            },
            "history": [],
            "tools": []
        }

        # Setup Gemini mock by patching module's namespace
        mock_client = Mock()
        mock_response = Mock()
        mock_response.text = "Based on our policy, you can return items within 30 days."
        mock_response.usage_metadata = Mock()
        mock_response.usage_metadata.prompt_token_count = 100
        mock_response.usage_metadata.candidates_token_count = 50
        mock_response.candidates = [Mock()]
        mock_response.candidates[0].content.parts = []
        mock_client.models.generate_content.return_value = mock_response
        mock_genai = Mock()
        mock_genai.Client.return_value = mock_client
        step2_module.genai = mock_genai

        # Mock OpenAI embeddings (for RAG search) by patching module's namespace
        mock_openai_client = Mock()
        mock_embedding_response = Mock()
        mock_embedding_response.data = [Mock(embedding=mock_embedding)]
        mock_openai_client.embeddings.create.return_value = mock_embedding_response
        mock_openai_class = Mock(return_value=mock_openai_client)
        step2_module.OpenAI = mock_openai_class

        with patch('wmill.get_resource', mock_wmill.get_resource), \
             patch('wmill.get_variable', mock_wmill.get_variable):

            # Call Step 2 with RAG enabled
            result = step2_module.main(
                context_payload=context_payload,
                user_message="What is your return policy?",
                openai_api_key="test_openai_key",
                google_api_key="test_google_key",
                default_provider="google"
            )

            # Verify RAG was used
            assert "usage_info" in result
            assert result["usage_info"].get("rag_used") is True
            assert result["usage_info"].get("chunks_retrieved", 0) >= 0  # May or may not retrieve based on similarity

            # If chunks were retrieved, verify they're in the response
            if result.get("retrieved_sources"):
                assert len(result["retrieved_sources"]) > 0
                assert "source_name" in result["retrieved_sources"][0]


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
File: .coverage
================================================
SQLite format 3   @     	   
                                                            	 .WJ
V � ^
O-�
&�m	��a	c�                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         �Q�}tabletracertracer
CREATE TABLE tracer (
    -- A row per file indicating the tracer used for that file.
    file_id integer primary key,
    tracer text,
    foreign key (file_id) references file (id)
)�
�etablearcarcCREATE TABLE arc (
    -- If recording branches, a row per context per from/to line transition executed.
    file_id integer,            -- foreign key to `file`.
    context_id integer,         -- foreign key to `context`.
    fromno integer,             -- line number jumped from.
    tono integer,               -- line number jumped to.
    foreign key (file_id) references file (id),
    foreign key (context_id) references context (id),
    unique (file_id, context_id, fromno, tono)
)%9 indexsqlite_autoindex_arc_1arc��qtableline_bitsline_bits	CREATE TABLE line_bits (
    -- If recording lines, a row per context per file executed.
    -- All of the line numbers for that file/context are in one numbits.
    file_id integer,            -- foreign key to `file`.
    context_id integer,         -- foreign key to `context`.
    numbits blob,               -- see the numbits functions in coverage.numbits
    foreign key (file_id) references file (id),
    foreign key (context_id) references context (id),
    unique (file_id, context_id)
)1	E indexsqlite_autoindex_line_bits_1line_bits
��	tablecontextcontextCREATE TABLE context (
    -- A row per context measured.
    id integer primary key,
    context text,
    unique (context)
)-A indexsqlite_autoindex_context_1context��qtablefilefileCREATE TABLE file (
    -- A row per file measured.
    id integer primary key,
    path text,
    unique (path)
)'; indexsqlite_autoindex_file_1file�[�tablemetametaCREATE TABLE meta (
    -- Key-value pairs, to record metadata about the data
    key text,
    value text,
    unique (key)
    -- Possible keys:
    --  'has_arcs' boolean      -- Is this data recording branches?
    --  'sys_argv' text         -- The coverage command line that recorded the data.
    --  'version' text          -- The version of coverage.py that made the file.
    --  'when' text             -- Datetime when the file was created.
)'; indexsqlite_autoindex_meta_1meta       �++�utablecoverage_schemacoverage_schemaCREATE TABLE coverage_schema (
    -- One row, to record the version of the schema in this db.
    version integer
)
   � �                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 
   � ��                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    has_arcs0version7.13.0
   � ��                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            has_arcs
	version
   
� �I��>
�
�
I��                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       N
 �!/home/jdrq/Projects/Hetzner-Workspace/src/f/development/upload_document.pyT	 �-/home/jdrq/Projects/Hetzner-Workspace/src/f/development/RAG_process_documents.pyS �+/home/jdrq/Projects/Hetzner-Workspace/src/f/development/ingest_multiple_urls.pyP �%/home/jdrq/Projects/Hetzner-Workspace/src/f/development/utils/web_crawler.pyL �/home/jdrq/Projects/Hetzner-Workspace/src/f/development/3_3_log_usage.pyT �-/home/jdrq/Projects/Hetzner-Workspace/src/f/development/3_2_save_chat_history.pyY �7/home/jdrq/Projects/Hetzner-Workspace/src/f/development/3_1_send_reply_to_whatsapp.pyX �5/home/jdrq/Projects/Hetzner-Workspace/src/f/development/2_whatsapp_llm_processing.pyY �7/home/jdrq/Projects/Hetzner-Workspace/src/f/development/1_whatsapp_context_loading.pyZ �9/home/jdrq/Projects/Hetzner-Workspace/src/f/development/utils/check_knowledge_quota.py
   
� J��?
��
J��
�                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        O�!/home/jdrq/Projects/Hetzner-Workspace/src/f/development/upload_document.py
U�-/home/jdrq/Projects/Hetzner-Workspace/src/f/development/RAG_process_documents.py	T�+/home/jdrq/Projects/Hetzner-Workspace/src/f/development/ingest_multiple_urls.pyQ�%/home/jdrq/Projects/Hetzner-Workspace/src/f/development/utils/web_crawler.pyM�/home/jdrq/Projects/Hetzner-Workspace/src/f/development/3_3_log_usage.pyU�-/home/jdrq/Projects/Hetzner-Workspace/src/f/development/3_2_save_chat_history.pyZ�7/home/jdrq/Projects/Hetzner-Workspace/src/f/development/3_1_send_reply_to_whatsapp.pyY�5/home/jdrq/Projects/Hetzner-Workspace/src/f/development/2_whatsapp_llm_processing.pyZ�7/home/jdrq/Projects/Hetzner-Workspace/src/f/development/1_whatsapp_context_loading.pyZ�9	/home/jdrq/Projects/Hetzner-Workspace/src/f/development/utils/check_knowledge_quota.py
   � �                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
   � �                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
	
   � �6(���                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      )	T ��   dwpMDd_F��ǟ�c&�� �ٓ
`		4��  ��
 �?�?��׿ȏ!	D�?��<�^�  �?  �~�y�o�y	&��7�F�!�	�\ܩϷ��
	����� �x>�a�3��s���'�%'��ӓ?�|��}�|�?��d}�7�����N�����?��  ����/�����?�?�?�� ����k{<��Y�����x����� @��U:? ���� �����98	r�? ������!�!  ��㟹����? ���	�?�<� ����4

   � �������                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           									
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              

================================================
File: CLAUDE.md
================================================
Instructions are in @AGENTS.md

================================================
File: db/migrations/002_add_usage_triggers.sql
================================================
-- Migration: Add usage quota enforcement triggers
-- Purpose: Automatically disable chatbots when usage limits are exceeded
-- Date: 2025-12-29

-- =====================================================================
-- Function: Check and enforce usage limits
-- =====================================================================
-- This function is triggered whenever usage_summary is updated
-- It checks if the organization has exceeded their monthly limits
-- If limits are exceeded, it:
-- 1. Disables all chatbots for that organization
-- 2. Creates a system alert
-- 3. Logs the event

CREATE OR REPLACE FUNCTION check_usage_limits()
RETURNS TRIGGER AS $$
DECLARE
    org_record RECORD;
    chatbots_disabled INT;
BEGIN
    -- Get organization limits
    SELECT
        id,
        name,
        message_limit_monthly,
        token_limit_monthly
    INTO org_record
    FROM organizations
    WHERE id = NEW.organization_id;

    IF NOT FOUND THEN
        RETURN NEW;
    END IF;

    -- Check message limit
    IF NEW.current_period_messages >= org_record.message_limit_monthly THEN

        -- Disable all chatbots for this organization
        UPDATE chatbots
        SET is_active = FALSE
        WHERE organization_id = NEW.organization_id
          AND is_active = TRUE
        RETURNING NULL INTO chatbots_disabled;

        GET DIAGNOSTICS chatbots_disabled = ROW_COUNT;

        -- Create alert
        INSERT INTO system_alerts (
            organization_id,
            type,
            severity,
            message,
            metadata,
            created_at
        ) VALUES (
            NEW.organization_id,
            'QUOTA_EXCEEDED',
            'critical',
            format('Message limit reached (%s/%s). %s chatbots disabled.',
                   NEW.current_period_messages,
                   org_record.message_limit_monthly,
                   chatbots_disabled
            ),
            json_build_object(
                'limit_type', 'messages',
                'current', NEW.current_period_messages,
                'limit', org_record.message_limit_monthly,
                'chatbots_disabled', chatbots_disabled,
                'organization_name', org_record.name
            ),
            NOW()
        );

        RAISE NOTICE 'Organization % exceeded message limit. % chatbots disabled.',
                     org_record.name, chatbots_disabled;

    END IF;

    -- Check token limit
    IF NEW.current_period_tokens >= org_record.token_limit_monthly THEN

        -- Disable all chatbots for this organization (if not already disabled)
        UPDATE chatbots
        SET is_active = FALSE
        WHERE organization_id = NEW.organization_id
          AND is_active = TRUE
        RETURNING NULL INTO chatbots_disabled;

        GET DIAGNOSTICS chatbots_disabled = ROW_COUNT;

        IF chatbots_disabled > 0 THEN
            -- Create alert
            INSERT INTO system_alerts (
                organization_id,
                type,
                severity,
                message,
                metadata,
                created_at
            ) VALUES (
                NEW.organization_id,
                'QUOTA_EXCEEDED',
                'critical',
                format('Token limit reached (%s/%s). %s chatbots disabled.',
                       NEW.current_period_tokens,
                       org_record.token_limit_monthly,
                       chatbots_disabled
                ),
                json_build_object(
                    'limit_type', 'tokens',
                    'current', NEW.current_period_tokens,
                    'limit', org_record.token_limit_monthly,
                    'chatbots_disabled', chatbots_disabled,
                    'organization_name', org_record.name
                ),
                NOW()
            );

            RAISE NOTICE 'Organization % exceeded token limit. % chatbots disabled.',
                         org_record.name, chatbots_disabled;
        END IF;

    END IF;

    -- Check for warning thresholds (80% of limit)
    IF NEW.current_period_messages >= (org_record.message_limit_monthly * 0.8)
       AND NEW.current_period_messages < org_record.message_limit_monthly THEN

        -- Create warning alert
        INSERT INTO system_alerts (
            organization_id,
            type,
            severity,
            message,
            metadata,
            created_at
        ) VALUES (
            NEW.organization_id,
            'QUOTA_EXCEEDED',
            'warning',
            format('Approaching message limit: %s/%s (%.0f%%)',
                   NEW.current_period_messages,
                   org_record.message_limit_monthly,
                   (NEW.current_period_messages::float / org_record.message_limit_monthly) * 100
            ),
            json_build_object(
                'limit_type', 'messages',
                'current', NEW.current_period_messages,
                'limit', org_record.message_limit_monthly,
                'percentage', (NEW.current_period_messages::float / org_record.message_limit_monthly) * 100,
                'threshold', 80
            ),
            NOW()
        )
        ON CONFLICT DO NOTHING;  -- Avoid duplicate warnings

    END IF;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- =====================================================================
-- Trigger: Enforce usage limits on usage_summary updates
-- =====================================================================

DROP TRIGGER IF EXISTS trigger_check_usage_limits ON usage_summary;

CREATE TRIGGER trigger_check_usage_limits
AFTER UPDATE ON usage_summary
FOR EACH ROW
WHEN (
    -- Only trigger if counts actually changed
    NEW.current_period_messages > OLD.current_period_messages OR
    NEW.current_period_tokens > OLD.current_period_tokens
)
EXECUTE FUNCTION check_usage_limits();

-- =====================================================================
-- Function: Re-enable chatbots when new billing period starts
-- =====================================================================
-- This should be called via a scheduled job or manually
-- It resets usage counters and re-enables chatbots for orgs that were disabled

CREATE OR REPLACE FUNCTION reset_billing_period()
RETURNS TABLE (
    organization_id UUID,
    organization_name TEXT,
    chatbots_enabled INT,
    old_message_count BIGINT,
    old_token_count BIGINT
) AS $$
DECLARE
    org RECORD;
    chatbots_count INT;
BEGIN
    -- Loop through all organizations
    FOR org IN
        SELECT o.id, o.name, o.billing_period_start, o.billing_period_end,
               us.current_period_messages, us.current_period_tokens
        FROM organizations o
        LEFT JOIN usage_summary us ON o.id = us.organization_id
        WHERE o.billing_period_end < NOW()
    LOOP
        -- Re-enable chatbots that were disabled due to quota
        UPDATE chatbots
        SET is_active = TRUE
        WHERE organization_id = org.id
          AND is_active = FALSE;

        GET DIAGNOSTICS chatbots_count = ROW_COUNT;

        -- Reset usage counters
        UPDATE usage_summary
        SET
            current_period_messages = 0,
            current_period_tokens = 0,
            period_start = NOW(),
            period_end = NOW() + INTERVAL '1 month'
        WHERE organization_id = org.id;

        -- Update organization billing period
        UPDATE organizations
        SET
            billing_period_start = NOW(),
            billing_period_end = NOW() + INTERVAL '1 month'
        WHERE id = org.id;

        -- Create info alert about reset
        INSERT INTO system_alerts (
            organization_id,
            type,
            severity,
            message,
            metadata,
            created_at
        ) VALUES (
            org.id,
            'OTHER',
            'info',
            format('Billing period reset. Previous usage: %s messages, %s tokens. %s chatbots re-enabled.',
                   org.current_period_messages,
                   org.current_period_tokens,
                   chatbots_count
            ),
            json_build_object(
                'previous_messages', org.current_period_messages,
                'previous_tokens', org.current_period_tokens,
                'chatbots_enabled', chatbots_count,
                'new_period_start', NOW(),
                'new_period_end', NOW() + INTERVAL '1 month'
            ),
            NOW()
        );

        -- Return results
        organization_id := org.id;
        organization_name := org.name;
        chatbots_enabled := chatbots_count;
        old_message_count := org.current_period_messages;
        old_token_count := org.current_period_tokens;

        RETURN NEXT;
    END LOOP;

    RETURN;
END;
$$ LANGUAGE plpgsql;

-- =====================================================================
-- Comments
-- =====================================================================

COMMENT ON FUNCTION check_usage_limits() IS 'Automatically enforces message and token limits by disabling chatbots when quotas are exceeded';
COMMENT ON FUNCTION reset_billing_period() IS 'Resets usage counters and re-enables chatbots when billing period ends (should be run monthly via cron)';
COMMENT ON TRIGGER trigger_check_usage_limits ON usage_summary IS 'Enforces usage quotas in real-time as messages are processed';

-- =====================================================================
-- Usage Examples
-- =====================================================================

-- To manually reset billing period for all organizations:
-- SELECT * FROM reset_billing_period();

-- To check current usage for an organization:
-- SELECT
--     o.name,
--     us.current_period_messages,
--     o.message_limit_monthly,
--     us.current_period_tokens,
--     o.token_limit_monthly,
--     (us.current_period_messages::float / o.message_limit_monthly) * 100 as message_usage_percent,
--     (us.current_period_tokens::float / o.token_limit_monthly) * 100 as token_usage_percent
-- FROM organizations o
-- JOIN usage_summary us ON o.id = us.organization_id
-- WHERE o.id = 'your-org-id';


================================================
File: tests/integration/README.md
================================================
# Integration Tests for WhatsApp Chatbot Flow

## Overview

The integration tests in `test_full_flow.py` verify the end-to-end behavior of the WhatsApp chatbot system across all processing steps:

1. **Step 1**: Context loading - validates chatbot, loads user data, checks quotas
2. **Step 2**: LLM processing - calls Gemini/OpenAI, handles tool calls
3. **Step 3.1**: Send reply to WhatsApp
4. **Step 3.2**: Save messages to database
5. **Step 3.3**: Log token usage

## Test Categories

### 1. Happy Path Tests (`TestCompleteFlow`)

**test_complete_message_flow_success**
- Tests the complete flow from incoming message to saved history
- Verifies all steps execute successfully
- Checks database state after each step
- Validates message delivery, history saving, and usage logging

### 2. Error Propagation Tests (`TestErrorPropagation`)

**test_step1_failure_stops_flow**
- Tests behavior when chatbot is not found
- Verifies downstream steps handle Step 1 failure gracefully
- Checks that no data is saved when Step 1 fails

**test_step2_llm_error_handled**
- Tests LLM failure handling
- Verifies fallback message is returned
- Checks that appropriate error responses are generated

**test_step3_whatsapp_failure_prevents_history_save**
- Tests WhatsApp API failure handling
- **CRITICAL**: Verifies messages are NOT saved when delivery fails
- Ensures no usage is charged when message not delivered

### 3. Idempotency Tests (`TestIdempotency`)

**test_duplicate_message_detected**
- Tests duplicate message detection
- Verifies same message ID is rejected on second attempt
- Checks webhook event status is "completed"

**test_failed_message_can_be_retried**
- Tests retry mechanism for failed messages
- Verifies failed messages can be reprocessed
- Checks status transitions from "failed" to "processing"

**test_currently_processing_message_rejected**
- Tests concurrent request handling
- Verifies messages currently processing are rejected

### 4. Quota Enforcement Tests (`TestQuotaEnforcement`)

**test_message_rejected_when_quota_exceeded**
- Tests message quota enforcement
- Verifies users over quota are rejected
- Checks webhook event is marked as "failed"

**test_usage_correctly_increments_after_success**
- Tests usage counter updates
- Verifies usage_logs and usage_summary tables are updated
- Checks token and message counts are accurate

### 5. RAG Flow Tests (`TestRAGFlow`)

**test_rag_retrieval_included_in_context**
- Tests knowledge base retrieval
- Verifies relevant chunks are retrieved
- Checks RAG usage is tracked in usage_info

## Running the Tests

### Run All Integration Tests
```bash
pytest tests/integration/test_full_flow.py -v
```

### Run Specific Test Class
```bash
pytest tests/integration/test_full_flow.py::TestCompleteFlow -v
```

### Run Single Test
```bash
pytest tests/integration/test_full_flow.py::TestIdempotency::test_duplicate_message_detected -v
```

### Run with Detailed Output
```bash
pytest tests/integration/test_full_flow.py -xvs
```

## Current Test Status

**Passing Tests (2/10):**
- ✅ test_step1_failure_stops_flow
- ✅ test_duplicate_message_detected

**Known Issues:**

### 1. Google Gemini Mocking Issue
**Affected Tests:**
- test_complete_message_flow_success
- test_step2_llm_error_handled
- test_step3_whatsapp_failure_prevents_history_save

**Error:** `AttributeError: module 'google' has no attribute 'genai'`

**Cause:** The `google.genai` module needs to be properly mocked before importing Step 2. The current mocking strategy doesn't fully cover the new Google Gemini SDK.

**Fix:** Need to mock the entire `google.genai.Client` and `google.genai.types` modules before running tests that use Step 2.

### 2. Database Deadlock Issues
**Affected Tests:**
- test_currently_processing_message_rejected
- test_rag_retrieval_included_in_context

**Error:** `psycopg2.errors.DeadlockDetected: deadlock detected`

**Cause:** Multiple tests running in parallel are accessing the database simultaneously, causing lock conflicts when `clean_db` fixture tries to drop/create tables.

**Fix:** Force pytest to run integration tests sequentially using `-n 1` or pytest-xdist settings.

### 3. Database Schema Issues
**Affected Tests:**
- test_failed_message_can_be_retried
- test_message_rejected_when_quota_exceeded

**Error:** `psycopg2.errors.UndefinedTable: relation "webhook_events" does not exist`

**Cause:** The `clean_db` fixture is being called multiple times in parallel, causing race conditions where one test drops tables while another is trying to use them.

**Fix:** Use scoped fixtures or run tests sequentially.

### 4. Unique Constraint Violation
**Affected Tests:**
- test_usage_correctly_increments_after_success

**Error:** `duplicate key value violates unique constraint "pg_type_typname_nsp_index"`

**Cause:** Database schema corruption from concurrent `DROP/CREATE` operations.

**Fix:** Run tests sequentially or use database transactions with rollback.

## Recommended Fixes

### Short-term Solutions

1. **Run tests sequentially:**
   ```bash
   pytest tests/integration/test_full_flow.py -v --dist=no
   ```

2. **Mock google.genai properly:**
   Add to conftest.py:
   ```python
   @pytest.fixture(autouse=True)
   def mock_google_genai():
       with patch('google.genai') as mock:
           yield mock
   ```

3. **Use separate test databases:**
   Each test should use its own database to avoid conflicts.

### Long-term Solutions

1. **Use database transactions with rollback:**
   - Change `clean_db` to use transactions instead of DROP/CREATE
   - Each test runs in a transaction that gets rolled back
   - Much faster and no race conditions

2. **Improve mocking strategy:**
   - Create comprehensive mock fixtures for all external dependencies
   - Use dependency injection to make testing easier
   - Consider using factory patterns for test data

3. **Add test isolation:**
   - Ensure each test is truly independent
   - Use unique IDs for test data (e.g., UUID-based)
   - Clear all caches between tests

## Test Data

### Default Test Data (from seed.sql)
- **Organization:** JD Labs Corporation
- **Chatbot:** Test WhatsApp Bot (phone_id: test_phone_123)
- **User:** Integration Test User (phone: 15559876543)
- **Message ID:** wamid.integration.test.001

### Fixtures Available
- `test_message_data`: Standard message payload
- `seed_test_data`: Returns org_id, chatbot_id, contact_id
- `step1_module`, `step2_module`, etc.: Imported step modules with mocks
- `mock_wmill`: Windmill SDK mock
- `mock_llm`: LLM provider mocks
- `mock_whatsapp`: WhatsApp API mock

## Next Steps

To make these tests production-ready:

1. ✅ **Fix google.genai mocking** - Update import strategy
2. ✅ **Enable sequential execution** - Add pytest.ini config
3. ✅ **Use transaction-based cleanup** - Modify clean_db fixture
4. ⬜ **Add more edge cases** - Test network failures, timeouts
5. ⬜ **Add performance tests** - Measure response times
6. ⬜ **Add concurrency tests** - Test parallel message processing
7. ⬜ **Add end-to-end webhook tests** - Test full webhook flow

## Architecture Notes

### Why Integration Tests Matter

These tests are critical because they verify:
- **Data consistency** across multiple database writes
- **Error handling** propagates correctly through the pipeline
- **Idempotency** prevents duplicate processing
- **Quota enforcement** prevents abuse
- **Transaction boundaries** ensure data integrity

### Key Behaviors Tested

1. **Atomicity**: If WhatsApp delivery fails, no history is saved
2. **Idempotency**: Duplicate messages are rejected
3. **Retry logic**: Failed messages can be retried
4. **Quota enforcement**: Usage limits are enforced before processing
5. **Error propagation**: Failures at any step prevent downstream execution

## Contributing

When adding new integration tests:

1. Follow the AAA pattern (Arrange, Act, Assert)
2. Use descriptive test names (test_<behavior>_<scenario>_<expected>)
3. Add docstrings explaining GOAL, GIVEN, WHEN, THEN
4. Clean up test data in fixture teardown
5. Use fixtures for common setup
6. Mock external dependencies (LLM, WhatsApp, etc.)
7. Verify database state changes
8. Check both success and error paths


================================================
File: db/migrations/001_add_leads_table.sql
================================================
-- Migration 001: Add leads table for MCP lead capture
-- This table stores potential customers captured by the sales chatbot

CREATE TABLE IF NOT EXISTS leads (
    id BIGSERIAL PRIMARY KEY,
    chatbot_id UUID REFERENCES chatbots(id) ON DELETE SET NULL,

    -- Contact information
    phone VARCHAR(50) NOT NULL,
    name VARCHAR(255),
    email VARCHAR(255),
    company VARCHAR(255),

    -- Sales information
    estimated_messages INTEGER,
    notes TEXT,

    -- Tracking
    contact_count INTEGER DEFAULT 1,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    -- Indexes for performance
    CONSTRAINT leads_phone_check CHECK (phone IS NOT NULL AND phone != '')
);

-- Index for fast lookup by phone
CREATE INDEX idx_leads_phone ON leads(phone);

-- Index for filtering by chatbot
CREATE INDEX idx_leads_chatbot_id ON leads(chatbot_id);

-- Index for sorting by creation date
CREATE INDEX idx_leads_created_at ON leads(created_at DESC);

-- Comment on table
COMMENT ON TABLE leads IS 'Stores potential customers captured through chatbot interactions';
COMMENT ON COLUMN leads.contact_count IS 'Number of times this lead has interacted with the chatbot';
COMMENT ON COLUMN leads.estimated_messages IS 'Estimated monthly message volume provided by the lead';


================================================
File: htmlcov/coverage_html_cb_bcae5fc4.js
================================================
// Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0
// For details: https://github.com/coveragepy/coveragepy/blob/main/NOTICE.txt

// Coverage.py HTML report browser code.
/*jslint browser: true, sloppy: true, vars: true, plusplus: true, maxerr: 50, indent: 4 */
/*global coverage: true, document, window, $ */

coverage = {};

// General helpers
function debounce(callback, wait) {
    let timeoutId = null;
    return function(...args) {
        clearTimeout(timeoutId);
        timeoutId = setTimeout(() => {
            callback.apply(this, args);
        }, wait);
    };
};

function checkVisible(element) {
    const rect = element.getBoundingClientRect();
    const viewBottom = Math.max(document.documentElement.clientHeight, window.innerHeight);
    const viewTop = 30;
    return !(rect.bottom < viewTop || rect.top >= viewBottom);
}

function on_click(sel, fn) {
    const elt = document.querySelector(sel);
    if (elt) {
        elt.addEventListener("click", fn);
    }
}

// Helpers for table sorting
function getCellValue(row, column = 0) {
    const cell = row.cells[column]  // nosemgrep: eslint.detect-object-injection
    if (cell.childElementCount == 1) {
        var child = cell.firstElementChild;
        if (child.tagName === "A") {
            child = child.firstElementChild;
        }
        if (child instanceof HTMLDataElement && child.value) {
            return child.value;
        }
    }
    return cell.innerText || cell.textContent;
}

function rowComparator(rowA, rowB, column = 0) {
    let valueA = getCellValue(rowA, column);
    let valueB = getCellValue(rowB, column);
    if (!isNaN(valueA) && !isNaN(valueB)) {
        return valueA - valueB;
    }
    return valueA.localeCompare(valueB, undefined, {numeric: true});
}

function sortColumn(th) {
    // Get the current sorting direction of the selected header,
    // clear state on other headers and then set the new sorting direction.
    const currentSortOrder = th.getAttribute("aria-sort");
    [...th.parentElement.cells].forEach(header => header.setAttribute("aria-sort", "none"));
    var direction;
    if (currentSortOrder === "none") {
        direction = th.dataset.defaultSortOrder || "ascending";
    }
    else if (currentSortOrder === "ascending") {
        direction = "descending";
    }
    else {
        direction = "ascending";
    }
    th.setAttribute("aria-sort", direction);

    const column = [...th.parentElement.cells].indexOf(th)

    // Sort all rows and afterwards append them in order to move them in the DOM.
    Array.from(th.closest("table").querySelectorAll("tbody tr"))
        .sort((rowA, rowB) => rowComparator(rowA, rowB, column) * (direction === "ascending" ? 1 : -1))
        .forEach(tr => tr.parentElement.appendChild(tr));

    // Save the sort order for next time.
    if (th.id !== "region") {
        let th_id = "file";  // Sort by file if we don't have a column id
        let current_direction = direction;
        const stored_list = localStorage.getItem(coverage.INDEX_SORT_STORAGE);
        if (stored_list) {
            ({th_id, direction} = JSON.parse(stored_list))
        }
        localStorage.setItem(coverage.INDEX_SORT_STORAGE, JSON.stringify({
            "th_id": th.id,
            "direction": current_direction
        }));
        if (th.id !== th_id || document.getElementById("region")) {
            // Sort column has changed, unset sorting by function or class.
            localStorage.setItem(coverage.SORTED_BY_REGION, JSON.stringify({
                "by_region": false,
                "region_direction": current_direction
            }));
        }
    }
    else {
        // Sort column has changed to by function or class, remember that.
        localStorage.setItem(coverage.SORTED_BY_REGION, JSON.stringify({
            "by_region": true,
            "region_direction": direction
        }));
    }
}

// Find all the elements with data-shortcut attribute, and use them to assign a shortcut key.
coverage.assign_shortkeys = function () {
    document.querySelectorAll("[data-shortcut]").forEach(element => {
        document.addEventListener("keypress", event => {
            if (event.target.tagName.toLowerCase() === "input") {
                return; // ignore keypress from search filter
            }
            if (event.key === element.dataset.shortcut) {
                element.click();
            }
        });
    });
};

// Create the events for the filter box.
coverage.wire_up_filter = function () {
    // Populate the filter and hide100 inputs if there are saved values for them.
    const saved_filter_value = localStorage.getItem(coverage.FILTER_STORAGE);
    if (saved_filter_value) {
        document.getElementById("filter").value = saved_filter_value;
    }
    const saved_hide100_value = localStorage.getItem(coverage.HIDE100_STORAGE);
    if (saved_hide100_value) {
        document.getElementById("hide100").checked = JSON.parse(saved_hide100_value);
    }

    // Cache elements.
    const table = document.querySelector("table.index");
    const table_body_rows = table.querySelectorAll("tbody tr");
    const no_rows = document.getElementById("no_rows");

    const footer = table.tFoot.rows[0];
    const ratio_columns = Array.from(footer.cells).map(cell => Boolean(cell.dataset.ratio));

    // Observe filter keyevents.
    const filter_handler = (event => {
        // Keep running total of each metric, first index contains number of shown rows
        const totals = ratio_columns.map(
            is_ratio => is_ratio ? {"numer": 0, "denom": 0} : 0
        );

        var text = document.getElementById("filter").value;
        // Store filter value
        localStorage.setItem(coverage.FILTER_STORAGE, text);
        const casefold = (text === text.toLowerCase());
        const hide100 = document.getElementById("hide100").checked;
        // Store hide value.
        localStorage.setItem(coverage.HIDE100_STORAGE, JSON.stringify(hide100));

        // Hide / show elements.
        table_body_rows.forEach(row => {
            var show = false;
            // Check the text filter.
            for (let column = 0; column < totals.length; column++) {
                cell = row.cells[column];
                if (cell.classList.contains("name")) {
                    var celltext = cell.textContent;
                    if (casefold) {
                        celltext = celltext.toLowerCase();
                    }
                    if (celltext.includes(text)) {
                        show = true;
                    }
                }
            }

            // Check the "hide covered" filter.
            if (show && hide100) {
                const [numer, denom] = row.cells[row.cells.length - 1].dataset.ratio.split(" ");
                show = (numer !== denom);
            }

            if (!show) {
                // hide
                row.classList.add("hidden");
                return;
            }

            // show
            row.classList.remove("hidden");
            totals[0]++;

            for (let column = 0; column < totals.length; column++) {
                // Accumulate dynamic totals
                cell = row.cells[column]  // nosemgrep: eslint.detect-object-injection
                if (cell.matches(".name, .spacer")) {
                    continue;
                }
                if (ratio_columns[column] && cell.dataset.ratio) {
                    // Column stores a ratio
                    const [numer, denom] = cell.dataset.ratio.split(" ");
                    totals[column]["numer"] += parseInt(numer, 10);  // nosemgrep: eslint.detect-object-injection
                    totals[column]["denom"] += parseInt(denom, 10);  // nosemgrep: eslint.detect-object-injection
                }
                else {
                    totals[column] += parseInt(cell.textContent, 10);  // nosemgrep: eslint.detect-object-injection
                }
            }
        });

        // Show placeholder if no rows will be displayed.
        if (!totals[0]) {
            // Show placeholder, hide table.
            no_rows.style.display = "block";
            table.style.display = "none";
            return;
        }

        // Hide placeholder, show table.
        no_rows.style.display = null;
        table.style.display = null;

        // Calculate new dynamic sum values based on visible rows.
        for (let column = 0; column < totals.length; column++) {
            // Get footer cell element.
            const cell = footer.cells[column];  // nosemgrep: eslint.detect-object-injection
            if (cell.matches(".name, .spacer")) {
                continue;
            }

            // Set value into dynamic footer cell element.
            if (ratio_columns[column]) {
                // Percentage column uses the numerator and denominator,
                // and adapts to the number of decimal places.
                const match = /\.([0-9]+)/.exec(cell.textContent);
                const places = match ? match[1].length : 0;
                const { numer, denom } = totals[column];  // nosemgrep: eslint.detect-object-injection
                cell.dataset.ratio = `${numer} ${denom}`;
                // Check denom to prevent NaN if filtered files contain no statements
                cell.textContent = denom
                    ? `${(numer * 100 / denom).toFixed(places)}%`
                    : `${(100).toFixed(places)}%`;
            }
            else {
                cell.textContent = totals[column];  // nosemgrep: eslint.detect-object-injection
            }
        }
    });

    document.getElementById("filter").addEventListener("input", debounce(filter_handler));
    document.getElementById("hide100").addEventListener("input", debounce(filter_handler));

    // Trigger change event on setup, to force filter on page refresh
    // (filter value may still be present).
    document.getElementById("filter").dispatchEvent(new Event("input"));
    document.getElementById("hide100").dispatchEvent(new Event("input"));
};
coverage.FILTER_STORAGE = "COVERAGE_FILTER_VALUE";
coverage.HIDE100_STORAGE = "COVERAGE_HIDE100_VALUE";

// Set up the click-to-sort columns.
coverage.wire_up_sorting = function () {
    document.querySelectorAll("[data-sortable] th[aria-sort]").forEach(
        th => th.addEventListener("click", e => sortColumn(e.target))
    );

    // Look for a localStorage item containing previous sort settings:
    let th_id = "file", direction = "ascending";
    const stored_list = localStorage.getItem(coverage.INDEX_SORT_STORAGE);
    if (stored_list) {
        ({th_id, direction} = JSON.parse(stored_list));
    }
    let by_region = false, region_direction = "ascending";
    const sorted_by_region = localStorage.getItem(coverage.SORTED_BY_REGION);
    if (sorted_by_region) {
        ({
            by_region,
            region_direction
        } = JSON.parse(sorted_by_region));
    }

    const region_id = "region";
    if (by_region && document.getElementById(region_id)) {
        direction = region_direction;
    }
    // If we are in a page that has a column with id of "region", sort on
    // it if the last sort was by function or class.
    let th;
    if (document.getElementById(region_id)) {
        th = document.getElementById(by_region ? region_id : th_id);
    }
    else {
        th = document.getElementById(th_id);
    }
    th.setAttribute("aria-sort", direction === "ascending" ? "descending" : "ascending");
    th.click()
};

coverage.INDEX_SORT_STORAGE = "COVERAGE_INDEX_SORT_2";
coverage.SORTED_BY_REGION = "COVERAGE_SORT_REGION";

// Loaded on index.html
coverage.index_ready = function () {
    coverage.assign_shortkeys();
    coverage.wire_up_filter();
    coverage.wire_up_sorting();

    on_click(".button_prev_file", coverage.to_prev_file);
    on_click(".button_next_file", coverage.to_next_file);

    on_click(".button_show_hide_help", coverage.show_hide_help);
};

// -- pyfile stuff --

coverage.LINE_FILTERS_STORAGE = "COVERAGE_LINE_FILTERS";

coverage.pyfile_ready = function () {
    // If we're directed to a particular line number, highlight the line.
    var frag = location.hash;
    if (frag.length > 2 && frag[1] === "t") {
        document.querySelector(frag).closest(".n").classList.add("highlight");
        coverage.set_sel(parseInt(frag.substr(2), 10));
    }
    else {
        coverage.set_sel(0);
    }

    on_click(".button_toggle_run", coverage.toggle_lines);
    on_click(".button_toggle_mis", coverage.toggle_lines);
    on_click(".button_toggle_exc", coverage.toggle_lines);
    on_click(".button_toggle_par", coverage.toggle_lines);

    on_click(".button_next_chunk", coverage.to_next_chunk_nicely);
    on_click(".button_prev_chunk", coverage.to_prev_chunk_nicely);
    on_click(".button_top_of_page", coverage.to_top);
    on_click(".button_first_chunk", coverage.to_first_chunk);

    on_click(".button_prev_file", coverage.to_prev_file);
    on_click(".button_next_file", coverage.to_next_file);
    on_click(".button_to_index", coverage.to_index);

    on_click(".button_show_hide_help", coverage.show_hide_help);

    coverage.filters = undefined;
    try {
        coverage.filters = localStorage.getItem(coverage.LINE_FILTERS_STORAGE);
    } catch(err) {}

    if (coverage.filters) {
        coverage.filters = JSON.parse(coverage.filters);
    }
    else {
        coverage.filters = {run: false, exc: true, mis: true, par: true};
    }

    for (cls in coverage.filters) {
        coverage.set_line_visibilty(cls, coverage.filters[cls]);  // nosemgrep: eslint.detect-object-injection
    }

    coverage.assign_shortkeys();
    coverage.init_scroll_markers();
    coverage.wire_up_sticky_header();

    document.querySelectorAll("[id^=ctxs]").forEach(
        cbox => cbox.addEventListener("click", coverage.expand_contexts)
    );

    // Rebuild scroll markers when the window height changes.
    window.addEventListener("resize", coverage.build_scroll_markers);
};

coverage.toggle_lines = function (event) {
    const btn = event.target.closest("button");
    const category = btn.value
    const show = !btn.classList.contains("show_" + category);
    coverage.set_line_visibilty(category, show);
    coverage.build_scroll_markers();
    coverage.filters[category] = show;
    try {
        localStorage.setItem(coverage.LINE_FILTERS_STORAGE, JSON.stringify(coverage.filters));
    } catch(err) {}
};

coverage.set_line_visibilty = function (category, should_show) {
    const cls = "show_" + category;
    const btn = document.querySelector(".button_toggle_" + category);
    if (btn) {
        if (should_show) {
            document.querySelectorAll("#source ." + category).forEach(e => e.classList.add(cls));
            btn.classList.add(cls);
        }
        else {
            document.querySelectorAll("#source ." + category).forEach(e => e.classList.remove(cls));
            btn.classList.remove(cls);
        }
    }
};

// Return the nth line div.
coverage.line_elt = function (n) {
    return document.getElementById("t" + n)?.closest("p");
};

// Set the selection.  b and e are line numbers.
coverage.set_sel = function (b, e) {
    // The first line selected.
    coverage.sel_begin = b;
    // The next line not selected.
    coverage.sel_end = (e === undefined) ? b+1 : e;
};

coverage.to_top = function () {
    coverage.set_sel(0, 1);
    coverage.scroll_window(0);
};

coverage.to_first_chunk = function () {
    coverage.set_sel(0, 1);
    coverage.to_next_chunk();
};

coverage.to_prev_file = function () {
    window.location = document.getElementById("prevFileLink").href;
}

coverage.to_next_file = function () {
    window.location = document.getElementById("nextFileLink").href;
}

coverage.to_index = function () {
    location.href = document.getElementById("indexLink").href;
}

coverage.show_hide_help = function () {
    const helpCheck = document.getElementById("help_panel_state")
    helpCheck.checked = !helpCheck.checked;
}

// Return a string indicating what kind of chunk this line belongs to,
// or null if not a chunk.
coverage.chunk_indicator = function (line_elt) {
    const classes = line_elt?.className;
    if (!classes) {
        return null;
    }
    const match = classes.match(/\bshow_\w+\b/);
    if (!match) {
        return null;
    }
    return match[0];
};

coverage.to_next_chunk = function () {
    const c = coverage;

    // Find the start of the next colored chunk.
    var probe = c.sel_end;
    var chunk_indicator, probe_line;
    while (true) {
        probe_line = c.line_elt(probe);
        if (!probe_line) {
            return;
        }
        chunk_indicator = c.chunk_indicator(probe_line);
        if (chunk_indicator) {
            break;
        }
        probe++;
    }

    // There's a next chunk, `probe` points to it.
    var begin = probe;

    // Find the end of this chunk.
    var next_indicator = chunk_indicator;
    while (next_indicator === chunk_indicator) {
        probe++;
        probe_line = c.line_elt(probe);
        next_indicator = c.chunk_indicator(probe_line);
    }
    c.set_sel(begin, probe);
    c.show_selection();
};

coverage.to_prev_chunk = function () {
    const c = coverage;

    // Find the end of the prev colored chunk.
    var probe = c.sel_begin-1;
    var probe_line = c.line_elt(probe);
    if (!probe_line) {
        return;
    }
    var chunk_indicator = c.chunk_indicator(probe_line);
    while (probe > 1 && !chunk_indicator) {
        probe--;
        probe_line = c.line_elt(probe);
        if (!probe_line) {
            return;
        }
        chunk_indicator = c.chunk_indicator(probe_line);
    }

    // There's a prev chunk, `probe` points to its last line.
    var end = probe+1;

    // Find the beginning of this chunk.
    var prev_indicator = chunk_indicator;
    while (prev_indicator === chunk_indicator) {
        probe--;
        if (probe <= 0) {
            return;
        }
        probe_line = c.line_elt(probe);
        prev_indicator = c.chunk_indicator(probe_line);
    }
    c.set_sel(probe+1, end);
    c.show_selection();
};

// Returns 0, 1, or 2: how many of the two ends of the selection are on
// the screen right now?
coverage.selection_ends_on_screen = function () {
    if (coverage.sel_begin === 0) {
        return 0;
    }

    const begin = coverage.line_elt(coverage.sel_begin);
    const end = coverage.line_elt(coverage.sel_end-1);

    return (
        (checkVisible(begin) ? 1 : 0)
        + (checkVisible(end) ? 1 : 0)
    );
};

coverage.to_next_chunk_nicely = function () {
    if (coverage.selection_ends_on_screen() === 0) {
        // The selection is entirely off the screen:
        // Set the top line on the screen as selection.

        // This will select the top-left of the viewport
        // As this is most likely the span with the line number we take the parent
        const line = document.elementFromPoint(0, 0).parentElement;
        if (line.parentElement !== document.getElementById("source")) {
            // The element is not a source line but the header or similar
            coverage.select_line_or_chunk(1);
        }
        else {
            // We extract the line number from the id
            coverage.select_line_or_chunk(parseInt(line.id.substring(1), 10));
        }
    }
    coverage.to_next_chunk();
};

coverage.to_prev_chunk_nicely = function () {
    if (coverage.selection_ends_on_screen() === 0) {
        // The selection is entirely off the screen:
        // Set the lowest line on the screen as selection.

        // This will select the bottom-left of the viewport
        // As this is most likely the span with the line number we take the parent
        const line = document.elementFromPoint(document.documentElement.clientHeight-1, 0).parentElement;
        if (line.parentElement !== document.getElementById("source")) {
            // The element is not a source line but the header or similar
            coverage.select_line_or_chunk(coverage.lines_len);
        }
        else {
            // We extract the line number from the id
            coverage.select_line_or_chunk(parseInt(line.id.substring(1), 10));
        }
    }
    coverage.to_prev_chunk();
};

// Select line number lineno, or if it is in a colored chunk, select the
// entire chunk
coverage.select_line_or_chunk = function (lineno) {
    var c = coverage;
    var probe_line = c.line_elt(lineno);
    if (!probe_line) {
        return;
    }
    var the_indicator = c.chunk_indicator(probe_line);
    if (the_indicator) {
        // The line is in a highlighted chunk.
        // Search backward for the first line.
        var probe = lineno;
        var indicator = the_indicator;
        while (probe > 0 && indicator === the_indicator) {
            probe--;
            probe_line = c.line_elt(probe);
            if (!probe_line) {
                break;
            }
            indicator = c.chunk_indicator(probe_line);
        }
        var begin = probe + 1;

        // Search forward for the last line.
        probe = lineno;
        indicator = the_indicator;
        while (indicator === the_indicator) {
            probe++;
            probe_line = c.line_elt(probe);
            indicator = c.chunk_indicator(probe_line);
        }

        coverage.set_sel(begin, probe);
    }
    else {
        coverage.set_sel(lineno);
    }
};

coverage.show_selection = function () {
    // Highlight the lines in the chunk
    document.querySelectorAll("#source .highlight").forEach(e => e.classList.remove("highlight"));
    for (let probe = coverage.sel_begin; probe < coverage.sel_end; probe++) {
        coverage.line_elt(probe).querySelector(".n").classList.add("highlight");
    }

    coverage.scroll_to_selection();
};

coverage.scroll_to_selection = function () {
    // Scroll the page if the chunk isn't fully visible.
    if (coverage.selection_ends_on_screen() < 2) {
        const element = coverage.line_elt(coverage.sel_begin);
        coverage.scroll_window(element.offsetTop - 60);
    }
};

coverage.scroll_window = function (to_pos) {
    window.scroll({top: to_pos, behavior: "smooth"});
};

coverage.init_scroll_markers = function () {
    // Init some variables
    coverage.lines_len = document.querySelectorAll("#source > p").length;

    // Build html
    coverage.build_scroll_markers();
};

coverage.build_scroll_markers = function () {
    const temp_scroll_marker = document.getElementById("scroll_marker")
    if (temp_scroll_marker) temp_scroll_marker.remove();
    // Don't build markers if the window has no scroll bar.
    if (document.body.scrollHeight <= window.innerHeight) {
        return;
    }

    const marker_scale = window.innerHeight / document.body.scrollHeight;
    const line_height = Math.min(Math.max(3, window.innerHeight / coverage.lines_len), 10);

    let previous_line = -99, last_mark, last_top;

    const scroll_marker = document.createElement("div");
    scroll_marker.id = "scroll_marker";
    document.getElementById("source").querySelectorAll(
        "p.show_run, p.show_mis, p.show_exc, p.show_exc, p.show_par"
    ).forEach(element => {
        const line_top = Math.floor(element.offsetTop * marker_scale);
        const line_number = parseInt(element.querySelector(".n a").id.substr(1));

        if (line_number === previous_line + 1) {
            // If this solid missed block just make previous mark higher.
            last_mark.style.height = `${line_top + line_height - last_top}px`;
        }
        else {
            // Add colored line in scroll_marker block.
            last_mark = document.createElement("div");
            last_mark.id = `m${line_number}`;
            last_mark.classList.add("marker");
            last_mark.style.height = `${line_height}px`;
            last_mark.style.top = `${line_top}px`;
            scroll_marker.append(last_mark);
            last_top = line_top;
        }

        previous_line = line_number;
    });

    // Append last to prevent layout calculation
    document.body.append(scroll_marker);
};

coverage.wire_up_sticky_header = function () {
    const header = document.querySelector("header");
    const header_bottom = (
        header.querySelector(".content h2").getBoundingClientRect().top -
        header.getBoundingClientRect().top
    );

    function updateHeader() {
        if (window.scrollY > header_bottom) {
            header.classList.add("sticky");
        }
        else {
            header.classList.remove("sticky");
        }
    }

    window.addEventListener("scroll", updateHeader);
    updateHeader();
};

coverage.expand_contexts = function (e) {
    var ctxs = e.target.parentNode.querySelector(".ctxs");

    if (!ctxs.classList.contains("expanded")) {
        var ctxs_text = ctxs.textContent;
        var width = Number(ctxs_text[0]);
        ctxs.textContent = "";
        for (var i = 1; i < ctxs_text.length; i += width) {
            key = ctxs_text.substring(i, i + width).trim();
            ctxs.appendChild(document.createTextNode(contexts[key]));
            ctxs.appendChild(document.createElement("br"));
        }
        ctxs.classList.add("expanded");
    }
};

document.addEventListener("DOMContentLoaded", () => {
    if (document.body.classList.contains("indexfile")) {
        coverage.index_ready();
    }
    else {
        coverage.pyfile_ready();
    }
});


================================================
File: tests/integration/test_database_operations.py
================================================
"""
Comprehensive integration tests for PostgreSQL database operations.

Tests CRUD operations, triggers, functions, and constraints for all core tables
in the RAG-enabled WhatsApp chatbot system.

Test Categories:
1. CRUD Tests - Basic create, read, update, delete operations
2. Database Trigger Tests - Verify triggers work as expected
3. Database Function Tests - Test stored procedures and functions
4. Constraint Tests - Verify constraints are enforced
"""

import pytest
import psycopg2
from datetime import datetime, timedelta
from decimal import Decimal


# ============================================================================
# 1. CRUD TESTS FOR CORE TABLES
# ============================================================================

class TestOrganizationsCRUD:
    """Test CRUD operations on organizations table."""

    def test_create_organization_with_defaults(self, db_with_data):
        """
        GOAL: Verify organization can be created with default values
        GIVEN: A database connection
        WHEN: An organization is created with only required fields
        THEN: The organization is created with proper default values
        """
        db_with_data.execute("""
            INSERT INTO organizations (name, slug)
            VALUES (%s, %s)
            RETURNING id, plan_tier, is_active, current_knowledge_pdfs, current_storage_mb
        """, ('Test Org', 'test-org'))

        result = db_with_data.fetchone()

        assert result['plan_tier'] == 'free'
        assert result['is_active'] is True
        assert result['current_knowledge_pdfs'] == 0
        assert result['current_storage_mb'] == Decimal('0')

    def test_read_organization_by_id(self, db_with_data):
        """
        GOAL: Verify organizations can be retrieved by ID
        GIVEN: Seeded database with test organization
        WHEN: Organization is queried by ID
        THEN: Correct organization data is returned
        """
        org_id = '11111111-1111-1111-1111-111111111111'

        db_with_data.execute("""
            SELECT name, slug, plan_tier, message_limit_monthly
            FROM organizations
            WHERE id = %s
        """, (org_id,))

        result = db_with_data.fetchone()

        assert result is not None
        assert result['name'] == 'JD Labs Corporation'
        assert result['slug'] == 'jd-labs-corp'
        assert result['plan_tier'] == 'pro'
        assert result['message_limit_monthly'] == 1000

    def test_update_organization_plan_tier(self, db_with_data):
        """
        GOAL: Verify organization plan tier can be updated
        GIVEN: Existing organization
        WHEN: Plan tier is updated with new limits
        THEN: Organization is updated and updated_at changes
        """
        org_id = '11111111-1111-1111-1111-111111111111'

        db_with_data.execute("""
            UPDATE organizations
            SET plan_tier = 'enterprise',
                message_limit_monthly = 10000,
                token_limit_monthly = 5000000
            WHERE id = %s
            RETURNING plan_tier, message_limit_monthly, token_limit_monthly
        """, (org_id,))

        result = db_with_data.fetchone()

        assert result['plan_tier'] == 'enterprise'
        assert result['message_limit_monthly'] == 10000
        assert result['token_limit_monthly'] == 5000000

    def test_delete_organization_cascades_to_users(self, db_with_data):
        """
        GOAL: Verify deleting organization cascades to related users
        GIVEN: Organization with associated users
        WHEN: Organization is deleted
        THEN: Associated users are also deleted (CASCADE)
        """
        # Create test organization with UUID
        import uuid
        test_org_id = str(uuid.uuid4())

        db_with_data.execute("""
            INSERT INTO organizations (id, name, slug)
            VALUES (%s::uuid, %s, %s)
        """, (test_org_id, 'Delete Test Org', 'delete-test'))

        # Create test user
        db_with_data.execute("""
            INSERT INTO users (organization_id, email, full_name)
            VALUES (%s::uuid, %s, %s)
            RETURNING id
        """, (test_org_id, 'testdelete@example.com', 'Test Delete User'))

        user_result = db_with_data.fetchone()
        user_id = user_result['id']

        # Delete organization
        db_with_data.execute("""
            DELETE FROM organizations WHERE id = %s::uuid
        """, (test_org_id,))

        # Verify user was cascade deleted
        db_with_data.execute("""
            SELECT COUNT(*) as count FROM users WHERE id = %s
        """, (user_id,))

        result = db_with_data.fetchone()
        assert result['count'] == 0


class TestChatbotsCRUD:
    """Test CRUD operations on chatbots table."""

    def test_create_chatbot_with_rag_enabled(self, db_with_data):
        """
        GOAL: Verify chatbot can be created with RAG enabled
        GIVEN: Valid organization
        WHEN: Chatbot is created with rag_enabled=TRUE
        THEN: Chatbot is created successfully with RAG configuration
        """
        org_id = '11111111-1111-1111-1111-111111111111'

        db_with_data.execute("""
            INSERT INTO chatbots (
                organization_id,
                name,
                whatsapp_phone_number_id,
                rag_enabled,
                model_name
            )
            VALUES (%s, %s, %s, %s, %s)
            RETURNING id, rag_enabled, is_active, temperature
        """, (org_id, 'RAG Test Bot', 'phone-rag-test-001', True, 'gpt-4'))

        result = db_with_data.fetchone()

        assert result['rag_enabled'] is True
        assert result['is_active'] is True
        assert result['temperature'] == Decimal('0.7')

    def test_read_chatbot_with_organization(self, db_with_data):
        """
        GOAL: Verify chatbot can be retrieved with organization details
        GIVEN: Chatbot in database
        WHEN: Chatbot is queried with JOIN to organization
        THEN: Complete chatbot and organization data is returned
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        db_with_data.execute("""
            SELECT
                c.id,
                c.name as chatbot_name,
                c.whatsapp_phone_number_id,
                c.rag_enabled,
                o.name as org_name,
                o.plan_tier
            FROM chatbots c
            JOIN organizations o ON c.organization_id = o.id
            WHERE c.id = %s
        """, (chatbot_id,))

        result = db_with_data.fetchone()

        assert result is not None
        assert result['chatbot_name'] == 'MVP Test Bot'
        assert result['org_name'] == 'JD Labs Corporation'
        assert result['plan_tier'] == 'pro'

    def test_update_chatbot_model_and_temperature(self, db_with_data):
        """
        GOAL: Verify chatbot AI configuration can be updated
        GIVEN: Existing chatbot
        WHEN: Model and temperature are updated
        THEN: Changes are persisted correctly
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        db_with_data.execute("""
            UPDATE chatbots
            SET model_name = %s,
                temperature = %s
            WHERE id = %s
            RETURNING model_name, temperature
        """, ('gpt-4-turbo', 0.3, chatbot_id))

        result = db_with_data.fetchone()

        assert result['model_name'] == 'gpt-4-turbo'
        assert result['temperature'] == Decimal('0.3')

    def test_chatbot_unique_phone_number_constraint(self, db_with_data):
        """
        GOAL: Verify whatsapp_phone_number_id must be unique
        GIVEN: Existing chatbot with phone number
        WHEN: Another chatbot is created with same phone number
        THEN: Unique constraint violation is raised
        """
        org_id = '11111111-1111-1111-1111-111111111111'

        with pytest.raises(psycopg2.IntegrityError) as exc_info:
            db_with_data.execute("""
                INSERT INTO chatbots (
                    organization_id,
                    name,
                    whatsapp_phone_number_id
                )
                VALUES (%s, %s, %s)
            """, (org_id, 'Duplicate Phone Bot', 'test_phone_123'))

        assert 'whatsapp_phone_number_id' in str(exc_info.value).lower()


class TestContactsCRUD:
    """Test CRUD operations on contacts table."""

    def test_create_contact_with_variables(self, db_with_data):
        """
        GOAL: Verify contact can be created with JSONB variables
        GIVEN: Valid chatbot
        WHEN: Contact is created with custom variables
        THEN: Contact is created with variables stored as JSONB
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        db_with_data.execute("""
            INSERT INTO contacts (
                chatbot_id,
                phone_number,
                name,
                variables,
                tags
            )
            VALUES (%s, %s, %s, %s, %s)
            RETURNING id, variables, tags, conversation_mode
        """, (
            chatbot_id,
            '15551112222',
            'Test Contact',
            '{"lead_score": 85, "industry": "tech"}',
            ['hot-lead', 'enterprise']
        ))

        result = db_with_data.fetchone()

        assert result['variables']['lead_score'] == 85
        assert result['variables']['industry'] == 'tech'
        assert 'hot-lead' in result['tags']
        assert result['conversation_mode'] == 'auto'

    def test_contact_unique_phone_per_chatbot(self, db_with_data):
        """
        GOAL: Verify phone number must be unique per chatbot
        GIVEN: Contact exists for a chatbot
        WHEN: Another contact with same phone is created for same chatbot
        THEN: Unique constraint violation is raised
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'
        phone = '15550001234'  # Alice's existing phone

        with pytest.raises(psycopg2.IntegrityError) as exc_info:
            db_with_data.execute("""
                INSERT INTO contacts (chatbot_id, phone_number, name)
                VALUES (%s, %s, %s)
            """, (chatbot_id, phone, 'Duplicate Alice'))

        assert 'chatbot_id' in str(exc_info.value).lower() or 'phone_number' in str(exc_info.value).lower()

    def test_update_contact_conversation_mode(self, db_with_data):
        """
        GOAL: Verify contact conversation mode can be toggled
        GIVEN: Contact in auto mode
        WHEN: Conversation mode is updated to manual
        THEN: Mode is changed and updated_at is updated
        """
        contact_id = '44444444-4444-4444-4444-444444444444'

        db_with_data.execute("""
            UPDATE contacts
            SET conversation_mode = 'manual',
                unread_count = 5
            WHERE id = %s
            RETURNING conversation_mode, unread_count
        """, (contact_id,))

        result = db_with_data.fetchone()

        assert result['conversation_mode'] == 'manual'
        assert result['unread_count'] == 5


class TestMessagesCRUD:
    """Test CRUD operations on messages table."""

    def test_create_user_message_with_whatsapp_id(self, db_with_data):
        """
        GOAL: Verify user message can be created with WhatsApp message ID
        GIVEN: Valid contact
        WHEN: User message is inserted
        THEN: Message is created with whatsapp_message_id for idempotency
        """
        contact_id = '44444444-4444-4444-4444-444444444444'

        db_with_data.execute("""
            INSERT INTO messages (
                contact_id,
                role,
                content,
                whatsapp_message_id
            )
            VALUES (%s, %s, %s, %s)
            RETURNING id, role, whatsapp_message_id
        """, (contact_id, 'user', 'Test message', 'wamid.test.new.001'))

        result = db_with_data.fetchone()

        assert result['role'] == 'user'
        assert result['whatsapp_message_id'] == 'wamid.test.new.001'

    def test_create_assistant_message_with_tool_calls(self, db_with_data):
        """
        GOAL: Verify assistant message can store tool call data
        GIVEN: Valid contact
        WHEN: Assistant message with tool calls is created
        THEN: Tool call data is stored as JSONB
        """
        contact_id = '44444444-4444-4444-4444-444444444444'

        tool_calls = {
            "calls": [
                {
                    "tool": "calculate_pricing",
                    "args": {"message_volume": 5000, "tier": "professional"}
                }
            ]
        }

        tool_results = {
            "results": [
                {
                    "tool": "calculate_pricing",
                    "result": {"price": 999, "currency": "MXN"}
                }
            ]
        }

        db_with_data.execute("""
            INSERT INTO messages (
                contact_id,
                role,
                content,
                tool_calls,
                tool_results
            )
            VALUES (%s, %s, %s, %s, %s)
            RETURNING id, tool_calls, tool_results
        """, (contact_id, 'assistant', 'The price is 999 MXN',
              str(tool_calls).replace("'", '"'),
              str(tool_results).replace("'", '"')))

        result = db_with_data.fetchone()

        assert result['tool_calls'] is not None
        assert result['tool_results'] is not None

    def test_read_conversation_history_ordered(self, db_with_data):
        """
        GOAL: Verify messages can be retrieved in chronological order
        GIVEN: Contact with multiple messages
        WHEN: Messages are queried ordered by created_at
        THEN: Messages are returned in correct order
        """
        contact_id = '44444444-4444-4444-4444-444444444444'

        db_with_data.execute("""
            SELECT role, content, created_at
            FROM messages
            WHERE contact_id = %s
            ORDER BY created_at ASC
        """, (contact_id,))

        messages = db_with_data.fetchall()

        assert len(messages) > 0
        # Verify chronological order
        for i in range(1, len(messages)):
            assert messages[i]['created_at'] >= messages[i-1]['created_at']


class TestKnowledgeSourcesCRUD:
    """Test CRUD operations on knowledge_sources table."""

    def test_create_pdf_knowledge_source(self, db_with_data):
        """
        GOAL: Verify PDF knowledge source can be created
        GIVEN: Valid chatbot
        WHEN: PDF source is inserted
        THEN: Source is created with pending sync status
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        db_with_data.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name,
                file_path,
                file_size_bytes,
                sync_status
            )
            VALUES (%s, %s, %s, %s, %s, %s)
            RETURNING id, source_type, sync_status, chunks_count
        """, (chatbot_id, 'pdf', 'Product Manual.pdf', '/uploads/manual.pdf',
              1048576, 'pending'))

        result = db_with_data.fetchone()

        assert result['source_type'] == 'pdf'
        assert result['sync_status'] == 'pending'
        assert result['chunks_count'] == 0

    def test_update_knowledge_source_sync_status(self, db_with_data):
        """
        GOAL: Verify knowledge source sync status can be updated
        GIVEN: Knowledge source in pending state
        WHEN: Sync completes and status is updated
        THEN: Status changes to synced with timestamp
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create source
        db_with_data.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name,
                sync_status
            )
            VALUES (%s, %s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'url', 'Company Website', 'processing'))

        source_id = db_with_data.fetchone()['id']

        # Update to synced
        db_with_data.execute("""
            UPDATE knowledge_sources
            SET sync_status = 'synced',
                last_synced_at = NOW()
            WHERE id = %s
            RETURNING sync_status, last_synced_at
        """, (source_id,))

        result = db_with_data.fetchone()

        assert result['sync_status'] == 'synced'
        assert result['last_synced_at'] is not None


class TestDocumentChunksCRUD:
    """Test CRUD operations on document_chunks table."""

    def test_create_document_chunk_with_embedding(self, db_with_data):
        """
        GOAL: Verify document chunk can be created with vector embedding
        GIVEN: Valid knowledge source
        WHEN: Chunk with 1536-dim embedding is inserted
        THEN: Chunk is stored with embedding vector
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create knowledge source first
        db_with_data.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name,
                sync_status
            )
            VALUES (%s, %s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'pdf', 'Test Doc', 'processing'))

        source_id = db_with_data.fetchone()['id']

        # Create embedding vector (1536 dimensions)
        embedding = '[' + ', '.join(['0.1'] * 1536) + ']'

        # Insert chunk
        db_with_data.execute("""
            INSERT INTO document_chunks (
                knowledge_source_id,
                chatbot_id,
                content,
                chunk_index,
                embedding,
                metadata
            )
            VALUES (%s, %s, %s, %s, %s::vector, %s)
            RETURNING id, chunk_index, metadata
        """, (source_id, chatbot_id, 'This is test content', 0,
              embedding, '{"page": 1}'))

        result = db_with_data.fetchone()

        assert result['chunk_index'] == 0
        assert result['metadata']['page'] == 1

    def test_document_chunk_unique_constraint(self, db_with_data):
        """
        GOAL: Verify chunk_index must be unique per knowledge source
        GIVEN: Existing chunk with index 0
        WHEN: Another chunk with same index is created for same source
        THEN: Unique constraint violation is raised
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create knowledge source
        db_with_data.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'pdf', 'Test Doc'))

        source_id = db_with_data.fetchone()['id']

        # Insert first chunk
        db_with_data.execute("""
            INSERT INTO document_chunks (
                knowledge_source_id,
                chatbot_id,
                content,
                chunk_index
            )
            VALUES (%s, %s, %s, %s)
        """, (source_id, chatbot_id, 'Chunk 0', 0))

        # Try to insert duplicate
        with pytest.raises(psycopg2.IntegrityError) as exc_info:
            db_with_data.execute("""
                INSERT INTO document_chunks (
                    knowledge_source_id,
                    chatbot_id,
                    content,
                    chunk_index
                )
                VALUES (%s, %s, %s, %s)
            """, (source_id, chatbot_id, 'Duplicate Chunk 0', 0))

        assert 'unique' in str(exc_info.value).lower()


# ============================================================================
# 2. DATABASE TRIGGER TESTS
# ============================================================================

class TestKnowledgeSourceCounterTrigger:
    """
    Test the knowledge_source_counter_trigger that increments counters
    when knowledge sources are added.

    Trigger definition: Lines 460-463 in create.sql
    Function: increment_knowledge_counters() (Lines 423-458)
    """

    def test_pdf_insert_increments_pdf_counter(self, db_with_autocommit):
        """
        GOAL: Verify inserting PDF increments organization's PDF counter
        GIVEN: Organization with 0 PDFs
        WHEN: PDF knowledge source is inserted
        THEN: current_knowledge_pdfs increments by 1
        """
        org_id = '11111111-1111-1111-1111-111111111111'
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Get current count
        db_with_autocommit.execute("""
            SELECT current_knowledge_pdfs FROM organizations WHERE id = %s
        """, (org_id,))
        initial_count = db_with_autocommit.fetchone()['current_knowledge_pdfs']

        # Insert PDF
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name,
                file_size_bytes
            )
            VALUES (%s, %s, %s, %s)
        """, (chatbot_id, 'pdf', 'Test.pdf', 2097152))  # 2MB

        # Check counter
        db_with_autocommit.execute("""
            SELECT current_knowledge_pdfs FROM organizations WHERE id = %s
        """, (org_id,))
        new_count = db_with_autocommit.fetchone()['current_knowledge_pdfs']

        assert new_count == initial_count + 1

    def test_url_insert_increments_url_counter(self, db_with_autocommit):
        """
        GOAL: Verify inserting URL increments organization's URL counter
        GIVEN: Organization with 0 URLs
        WHEN: URL knowledge source is inserted
        THEN: current_knowledge_urls increments by 1
        """
        org_id = '11111111-1111-1111-1111-111111111111'
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Get current count
        db_with_autocommit.execute("""
            SELECT current_knowledge_urls FROM organizations WHERE id = %s
        """, (org_id,))
        initial_count = db_with_autocommit.fetchone()['current_knowledge_urls']

        # Insert URL
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name,
                file_size_bytes
            )
            VALUES (%s, %s, %s, %s)
        """, (chatbot_id, 'url', 'https://example.com', 10240))  # 10KB

        # Check counter
        db_with_autocommit.execute("""
            SELECT current_knowledge_urls FROM organizations WHERE id = %s
        """, (org_id,))
        new_count = db_with_autocommit.fetchone()['current_knowledge_urls']

        assert new_count == initial_count + 1

    def test_pdf_insert_updates_storage_counter(self, db_with_autocommit):
        """
        GOAL: Verify inserting PDF updates current_storage_mb
        GIVEN: Organization with current storage
        WHEN: PDF with file_size_bytes is inserted
        THEN: current_storage_mb increases by file size in MB
        """
        org_id = '11111111-1111-1111-1111-111111111111'
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Get current storage
        db_with_autocommit.execute("""
            SELECT current_storage_mb FROM organizations WHERE id = %s
        """, (org_id,))
        initial_storage = float(db_with_autocommit.fetchone()['current_storage_mb'])

        # Insert 5MB PDF
        file_size_bytes = 5 * 1024 * 1024  # 5MB
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name,
                file_size_bytes
            )
            VALUES (%s, %s, %s, %s)
        """, (chatbot_id, 'pdf', 'Large.pdf', file_size_bytes))

        # Check storage
        db_with_autocommit.execute("""
            SELECT current_storage_mb FROM organizations WHERE id = %s
        """, (org_id,))
        new_storage = float(db_with_autocommit.fetchone()['current_storage_mb'])

        expected_storage = initial_storage + 5.0
        assert abs(new_storage - expected_storage) < 0.01  # Allow small float difference

    def test_daily_ingestion_count_increments(self, db_with_autocommit):
        """
        GOAL: Verify inserting knowledge source updates daily_ingestion_counts
        GIVEN: Organization with no ingestions today
        WHEN: Knowledge source is inserted
        THEN: daily_ingestion_counts table is updated with count=1
        """
        org_id = '11111111-1111-1111-1111-111111111111'
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Insert knowledge source
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name
            )
            VALUES (%s, %s, %s)
        """, (chatbot_id, 'pdf', 'Daily Test.pdf'))

        # Check daily count
        db_with_autocommit.execute("""
            SELECT ingestion_count
            FROM daily_ingestion_counts
            WHERE organization_id = %s AND date = CURRENT_DATE
        """, (org_id,))

        result = db_with_autocommit.fetchone()
        assert result is not None
        assert result['ingestion_count'] >= 1

    def test_multiple_inserts_increment_daily_count(self, db_with_autocommit):
        """
        GOAL: Verify multiple knowledge sources increment daily count correctly
        GIVEN: Organization with existing daily count
        WHEN: Multiple knowledge sources are inserted
        THEN: daily_ingestion_counts increments for each insert
        """
        org_id = '11111111-1111-1111-1111-111111111111'
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Get current daily count
        db_with_autocommit.execute("""
            SELECT COALESCE(ingestion_count, 0) as count
            FROM daily_ingestion_counts
            WHERE organization_id = %s AND date = CURRENT_DATE
        """, (org_id,))

        result = db_with_autocommit.fetchone()
        initial_count = result['count'] if result else 0

        # Insert 3 sources
        for i in range(3):
            db_with_autocommit.execute("""
                INSERT INTO knowledge_sources (
                    chatbot_id,
                    source_type,
                    name
                )
                VALUES (%s, %s, %s)
            """, (chatbot_id, 'pdf', f'Test{i}.pdf'))

        # Check count increased by 3
        db_with_autocommit.execute("""
            SELECT ingestion_count
            FROM daily_ingestion_counts
            WHERE organization_id = %s AND date = CURRENT_DATE
        """, (org_id,))

        final_count = db_with_autocommit.fetchone()['ingestion_count']
        assert final_count == initial_count + 3


class TestChunksCountTrigger:
    """
    Test the trigger_update_chunks_count that updates knowledge_sources.chunks_count
    when chunks are inserted or deleted.

    Trigger definition: Lines 417-420 in create.sql
    Function: update_knowledge_source_chunks_count() (Lines 399-415)
    """

    def test_insert_chunk_increments_count(self, db_with_autocommit):
        """
        GOAL: Verify inserting chunk increments knowledge_source chunks_count
        GIVEN: Knowledge source with 0 chunks
        WHEN: Document chunk is inserted
        THEN: chunks_count increments by 1
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create knowledge source
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'pdf', 'Chunk Test.pdf'))

        source_id = db_with_autocommit.fetchone()['id']

        # Insert chunk
        db_with_autocommit.execute("""
            INSERT INTO document_chunks (
                knowledge_source_id,
                chatbot_id,
                content,
                chunk_index
            )
            VALUES (%s, %s, %s, %s)
        """, (source_id, chatbot_id, 'Test content', 0))

        # Check count
        db_with_autocommit.execute("""
            SELECT chunks_count FROM knowledge_sources WHERE id = %s
        """, (source_id,))

        result = db_with_autocommit.fetchone()
        assert result['chunks_count'] == 1

    def test_multiple_chunks_increment_count_correctly(self, db_with_autocommit):
        """
        GOAL: Verify inserting multiple chunks increments count correctly
        GIVEN: Knowledge source with 0 chunks
        WHEN: 5 chunks are inserted
        THEN: chunks_count equals 5
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create knowledge source
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'pdf', 'Multi Chunk.pdf'))

        source_id = db_with_autocommit.fetchone()['id']

        # Insert 5 chunks
        for i in range(5):
            db_with_autocommit.execute("""
                INSERT INTO document_chunks (
                    knowledge_source_id,
                    chatbot_id,
                    content,
                    chunk_index
                )
                VALUES (%s, %s, %s, %s)
            """, (source_id, chatbot_id, f'Content {i}', i))

        # Check count
        db_with_autocommit.execute("""
            SELECT chunks_count FROM knowledge_sources WHERE id = %s
        """, (source_id,))

        result = db_with_autocommit.fetchone()
        assert result['chunks_count'] == 5

    def test_delete_chunk_decrements_count(self, db_with_autocommit):
        """
        GOAL: Verify deleting chunk decrements knowledge_source chunks_count
        GIVEN: Knowledge source with 3 chunks
        WHEN: One chunk is deleted
        THEN: chunks_count decrements by 1
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create knowledge source
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'pdf', 'Delete Test.pdf'))

        source_id = db_with_autocommit.fetchone()['id']

        # Insert 3 chunks
        chunk_ids = []
        for i in range(3):
            db_with_autocommit.execute("""
                INSERT INTO document_chunks (
                    knowledge_source_id,
                    chatbot_id,
                    content,
                    chunk_index
                )
                VALUES (%s, %s, %s, %s)
                RETURNING id
            """, (source_id, chatbot_id, f'Content {i}', i))
            chunk_ids.append(db_with_autocommit.fetchone()['id'])

        # Delete one chunk
        db_with_autocommit.execute("""
            DELETE FROM document_chunks WHERE id = %s
        """, (chunk_ids[0],))

        # Check count
        db_with_autocommit.execute("""
            SELECT chunks_count FROM knowledge_sources WHERE id = %s
        """, (source_id,))

        result = db_with_autocommit.fetchone()
        assert result['chunks_count'] == 2


# ============================================================================
# 3. DATABASE FUNCTION TESTS
# ============================================================================

class TestSearchKnowledgeBaseFunction:
    """
    Test the search_knowledge_base() function for RAG similarity search.

    Function definition: Lines 344-375 in create.sql
    """

    def test_search_returns_similar_chunks_above_threshold(self, db_with_autocommit):
        """
        GOAL: Verify search_knowledge_base returns chunks above similarity threshold
        GIVEN: Chunks with embeddings in database
        WHEN: Function is called with query embedding and threshold=0.7
        THEN: Only chunks with similarity >= 0.7 are returned
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create knowledge source
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name,
                sync_status
            )
            VALUES (%s, %s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'pdf', 'Search Test.pdf', 'synced'))

        source_id = db_with_autocommit.fetchone()['id']

        # Create similar embedding (1536 dimensions, all 0.1)
        similar_embedding = '[' + ', '.join(['0.1'] * 1536) + ']'

        # Insert chunks with embeddings
        db_with_autocommit.execute("""
            INSERT INTO document_chunks (
                knowledge_source_id,
                chatbot_id,
                content,
                chunk_index,
                embedding,
                metadata
            )
            VALUES (%s, %s, %s, %s, %s::vector, %s)
        """, (source_id, chatbot_id, 'Similar content', 0,
              similar_embedding, '{"page": 1}'))

        # Call search function
        query_embedding = '[' + ', '.join(['0.1'] * 1536) + ']'

        db_with_autocommit.execute("""
            SELECT * FROM search_knowledge_base(
                %s::uuid,
                %s::vector,
                5,
                0.7
            )
        """, (chatbot_id, query_embedding))

        results = db_with_autocommit.fetchall()

        # Should return at least the similar chunk
        assert len(results) > 0
        # All results should have similarity >= 0.7
        for result in results:
            assert result['similarity'] >= 0.7

    def test_search_respects_chatbot_filter(self, db_with_autocommit):
        """
        GOAL: Verify search_knowledge_base only returns chunks for specified chatbot
        GIVEN: Chunks for multiple chatbots
        WHEN: Search is called for specific chatbot
        THEN: Only chunks from that chatbot are returned
        """
        org_id = '11111111-1111-1111-1111-111111111111'
        chatbot_id_1 = '22222222-2222-2222-2222-222222222222'

        # Create second chatbot
        db_with_autocommit.execute("""
            INSERT INTO chatbots (
                organization_id,
                name,
                whatsapp_phone_number_id
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (org_id, 'Bot 2', 'phone-test-search-002'))

        chatbot_id_2 = db_with_autocommit.fetchone()['id']

        # Create knowledge sources for both chatbots
        for chatbot_id, name in [(chatbot_id_1, 'Doc1'), (chatbot_id_2, 'Doc2')]:
            db_with_autocommit.execute("""
                INSERT INTO knowledge_sources (
                    chatbot_id,
                    source_type,
                    name
                )
                VALUES (%s, %s, %s)
                RETURNING id
            """, (chatbot_id, 'pdf', name))

            source_id = db_with_autocommit.fetchone()['id']

            # Insert chunk
            embedding = '[' + ', '.join(['0.1'] * 1536) + ']'
            db_with_autocommit.execute("""
                INSERT INTO document_chunks (
                    knowledge_source_id,
                    chatbot_id,
                    content,
                    chunk_index,
                    embedding
                )
                VALUES (%s, %s, %s, %s, %s::vector)
            """, (source_id, chatbot_id, f'Content for {name}', 0, embedding))

        # Search for chatbot 1 only
        query_embedding = '[' + ', '.join(['0.1'] * 1536) + ']'

        db_with_autocommit.execute("""
            SELECT * FROM search_knowledge_base(
                %s::uuid,
                %s::vector,
                10,
                0.5
            )
        """, (chatbot_id_1, query_embedding))

        results = db_with_autocommit.fetchall()

        # Should only have results from Doc1
        assert len(results) > 0
        for result in results:
            assert result['source_name'] == 'Doc1'

    def test_search_respects_limit_parameter(self, db_with_autocommit):
        """
        GOAL: Verify search_knowledge_base respects the limit parameter
        GIVEN: 10 chunks in database
        WHEN: Search is called with limit=3
        THEN: At most 3 results are returned
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create knowledge source
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'pdf', 'Limit Test.pdf'))

        source_id = db_with_autocommit.fetchone()['id']

        # Insert 10 chunks
        embedding = '[' + ', '.join(['0.1'] * 1536) + ']'
        for i in range(10):
            db_with_autocommit.execute("""
                INSERT INTO document_chunks (
                    knowledge_source_id,
                    chatbot_id,
                    content,
                    chunk_index,
                    embedding
                )
                VALUES (%s, %s, %s, %s, %s::vector)
            """, (source_id, chatbot_id, f'Chunk {i}', i, embedding))

        # Search with limit=3
        query_embedding = '[' + ', '.join(['0.1'] * 1536) + ']'

        db_with_autocommit.execute("""
            SELECT * FROM search_knowledge_base(
                %s::uuid,
                %s::vector,
                3,
                0.5
            )
        """, (chatbot_id, query_embedding))

        results = db_with_autocommit.fetchall()

        assert len(results) <= 3

    def test_search_returns_metadata_and_source_info(self, db_with_autocommit):
        """
        GOAL: Verify search results include metadata and source information
        GIVEN: Chunks with metadata
        WHEN: Search is performed
        THEN: Results include chunk metadata and source details
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create knowledge source
        db_with_autocommit.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'pdf', 'Metadata Test.pdf'))

        source_id = db_with_autocommit.fetchone()['id']

        # Insert chunk with metadata
        embedding = '[' + ', '.join(['0.2'] * 1536) + ']'
        db_with_autocommit.execute("""
            INSERT INTO document_chunks (
                knowledge_source_id,
                chatbot_id,
                content,
                chunk_index,
                embedding,
                metadata
            )
            VALUES (%s, %s, %s, %s, %s::vector, %s)
        """, (source_id, chatbot_id, 'Test content', 0,
              embedding, '{"page": 5, "section": "Introduction"}'))

        # Search
        query_embedding = '[' + ', '.join(['0.2'] * 1536) + ']'

        db_with_autocommit.execute("""
            SELECT * FROM search_knowledge_base(
                %s::uuid,
                %s::vector,
                5,
                0.5
            )
        """, (chatbot_id, query_embedding))

        results = db_with_autocommit.fetchall()

        assert len(results) > 0
        result = results[0]
        assert result['source_name'] == 'Metadata Test.pdf'
        assert result['source_type'] == 'pdf'
        assert result['metadata']['page'] == 5
        assert result['metadata']['section'] == 'Introduction'


class TestGetCurrentUsageFunction:
    """
    Test the get_current_usage() function for billing period usage calculation.

    Function definition: Lines 490-511 in create.sql
    """

    def test_get_usage_sums_messages_and_tokens(self, db_with_data):
        """
        GOAL: Verify get_current_usage correctly sums messages and tokens
        GIVEN: Usage logs for an organization
        WHEN: Function is called
        THEN: Correct total messages and tokens are returned
        """
        org_id = '11111111-1111-1111-1111-111111111111'
        chatbot_id = '22222222-2222-2222-2222-222222222222'
        contact_id = '44444444-4444-4444-4444-444444444444'

        # Get billing period for the org
        db_with_data.execute("""
            SELECT billing_period_start, billing_period_end
            FROM organizations WHERE id = %s
        """, (org_id,))

        org = db_with_data.fetchone()

        # Insert test usage logs within the billing period
        for i in range(3):
            db_with_data.execute("""
                INSERT INTO usage_logs (
                    organization_id,
                    chatbot_id,
                    contact_id,
                    message_count,
                    tokens_total,
                    created_at
                )
                VALUES (%s, %s, %s, %s, %s, NOW())
            """, (org_id, chatbot_id, contact_id, 1, 100 + (i * 50)))

        # Call function
        db_with_data.execute("""
            SELECT * FROM get_current_usage(%s)
        """, (org_id,))

        result = db_with_data.fetchone()

        # Should have at least 3 messages and 100 + 150 + 200 = 450 tokens from our inserts
        assert result['messages_used'] >= 3
        assert result['tokens_used'] >= 450

    def test_get_usage_respects_billing_period(self, db_with_data):
        """
        GOAL: Verify function only counts usage within billing period
        GIVEN: Usage logs outside and inside billing period
        WHEN: Function is called
        THEN: Only usage within billing period is counted
        """
        org_id = '11111111-1111-1111-1111-111111111111'

        # Get billing period
        db_with_data.execute("""
            SELECT billing_period_start, billing_period_end
            FROM organizations WHERE id = %s
        """, (org_id,))

        org = db_with_data.fetchone()

        # Add usage outside billing period
        chatbot_id = '22222222-2222-2222-2222-222222222222'
        contact_id = '44444444-4444-4444-4444-444444444444'

        outside_date = org['billing_period_end'] + timedelta(days=1)

        db_with_data.execute("""
            INSERT INTO usage_logs (
                organization_id,
                chatbot_id,
                contact_id,
                message_count,
                tokens_total,
                created_at
            )
            VALUES (%s, %s, %s, %s, %s, %s)
        """, (org_id, chatbot_id, contact_id, 1, 500, outside_date))

        # Get usage - should not include the outside usage
        db_with_data.execute("""
            SELECT * FROM get_current_usage(%s)
        """, (org_id,))

        result = db_with_data.fetchone()

        # Should not include the 500 tokens from outside period
        # (Exact value depends on seed data, but should be < 500 + seed total)
        assert result['tokens_used'] < 2000  # Reasonable upper bound

    def test_get_usage_returns_zero_for_new_org(self, db_with_data):
        """
        GOAL: Verify function returns zero for organization with no usage
        GIVEN: New organization with no usage logs
        WHEN: Function is called
        THEN: 0 messages and 0 tokens are returned
        """
        # Create new org
        db_with_data.execute("""
            INSERT INTO organizations (name, slug)
            VALUES (%s, %s)
            RETURNING id
        """, ('New Org', 'new-org'))

        new_org_id = db_with_data.fetchone()['id']

        # Get usage
        db_with_data.execute("""
            SELECT * FROM get_current_usage(%s)
        """, (new_org_id,))

        result = db_with_data.fetchone()

        assert result['messages_used'] == 0
        assert result['tokens_used'] == 0


# ============================================================================
# 4. CONSTRAINT TESTS
# ============================================================================

class TestUniqueConstraints:
    """Test UNIQUE constraints across tables."""

    def test_organization_slug_must_be_unique(self, db_with_data):
        """
        GOAL: Verify organization slug must be unique
        GIVEN: Organization with slug 'test-slug'
        WHEN: Another organization with same slug is created
        THEN: Unique constraint violation is raised
        """
        db_with_data.execute("""
            INSERT INTO organizations (name, slug)
            VALUES (%s, %s)
        """, ('First Org', 'unique-slug-test'))

        with pytest.raises(psycopg2.IntegrityError) as exc_info:
            db_with_data.execute("""
                INSERT INTO organizations (name, slug)
                VALUES (%s, %s)
            """, ('Second Org', 'unique-slug-test'))

        assert 'unique' in str(exc_info.value).lower()

    def test_user_email_must_be_unique(self, db_with_data):
        """
        GOAL: Verify user email must be unique across all organizations
        GIVEN: User with email 'test@example.com'
        WHEN: Another user with same email is created
        THEN: Unique constraint violation is raised
        """
        org_id = '11111111-1111-1111-1111-111111111111'

        db_with_data.execute("""
            INSERT INTO users (organization_id, email, full_name)
            VALUES (%s, %s, %s)
        """, (org_id, 'unique@test.com', 'First User'))

        with pytest.raises(psycopg2.IntegrityError) as exc_info:
            db_with_data.execute("""
                INSERT INTO users (organization_id, email, full_name)
                VALUES (%s, %s, %s)
            """, (org_id, 'unique@test.com', 'Second User'))

        assert 'email' in str(exc_info.value).lower() or 'unique' in str(exc_info.value).lower()

    def test_webhook_message_id_must_be_unique(self, db_with_data):
        """
        GOAL: Verify WhatsApp message ID must be unique in webhook_events
        GIVEN: Webhook event with message ID
        WHEN: Another event with same message ID is created
        THEN: Unique constraint violation is raised (idempotency)
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        db_with_data.execute("""
            INSERT INTO webhook_events (
                whatsapp_message_id,
                phone_number_id,
                chatbot_id
            )
            VALUES (%s, %s, %s)
        """, ('wamid.unique.test.001', 'phone_123', chatbot_id))

        with pytest.raises(psycopg2.IntegrityError) as exc_info:
            db_with_data.execute("""
                INSERT INTO webhook_events (
                    whatsapp_message_id,
                    phone_number_id,
                    chatbot_id
                )
                VALUES (%s, %s, %s)
            """, ('wamid.unique.test.001', 'phone_123', chatbot_id))

        assert 'whatsapp_message_id' in str(exc_info.value).lower() or 'unique' in str(exc_info.value).lower()


class TestForeignKeyConstraints:
    """Test foreign key constraints and CASCADE behavior."""

    def test_delete_chatbot_cascades_to_contacts(self, db_with_data):
        """
        GOAL: Verify deleting chatbot cascades to contacts
        GIVEN: Chatbot with contacts
        WHEN: Chatbot is deleted
        THEN: All associated contacts are deleted
        """
        org_id = '11111111-1111-1111-1111-111111111111'

        # Create chatbot
        db_with_data.execute("""
            INSERT INTO chatbots (
                organization_id,
                name,
                whatsapp_phone_number_id
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (org_id, 'Cascade Test Bot', 'phone-cascade-001'))

        chatbot_id = db_with_data.fetchone()['id']

        # Create contacts
        contact_ids = []
        for i in range(3):
            db_with_data.execute("""
                INSERT INTO contacts (
                    chatbot_id,
                    phone_number,
                    name
                )
                VALUES (%s, %s, %s)
                RETURNING id
            """, (chatbot_id, f'155500{i:05d}', f'Contact {i}'))
            contact_ids.append(db_with_data.fetchone()['id'])

        # Delete chatbot
        db_with_data.execute("""
            DELETE FROM chatbots WHERE id = %s
        """, (chatbot_id,))

        # Verify contacts deleted
        db_with_data.execute("""
            SELECT COUNT(*) as count FROM contacts
            WHERE id = ANY(%s::uuid[])
        """, (contact_ids,))

        result = db_with_data.fetchone()
        assert result['count'] == 0

    def test_delete_contact_cascades_to_messages(self, db_with_data):
        """
        GOAL: Verify deleting contact cascades to messages
        GIVEN: Contact with message history
        WHEN: Contact is deleted
        THEN: All associated messages are deleted
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create contact
        db_with_data.execute("""
            INSERT INTO contacts (
                chatbot_id,
                phone_number,
                name
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (chatbot_id, '15551234567', 'Message Cascade Test'))

        contact_id = db_with_data.fetchone()['id']

        # Create messages
        message_ids = []
        for i in range(5):
            db_with_data.execute("""
                INSERT INTO messages (
                    contact_id,
                    role,
                    content
                )
                VALUES (%s, %s, %s)
                RETURNING id
            """, (contact_id, 'user' if i % 2 == 0 else 'assistant', f'Message {i}'))
            message_ids.append(db_with_data.fetchone()['id'])

        # Delete contact
        db_with_data.execute("""
            DELETE FROM contacts WHERE id = %s
        """, (contact_id,))

        # Verify messages deleted
        db_with_data.execute("""
            SELECT COUNT(*) as count FROM messages
            WHERE id = ANY(%s::bigint[])
        """, (message_ids,))

        result = db_with_data.fetchone()
        assert result['count'] == 0

    def test_delete_knowledge_source_cascades_to_chunks(self, db_with_data):
        """
        GOAL: Verify deleting knowledge source cascades to chunks
        GIVEN: Knowledge source with document chunks
        WHEN: Knowledge source is deleted
        THEN: All associated chunks are deleted
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        # Create knowledge source
        db_with_data.execute("""
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name
            )
            VALUES (%s, %s, %s)
            RETURNING id
        """, (chatbot_id, 'pdf', 'Cascade Chunks Test.pdf'))

        source_id = db_with_data.fetchone()['id']

        # Create chunks
        chunk_ids = []
        for i in range(10):
            db_with_data.execute("""
                INSERT INTO document_chunks (
                    knowledge_source_id,
                    chatbot_id,
                    content,
                    chunk_index
                )
                VALUES (%s, %s, %s, %s)
                RETURNING id
            """, (source_id, chatbot_id, f'Chunk {i}', i))
            chunk_ids.append(db_with_data.fetchone()['id'])

        # Delete knowledge source
        db_with_data.execute("""
            DELETE FROM knowledge_sources WHERE id = %s
        """, (source_id,))

        # Verify chunks deleted
        db_with_data.execute("""
            SELECT COUNT(*) as count FROM document_chunks
            WHERE id = ANY(%s::uuid[])
        """, (chunk_ids,))

        result = db_with_data.fetchone()
        assert result['count'] == 0

    def test_delete_webhook_event_sets_null_on_usage_logs(self, db_with_data):
        """
        GOAL: Verify deleting webhook event sets usage_logs.webhook_event_id to NULL
        GIVEN: Usage log referencing webhook event
        WHEN: Webhook event is deleted
        THEN: usage_logs.webhook_event_id is set to NULL (not cascade delete)
        """
        org_id = '11111111-1111-1111-1111-111111111111'
        chatbot_id = '22222222-2222-2222-2222-222222222222'
        contact_id = '44444444-4444-4444-4444-444444444444'

        # Create webhook event
        db_with_data.execute("""
            INSERT INTO webhook_events (
                whatsapp_message_id,
                phone_number_id,
                chatbot_id,
                status
            )
            VALUES (%s, %s, %s, %s)
            RETURNING id
        """, ('wamid.set.null.test', 'phone_123', chatbot_id, 'completed'))

        webhook_id = db_with_data.fetchone()['id']

        # Create usage log
        db_with_data.execute("""
            INSERT INTO usage_logs (
                organization_id,
                chatbot_id,
                contact_id,
                webhook_event_id,
                message_count,
                tokens_total
            )
            VALUES (%s, %s, %s, %s, %s, %s)
            RETURNING id
        """, (org_id, chatbot_id, contact_id, webhook_id, 1, 100))

        usage_log_id = db_with_data.fetchone()['id']

        # Delete webhook event
        db_with_data.execute("""
            DELETE FROM webhook_events WHERE id = %s
        """, (webhook_id,))

        # Verify usage log still exists but webhook_event_id is NULL
        db_with_data.execute("""
            SELECT webhook_event_id FROM usage_logs WHERE id = %s
        """, (usage_log_id,))

        result = db_with_data.fetchone()
        assert result is not None
        assert result['webhook_event_id'] is None


class TestCheckConstraints:
    """Test CHECK constraints if any exist."""

    def test_organization_plan_tier_valid_values(self, db_with_data):
        """
        GOAL: Verify plan_tier accepts standard values
        GIVEN: Database connection
        WHEN: Organization is created with valid plan_tier
        THEN: Organization is created successfully
        """
        # Test all valid plan tiers
        for plan_tier in ['free', 'starter', 'pro', 'enterprise']:
            db_with_data.execute("""
                INSERT INTO organizations (name, slug, plan_tier)
                VALUES (%s, %s, %s)
                RETURNING plan_tier
            """, (f'{plan_tier} org', f'{plan_tier}-org-test', plan_tier))

            result = db_with_data.fetchone()
            assert result['plan_tier'] == plan_tier

    def test_message_role_valid_values(self, db_with_data):
        """
        GOAL: Verify message role accepts standard values
        GIVEN: Valid contact
        WHEN: Messages are created with different roles
        THEN: All standard roles are accepted
        """
        contact_id = '44444444-4444-4444-4444-444444444444'

        for role in ['user', 'assistant', 'system', 'tool']:
            db_with_data.execute("""
                INSERT INTO messages (contact_id, role, content)
                VALUES (%s, %s, %s)
                RETURNING role
            """, (contact_id, role, f'Test {role} message'))

            result = db_with_data.fetchone()
            assert result['role'] == role

    def test_contact_conversation_mode_valid_values(self, db_with_data):
        """
        GOAL: Verify conversation_mode accepts 'auto' and 'manual'
        GIVEN: Chatbot
        WHEN: Contacts are created with different modes
        THEN: Both modes are accepted
        """
        chatbot_id = '22222222-2222-2222-2222-222222222222'

        for mode in ['auto', 'manual']:
            db_with_data.execute("""
                INSERT INTO contacts (
                    chatbot_id,
                    phone_number,
                    name,
                    conversation_mode
                )
                VALUES (%s, %s, %s, %s)
                RETURNING conversation_mode
            """, (chatbot_id, f'15559999{mode[:3]}', f'{mode} Contact', mode))

            result = db_with_data.fetchone()
            assert result['conversation_mode'] == mode


================================================
File: tests/test_harness/llm_mock.py
================================================
"""
Mock LLM providers for testing.

This module mocks:
- OpenAI API
- Google Gemini API
"""

from typing import Dict, Any, List, Optional
from unittest.mock import Mock
from dataclasses import dataclass


@dataclass
class MockLLMResponse:
    """Mock LLM response structure."""
    text: str
    tokens_input: int
    tokens_output: int
    model: str
    provider: str


class LLMMock:
    """Mock for LLM providers (OpenAI, Google Gemini)."""
    
    def __init__(self):
        self.responses = []
        self.call_history = []
        self.default_response = MockLLMResponse(
            text="This is a test response from the AI assistant.",
            tokens_input=100,
            tokens_output=50,
            model="test-model",
            provider="test"
        )
    
    def add_response(
        self,
        text: str,
        tokens_input: int = 100,
        tokens_output: int = 50,
        model: str = "test-model",
        provider: str = "test"
    ):
        """
        Add a response to the queue.
        Responses are returned in FIFO order.
        
        Args:
            text: Response text
            tokens_input: Input token count
            tokens_output: Output token count
            model: Model name
            provider: Provider name
        """
        self.responses.append(MockLLMResponse(
            text=text,
            tokens_input=tokens_input,
            tokens_output=tokens_output,
            model=model,
            provider=provider
        ))
    
    def get_next_response(self) -> MockLLMResponse:
        """Get next response from queue, or default if queue is empty."""
        if self.responses:
            return self.responses.pop(0)
        return self.default_response
    
    def get_openai_client(self, api_key: str):
        """
        Mock OpenAI client.
        
        Returns:
            Mock OpenAI client object
        """
        mock_client = Mock()
        
        def create_completion(**kwargs):
            # Record the call
            self.call_history.append({
                "provider": "openai",
                "kwargs": kwargs
            })
            
            # Get response
            response = self.get_next_response()
            
            # Create mock response object
            mock_response = Mock()
            mock_message = Mock()
            mock_message.content = response.text
            mock_choice = Mock()
            mock_choice.message = mock_message
            mock_response.choices = [mock_choice]
            
            # Add usage info
            mock_usage = Mock()
            mock_usage.prompt_tokens = response.tokens_input
            mock_usage.completion_tokens = response.tokens_output
            mock_usage.total_tokens = response.tokens_input + response.tokens_output
            mock_response.usage = mock_usage
            
            return mock_response
        
        mock_client.chat.completions.create = create_completion
        return mock_client
    
    def get_google_client(self, model_name: str):
        """
        Mock Google Gemini client.
        
        Returns:
            Mock Google GenerativeModel object
        """
        mock_model = Mock()
        
        def start_chat(**kwargs):
            mock_chat = Mock()
            
            def send_message(content: str):
                # Record the call
                self.call_history.append({
                    "provider": "google",
                    "model": model_name,
                    "content": content
                })
                
                # Get response
                response = self.get_next_response()
                
                # Create mock response
                mock_response = Mock()
                mock_response.text = response.text
                
                # Add usage metadata if available
                mock_metadata = Mock()
                mock_metadata.prompt_token_count = response.tokens_input
                mock_metadata.candidates_token_count = response.tokens_output
                mock_metadata.total_token_count = response.tokens_input + response.tokens_output
                mock_response.usage_metadata = mock_metadata
                
                return mock_response
            
            mock_chat.send_message = send_message
            return mock_chat
        
        mock_model.start_chat = start_chat
        return mock_model
    
    def get_call_count(self, provider: Optional[str] = None) -> int:
        """
        Get number of LLM calls made.
        
        Args:
            provider: Filter by provider (openai, google), or None for all
        
        Returns:
            Number of calls
        """
        if provider:
            return sum(1 for call in self.call_history if call.get("provider") == provider)
        return len(self.call_history)
    
    def get_last_call(self) -> Optional[Dict[str, Any]]:
        """Get the last LLM call made."""
        return self.call_history[-1] if self.call_history else None
    
    def clear_history(self):
        """Clear call history."""
        self.call_history.clear()
    
    def reset(self):
        """Reset all responses and history."""
        self.responses.clear()
        self.call_history.clear()


class LLMResponseBuilder:
    """Builder for creating complex mock LLM responses."""
    
    def __init__(self):
        self.response = {
            "text": "",
            "tokens_input": 100,
            "tokens_output": 50,
            "model": "test-model",
            "provider": "test",
            "tool_calls": None,
        }
    
    def with_text(self, text: str):
        """Set response text."""
        self.response["text"] = text
        return self
    
    def with_tokens(self, input_tokens: int, output_tokens: int):
        """Set token counts."""
        self.response["tokens_input"] = input_tokens
        self.response["tokens_output"] = output_tokens
        return self
    
    def with_model(self, model: str, provider: str):
        """Set model and provider."""
        self.response["model"] = model
        self.response["provider"] = provider
        return self
    
    def with_tool_call(self, tool_name: str, arguments: Dict[str, Any]):
        """Add a tool call."""
        if self.response["tool_calls"] is None:
            self.response["tool_calls"] = []
        
        self.response["tool_calls"].append({
            "name": tool_name,
            "arguments": arguments
        })
        return self
    
    def build(self) -> MockLLMResponse:
        """Build the response object."""
        return MockLLMResponse(**{
            k: v for k, v in self.response.items()
            if k in ["text", "tokens_input", "tokens_output", "model", "provider"]
        })

================================================
File: tests/unit/test_web_crawler.py
================================================
"""
Unit tests for web crawler functionality.

Tests the web_crawler.py utility including URL discovery, relevance scoring,
robots.txt compliance, and rate limiting.
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
import sys
from pathlib import Path
import time
import requests_mock as rm

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "f" / "development" / "utils"))

from web_crawler import main as crawl_url, calculate_relevance_score


@pytest.mark.unit
class TestWebCrawler:
    """Test web crawling functionality."""

    @pytest.fixture
    def mock_html_response(self):
        """Mock HTML response with links."""
        return """
        <!DOCTYPE html>
        <html>
        <head><title>Test Documentation</title></head>
        <body>
            <h1>Documentation</h1>
            <a href="/docs/getting-started">Getting Started</a>
            <a href="/docs/api/reference">API Reference</a>
            <a href="/faq">FAQ</a>
            <a href="/support">Support</a>
            <a href="https://external-site.com">External Link</a>
            <a href="#section">Anchor Link</a>
        </body>
        </html>
        """

    @pytest.fixture
    def mock_robots_txt(self):
        """Mock robots.txt allowing all paths."""
        return """
        User-agent: *
        Allow: /
        """

    @pytest.fixture
    def mock_robots_txt_disallow(self):
        """Mock robots.txt disallowing some paths."""
        return """
        User-agent: *
        Disallow: /admin
        Disallow: /private
        Allow: /
        """

    def test_basic_crawl_success(self, mock_html_response, mock_robots_txt):
        """Test basic successful crawl of a URL."""
        base_url = "https://example.com"

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'), \
             patch('web_crawler.urllib.robotparser.RobotFileParser') as mock_robot_parser:

            # Configure the robot parser mock - need to mock read() and set_url() methods
            robot_instance = Mock()
            robot_instance.can_fetch = Mock(return_value=True)
            robot_instance.read = Mock()
            robot_instance.set_url = Mock()
            mock_robot_parser.return_value = robot_instance

            # Mock page response - use dict for headers instead of MagicMock
            page_response = Mock()
            page_response.status_code = 200
            page_response.text = mock_html_response
            # Use a real dict for headers so .get() and 'in' work properly
            page_response.headers = {"Content-Type": "text/html; charset=utf-8"}
            page_response.raise_for_status = Mock()

            # requests.get is only used for the actual page fetch
            mock_get.return_value = page_response

            # Execute crawl
            result = crawl_url(
                base_url=base_url,
                max_depth=1,
                max_pages=10
            )

            # Assert basic structure
            assert "discovered_urls" in result
            assert "total_discovered" in result
            assert "crawl_time_seconds" in result
            assert "base_domain" in result
            assert result["base_domain"] == "example.com"

            # Should discover at least the base URL
            assert result["total_discovered"] >= 1

    def test_relevance_scoring(self):
        """Test relevance score calculation."""
        base_domain = "example.com"
        keywords = ["docs", "api", "faq", "support"]

        # Test 1: Same domain + keyword in path (high score)
        score = calculate_relevance_score(
            url="https://example.com/docs/getting-started",
            title="Getting Started - Documentation",
            depth=0,
            base_domain=base_domain,
            keywords=keywords
        )
        assert score >= 0.7  # Same domain (0.4) + keyword (0.3) + no depth penalty

        # Test 2: Different domain (lower score)
        score = calculate_relevance_score(
            url="https://other-site.com/docs",
            title="Documentation",
            depth=0,
            base_domain=base_domain,
            keywords=keywords
        )
        assert score <= 0.4  # Only keyword bonus, no domain match

        # Test 3: Depth penalty
        score_depth_0 = calculate_relevance_score(
            url="https://example.com/docs",
            title="Docs",
            depth=0,
            base_domain=base_domain,
            keywords=keywords
        )
        score_depth_2 = calculate_relevance_score(
            url="https://example.com/docs",
            title="Docs",
            depth=2,
            base_domain=base_domain,
            keywords=keywords
        )
        assert score_depth_0 > score_depth_2  # Depth 0 should score higher

        # Test 4: No keyword match
        score = calculate_relevance_score(
            url="https://example.com/random/page",
            title="Random Page",
            depth=0,
            base_domain=base_domain,
            keywords=keywords
        )
        assert score == 0.4  # Only domain match bonus

    def test_suggested_flag_threshold(self):
        """Test that URLs with score > 0.5 are marked as suggested."""
        base_domain = "example.com"
        keywords = ["docs", "api"]

        # High score URL should be suggested
        high_score = calculate_relevance_score(
            url="https://example.com/docs/api",
            title="API Documentation",
            depth=0,
            base_domain=base_domain,
            keywords=keywords
        )
        assert high_score > 0.5

        # Low score URL should not be suggested
        low_score = calculate_relevance_score(
            url="https://other-site.com/random",
            title="Random",
            depth=3,
            base_domain=base_domain,
            keywords=keywords
        )
        assert low_score <= 0.5

    def test_robots_txt_compliance(self, mock_robots_txt_disallow):
        """Test that crawler respects robots.txt disallow rules."""
        base_url = "https://example.com"

        html_with_disallowed = """
        <html><body>
            <a href="/admin/users">Admin</a>
            <a href="/docs">Docs</a>
            <a href="/private/data">Private</a>
        </body></html>
        """

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'):
            robots_response = Mock()
            robots_response.status_code = 200
            robots_response.text = mock_robots_txt_disallow
            robots_response.raise_for_status = Mock()

            page_response = Mock()
            page_response.status_code = 200
            page_response.text = html_with_disallowed
            page_response.headers = {"Content-Type": "text/html; charset=utf-8"}
            page_response.raise_for_status = Mock()

            mock_get.side_effect = [robots_response, page_response]

            result = crawl_url(
                base_url=base_url,
                max_depth=2,
                max_pages=10
            )

            # Check that disallowed URLs are not in results
            discovered_urls = [item["url"] for item in result["discovered_urls"]]

            # Should not contain disallowed paths
            assert not any("/admin" in url for url in discovered_urls)
            assert not any("/private" in url for url in discovered_urls)

    def test_max_depth_limit(self, mock_html_response, mock_robots_txt):
        """Test that crawler respects max_depth limit."""
        base_url = "https://example.com"

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'):
            robots_response = Mock()
            robots_response.status_code = 200
            robots_response.text = mock_robots_txt
            robots_response.raise_for_status = Mock()

            page_response = Mock()
            page_response.status_code = 200
            page_response.text = mock_html_response
            page_response.headers = {"Content-Type": "text/html; charset=utf-8"}
            page_response.raise_for_status = Mock()

            mock_get.side_effect = [robots_response, page_response]

            result = crawl_url(
                base_url=base_url,
                max_depth=0,  # Should only crawl base URL
                max_pages=10
            )

            # With max_depth=0, should only have base URL or very few results
            assert result["total_discovered"] <= 2

    def test_max_pages_limit(self, mock_robots_txt):
        """Test that crawler respects max_pages limit."""
        base_url = "https://example.com"

        # Create HTML with many links
        many_links_html = """
        <html><body>
        """ + "\n".join([f'<a href="/page{i}">Page {i}</a>' for i in range(100)]) + """
        </body></html>
        """

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'):
            robots_response = Mock()
            robots_response.status_code = 200
            robots_response.text = mock_robots_txt
            robots_response.raise_for_status = Mock()

            page_response = Mock()
            page_response.status_code = 200
            page_response.text = many_links_html
            page_response.headers = {"Content-Type": "text/html; charset=utf-8"}
            page_response.raise_for_status = Mock()

            # Always return appropriate response
            def get_side_effect(*args, **kwargs):
                if "robots.txt" in args[0]:
                    return robots_response
                return page_response

            mock_get.side_effect = get_side_effect

            result = crawl_url(
                base_url=base_url,
                max_depth=2,
                max_pages=10  # Limit to 10 pages
            )

            # Should not exceed max_pages
            assert result["total_discovered"] <= 10

    def test_same_domain_only_filter(self, mock_robots_txt):
        """Test same_domain_only parameter filters external links."""
        base_url = "https://example.com"

        mixed_links_html = """
        <html><body>
            <a href="/internal/page1">Internal 1</a>
            <a href="/internal/page2">Internal 2</a>
            <a href="https://external.com/page">External</a>
            <a href="https://another-site.com/page">Another External</a>
        </body></html>
        """

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'):
            robots_response = Mock()
            robots_response.status_code = 200
            robots_response.text = mock_robots_txt
            robots_response.raise_for_status = Mock()

            page_response = Mock()
            page_response.status_code = 200
            page_response.text = mixed_links_html
            page_response.headers = {"Content-Type": "text/html; charset=utf-8"}
            page_response.raise_for_status = Mock()

            mock_get.side_effect = [robots_response, page_response]

            result = crawl_url(
                base_url=base_url,
                max_depth=1,
                max_pages=10,
                same_domain_only=True
            )

            # All discovered URLs should be from example.com
            for item in result["discovered_urls"]:
                assert "example.com" in item["url"]

    def test_error_handling_network_error(self):
        """Test graceful handling of network errors."""
        base_url = "https://example.com"

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'):
            # Simulate network error for all requests
            import requests
            mock_get.side_effect = requests.RequestException("Network error")

            result = crawl_url(
                base_url=base_url,
                max_depth=1,
                max_pages=10
            )

            # Should return valid result structure even on error, just with no discovered URLs
            assert "discovered_urls" in result
            assert "total_discovered" in result
            assert result["total_discovered"] == 0

    def test_error_handling_404(self, mock_robots_txt):
        """Test handling of 404 responses."""
        base_url = "https://example.com"

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'):
            robots_response = Mock()
            robots_response.status_code = 200
            robots_response.text = mock_robots_txt
            robots_response.raise_for_status = Mock()

            page_response = Mock()
            page_response.status_code = 404
            # 404 should raise an HTTPError when raise_for_status is called
            import requests
            page_response.raise_for_status = Mock(side_effect=requests.HTTPError("404 Not Found"))

            mock_get.side_effect = [robots_response, page_response]

            result = crawl_url(
                base_url=base_url,
                max_depth=1,
                max_pages=10
            )

            # Should handle 404 gracefully and return valid structure
            assert "discovered_urls" in result
            assert "total_discovered" in result

    def test_custom_keywords_filtering(self, mock_robots_txt):
        """Test custom keyword filtering."""
        base_url = "https://example.com"
        custom_keywords = ["pricing", "features", "contact"]

        html_with_keywords = """
        <html><body>
            <a href="/pricing">Pricing</a>
            <a href="/features">Features</a>
            <a href="/blog">Blog</a>
        </body></html>
        """

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'):
            robots_response = Mock()
            robots_response.status_code = 200
            robots_response.text = mock_robots_txt
            robots_response.raise_for_status = Mock()

            page_response = Mock()
            page_response.status_code = 200
            page_response.text = html_with_keywords
            page_response.headers = {"Content-Type": "text/html; charset=utf-8"}
            page_response.raise_for_status = Mock()

            mock_get.side_effect = [robots_response, page_response]

            result = crawl_url(
                base_url=base_url,
                max_depth=1,
                max_pages=10,
                filter_keywords=custom_keywords
            )

            # URLs with keywords should have higher relevance scores
            for item in result["discovered_urls"]:
                if any(kw in item["url"].lower() for kw in custom_keywords):
                    assert item["relevance_score"] > 0.5

    def test_rate_limiting_enforced(self, mock_html_response, mock_robots_txt):
        """Test that crawler enforces 1 request/second rate limit."""
        base_url = "https://example.com"

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep') as mock_sleep:

            robots_response = Mock()
            robots_response.status_code = 200
            robots_response.text = mock_robots_txt
            robots_response.raise_for_status = Mock()

            page_response = Mock()
            page_response.status_code = 200
            page_response.text = mock_html_response
            page_response.headers = {"Content-Type": "text/html; charset=utf-8"}
            page_response.raise_for_status = Mock()

            mock_get.side_effect = [robots_response, page_response, page_response]

            result = crawl_url(
                base_url=base_url,
                max_depth=1,
                max_pages=5
            )

            # Should have called sleep to rate limit (at least once)
            assert mock_sleep.call_count >= 1
            # Should sleep 1 second between requests
            mock_sleep.assert_called_with(1)

    def test_content_preview_extraction(self, mock_robots_txt):
        """Test extraction of content preview from pages."""
        base_url = "https://example.com"

        html_with_content = """
        <html><body>
            <h1>Main Title</h1>
            <p>This is the first paragraph with some content that should be extracted as a preview.</p>
            <p>This is another paragraph.</p>
        </body></html>
        """

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'):
            robots_response = Mock()
            robots_response.status_code = 200
            robots_response.text = mock_robots_txt
            robots_response.raise_for_status = Mock()

            page_response = Mock()
            page_response.status_code = 200
            page_response.text = html_with_content
            page_response.headers = {"Content-Type": "text/html; charset=utf-8"}
            page_response.raise_for_status = Mock()

            mock_get.side_effect = [robots_response, page_response]

            result = crawl_url(
                base_url=base_url,
                max_depth=0,
                max_pages=1
            )

            # Should have content preview
            if result["discovered_urls"]:
                first_url = result["discovered_urls"][0]
                assert "content_preview" in first_url
                assert len(first_url["content_preview"]) > 0

    def test_crawl_statistics(self, mock_html_response, mock_robots_txt):
        """Test that crawl statistics are correctly reported."""
        base_url = "https://example.com"

        with patch('web_crawler.requests.get') as mock_get, \
             patch('web_crawler.time.sleep'), \
             patch('web_crawler.urllib.robotparser.RobotFileParser') as mock_robot_parser:

            # Configure the robot parser mock - need to mock read() and set_url() methods
            robot_instance = Mock()
            robot_instance.can_fetch = Mock(return_value=True)
            robot_instance.read = Mock()
            robot_instance.set_url = Mock()
            mock_robot_parser.return_value = robot_instance

            page_response = Mock()
            page_response.status_code = 200
            page_response.text = mock_html_response
            # Use a real dict for headers so .get() and 'in' work properly
            page_response.headers = {"Content-Type": "text/html; charset=utf-8"}
            page_response.raise_for_status = Mock()

            # requests.get is only used for the actual page fetch
            mock_get.return_value = page_response

            result = crawl_url(
                base_url=base_url,
                max_depth=1,
                max_pages=10
            )

            # Should have basic statistics
            assert "total_discovered" in result
            assert "crawl_time_seconds" in result
            assert "base_domain" in result
            assert result["total_discovered"] >= 1
            assert result["crawl_time_seconds"] >= 0


================================================
File: f/development/RAG_process_documents.py
================================================
"""
Document Processing Pipeline for RAG

This script handles:
1. PDF extraction
2. Web page scraping
3. Text chunking
4. Embedding generation
5. Storage in pgvector

Can be triggered:
- On document upload
- As a scheduled job for batch processing
- On-demand via API
"""

import wmill
import psycopg2
from psycopg2.extras import RealDictCursor
from typing import List, Dict, Any, Optional
import hashlib
import re
from openai import OpenAI


def main(
    knowledge_source_id: str,
    chatbot_id: str,
    openai_api_key: str = "",  # From Windmill variable
    chunk_size: int = 1000,  # Characters per chunk
    chunk_overlap: int = 200,  # Overlap between chunks
    db_resource: str = "f/development/business_layer_db_postgreSQL",
) -> Dict[str, Any]:
    """
    Process a knowledge source and create embeddings.
    
    Args:
        knowledge_source_id: UUID of the knowledge source
        chatbot_id: UUID of the chatbot
        openai_api_key: OpenAI API key for embeddings
        chunk_size: Size of each text chunk
        chunk_overlap: Overlap between consecutive chunks
        db_resource: Database resource path
    
    Returns:
        Processing results with stats
    """
    
    # Setup database
    raw_config = wmill.get_resource(db_resource)
    db_params = {
        "host": raw_config.get("host"),
        "port": raw_config.get("port"),
        "user": raw_config.get("user"),
        "password": raw_config.get("password"),
        "dbname": raw_config.get("dbname"),
        "sslmode": "disable",
    }
    
    conn = psycopg2.connect(**db_params)
    cur = conn.cursor(cursor_factory=RealDictCursor)
    
    try:
        # 1. Fetch knowledge source
        cur.execute(
            """
            SELECT * FROM knowledge_sources 
            WHERE id = %s AND chatbot_id = %s
            """,
            (knowledge_source_id, chatbot_id)
        )
        source = cur.fetchone()
        
        if not source:
            return {"success": False, "error": "Knowledge source not found"}
        
        # Update status to processing
        cur.execute(
            """
            UPDATE knowledge_sources 
            SET sync_status = 'processing', last_synced_at = NOW()
            WHERE id = %s
            """,
            (knowledge_source_id,)
        )
        conn.commit()
        
        # 2. Extract content based on source type
        content = extract_content(source)
        
        if not content:
            _mark_failed(cur, knowledge_source_id, "Failed to extract content")
            conn.commit()
            return {"success": False, "error": "Content extraction failed"}
        
        # 3. Chunk the content
        chunks = chunk_text(
            content,
            chunk_size=chunk_size,
            overlap=chunk_overlap
        )
        
        print(f"Created {len(chunks)} chunks from source")
        
        # 4. Generate embeddings
        embeddings = generate_embeddings(chunks, openai_api_key)
        
        if not embeddings:
            _mark_failed(cur, knowledge_source_id, "Failed to generate embeddings")
            conn.commit()
            return {"success": False, "error": "Embedding generation failed"}
        
        # 5. Delete old chunks (if re-processing)
        cur.execute(
            "DELETE FROM document_chunks WHERE knowledge_source_id = %s",
            (knowledge_source_id,)
        )
        
        # 6. Insert new chunks with embeddings
        inserted_count = 0
        for i, (chunk_text, embedding) in enumerate(zip(chunks, embeddings)):
            # Extract metadata (page numbers, headers, etc.)
            metadata = extract_chunk_metadata(chunk_text, i, source)
            
            cur.execute(
                """
                INSERT INTO document_chunks (
                    knowledge_source_id,
                    chatbot_id,
                    content,
                    chunk_index,
                    embedding,
                    metadata
                ) VALUES (%s, %s, %s, %s, %s, %s)
                """,
                (
                    knowledge_source_id,
                    chatbot_id,
                    chunk_text,
                    i,
                    embedding,
                    metadata
                )
            )
            inserted_count += 1
        
        # 7. Update knowledge source status
        # Note: dimensions are hardcoded to 1536 for now, must match the embedding model used
        cur.execute(
            """
            UPDATE knowledge_sources 
            SET sync_status = 'synced',
                last_synced_at = NOW(),
                chunks_count = %s,
                embedding_model = 'text-embedding-ada-002',
                embedding_dimensions = 1536
            WHERE id = %s
            """,
            (len(chunks), knowledge_source_id)
        )
        
        conn.commit()
        
        return {
            "success": True,
            "chunks_created": len(chunks),
            "embeddings_generated": len(embeddings),
            "source_type": source["source_type"],
            "source_name": source["name"],
        }
        
    except Exception as e:
        print(f"Error processing document: {e}")
        conn.rollback()
        _mark_failed(cur, knowledge_source_id, str(e))
        conn.commit()
        return {"success": False, "error": str(e)}
        
    finally:
        cur.close()
        conn.close()


def extract_content(source: Dict[str, Any]) -> Optional[str]:
    """
    Extract text content from various source types.
    
    Args:
        source: Knowledge source record
    
    Returns:
        Extracted text content or None
    """
    source_type = source["source_type"]
    
    if source_type == "text":
        # Direct text input
        return source.get("content") or ""
    
    elif source_type == "pdf":
        # Extract text from PDF
        return extract_pdf_text(source["file_path"])
    
    elif source_type == "url":
        # Scrape web page
        return scrape_webpage(source["file_path"])  # file_path stores URL
    
    elif source_type == "doc":
        # Extract from Word doc
        return extract_doc_text(source["file_path"])
    
    return None


def extract_pdf_text(file_path: str) -> str:
    """
    Extract text from PDF file.
    
    Implementation options:
    - PyPDF2 (simple, pure Python)
    - pdfplumber (better for tables)
    - pymupdf (fastest)
    - Unstructured.io (best quality, slower)
    
    For MVP, using PyPDF2:
    """
    try:
        import PyPDF2
        
        text = ""
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                # Add page markers for metadata
                text += f"\n[PAGE {page_num + 1}]\n{page_text}\n"
        
        return text.strip()
        
    except Exception as e:
        print(f"PDF extraction error: {e}")
        return ""


def scrape_webpage(url: str) -> str:
    """
    Scrape text from webpage.
    
    Implementation options:
    - BeautifulSoup + requests (simple)
    - trafilatura (better text extraction)
    - newspaper3k (for articles)
    
    For MVP, using trafilatura (best balance):
    """
    try:
        import trafilatura
        
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return ""
        
        text = trafilatura.extract(
            downloaded,
            include_comments=False,
            include_tables=True,
            output_format='txt'
        )
        
        return text or ""
        
    except Exception as e:
        print(f"Web scraping error: {e}")
        return ""


def extract_doc_text(file_path: str) -> str:
    """
    Extract text from Word document.
    
    Using python-docx:
    """
    try:
        import docx
        
        doc = docx.Document(file_path)
        text = "\n\n".join([paragraph.text for paragraph in doc.paragraphs])
        return text.strip()
        
    except Exception as e:
        print(f"Doc extraction error: {e}")
        return ""


def chunk_text(
    text: str,
    chunk_size: int = 1000,
    overlap: int = 200
) -> List[str]:
    """
    Split text into overlapping chunks.
    
    Strategy:
    1. Try to split on paragraph boundaries
    2. Fall back to sentence boundaries
    3. Hard split if needed (rare)
    
    Args:
        text: Text to chunk
        chunk_size: Target size of each chunk (characters)
        overlap: Overlap between chunks (characters)
    
    Returns:
        List of text chunks
    """
    if not text:
        return []
    
    # Clean text
    text = re.sub(r'\s+', ' ', text).strip()
    
    # If text is smaller than chunk_size, return as-is
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        # Calculate end position
        end = start + chunk_size
        
        # If this is the last chunk
        if end >= len(text):
            chunks.append(text[start:].strip())
            break
        
        # Try to find a good breaking point
        # 1. Look for paragraph break (double newline)
        break_point = text.rfind('\n\n', start, end)
        
        # 2. Look for single newline
        if break_point == -1 or break_point < start + chunk_size // 2:
            break_point = text.rfind('\n', start, end)
        
        # 3. Look for sentence end
        if break_point == -1 or break_point < start + chunk_size // 2:
            break_point = text.rfind('. ', start, end)
        
        # 4. Look for any space
        if break_point == -1 or break_point < start + chunk_size // 2:
            break_point = text.rfind(' ', start, end)
        
        # 5. Hard break (should be rare)
        if break_point == -1 or break_point < start + chunk_size // 2:
            break_point = end
        
        # Add chunk
        chunk = text[start:break_point].strip()
        if chunk:
            chunks.append(chunk)
        
        # Move start position (with overlap)
        start = break_point - overlap
        if start < 0:
            start = 0
    
    return chunks


def generate_embeddings(
    chunks: List[str],
    api_key: str,
    model: str = "text-embedding-ada-002"
) -> List[List[float]]:
    """
    Generate embeddings for text chunks using OpenAI.
    
    Args:
        chunks: List of text chunks
        api_key: OpenAI API key
        model: Embedding model to use
    
    Returns:
        List of embedding vectors
    """
    if not chunks or not api_key:
        return []
    
    try:
        client = OpenAI(api_key=api_key)
        
        # Batch embeddings for efficiency (OpenAI allows up to 2048 inputs)
        # For safety, batch in groups of 100
        embeddings = []
        batch_size = 100
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            
            response = client.embeddings.create(
                model=model,
                input=batch
            )
            
            batch_embeddings = [item.embedding for item in response.data]
            embeddings.extend(batch_embeddings)
        
        return embeddings
        
    except Exception as e:
        print(f"Embedding generation error: {e}")
        return []


def extract_chunk_metadata(
    chunk_text: str,
    chunk_index: int,
    source: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Extract metadata from chunk text.
    
    Metadata can include:
    - Page numbers (from PDF)
    - Headers/titles
    - URLs (from web pages)
    - Timestamps
    
    Args:
        chunk_text: The chunk text
        chunk_index: Index of this chunk
        source: Source record
    
    Returns:
        Metadata dict
    """
    metadata = {
        "chunk_index": chunk_index,
        "source_type": source["source_type"],
        "source_name": source["name"],
    }
    
    # Extract page number from PDF chunks
    page_match = re.search(r'\[PAGE (\d+)\]', chunk_text)
    if page_match:
        metadata["page"] = int(page_match.group(1))
    
    # For URLs, include the source
    if source["source_type"] == "url":
        metadata["url"] = source["file_path"]
    
    # Extract first line as potential title/header
    lines = chunk_text.split('\n')
    if lines:
        first_line = lines[0].strip()
        if len(first_line) < 100:  # Likely a header
            metadata["header"] = first_line
    
    return metadata


def _mark_failed(cur, knowledge_source_id: str, error_message: str):
    """Mark knowledge source as failed."""
    cur.execute(
        """
        UPDATE knowledge_sources 
        SET sync_status = 'failed',
            error_message = %s,
            last_synced_at = NOW()
        WHERE id = %s
        """,
        (error_message, knowledge_source_id)
    )

================================================
File: tests/fixtures/sample_webhook_payloads.json
================================================


================================================
File: tests/test_harness/whatsapp_mock.py
================================================
"""
Mock WhatsApp API for testing.

This module mocks:
- requests.post() for WhatsApp API calls
- Webhook verification
- Message sending
"""

from typing import Dict, Any, List, Optional
from unittest.mock import Mock
import json
import requests


class WhatsAppMock:
    """Mock for WhatsApp Cloud API."""
    
    def __init__(self):
        self.sent_messages = []
        self.should_fail = False
        self.failure_status_code = 400
        self.failure_message = "Bad Request"
    
    def post(self, url: str, **kwargs) -> Mock:
        """
        Mock requests.post() for WhatsApp API calls.
        
        Args:
            url: API endpoint URL
            **kwargs: Request arguments (headers, json, data, etc.)
        
        Returns:
            Mock Response object
        """
        # Create mock response
        response = Mock()
        
        # Check if this is a WhatsApp API call
        if "graph.facebook.com" in url and "/messages" in url:
            return self._mock_send_message(url, response, **kwargs)
        
        # Default: successful response
        response.status_code = 200
        response.ok = True
        response.json.return_value = {"success": True}
        response.text = json.dumps({"success": True})
        return response
    
    def _mock_send_message(self, url: str, response: Mock, **kwargs) -> Mock:
        """Mock sending a WhatsApp message."""
        # Extract phone_number_id from URL
        # URL format: https://graph.facebook.com/v22.0/{phone_number_id}/messages
        parts = url.split("/")
        phone_number_id = parts[-2] if len(parts) >= 2 else "unknown"
        
        # Get message data
        message_data = kwargs.get("json", {})
        headers = kwargs.get("headers", {})
        
        # Record the message
        sent_message = {
            "url": url,
            "phone_number_id": phone_number_id,
            "to": message_data.get("to"),
            "text": message_data.get("text", {}).get("body"),
            "type": message_data.get("type"),
            "headers": headers,
            "full_payload": message_data,
        }
        self.sent_messages.append(sent_message)
        
        # Simulate failure if configured
        if self.should_fail:
            response.status_code = self.failure_status_code
            response.ok = False
            response.json.return_value = {
                "error": {
                    "message": self.failure_message,
                    "type": "OAuthException",
                    "code": self.failure_status_code
                }
            }
            response.text = json.dumps(response.json.return_value)
            # Raise HTTPError (subclass of RequestException) to match real requests behavior
            http_error = requests.exceptions.HTTPError(self.failure_message)
            http_error.response = response
            response.raise_for_status.side_effect = http_error
            return response
        
        # Successful response
        response.status_code = 200
        response.ok = True
        response.json.return_value = {
            "messaging_product": "whatsapp",
            "contacts": [{
                "input": message_data.get("to"),
                "wa_id": message_data.get("to")
            }],
            "messages": [{
                "id": f"wamid.mock.{len(self.sent_messages)}"
            }]
        }
        response.text = json.dumps(response.json.return_value)
        
        return response
    
    def get_sent_messages(self, to_phone: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Get list of sent messages.
        
        Args:
            to_phone: Filter by recipient phone number (optional)
        
        Returns:
            List of sent messages
        """
        if to_phone:
            return [msg for msg in self.sent_messages if msg["to"] == to_phone]
        return self.sent_messages.copy()
    
    def get_last_message(self, to_phone: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        Get the last sent message.
        
        Args:
            to_phone: Filter by recipient phone number (optional)
        
        Returns:
            Last message dict or None
        """
        messages = self.get_sent_messages(to_phone)
        return messages[-1] if messages else None
    
    def assert_message_sent(
        self,
        to_phone: str,
        expected_text: Optional[str] = None,
        contains: Optional[str] = None
    ):
        """
        Assert that a message was sent to a specific phone number.
        
        Args:
            to_phone: Expected recipient phone number
            expected_text: Exact text expected (optional)
            contains: Text that should be contained in message (optional)
        
        Raises:
            AssertionError: If message not found or doesn't match criteria
        """
        messages = self.get_sent_messages(to_phone)
        
        assert messages, f"No messages sent to {to_phone}"
        
        if expected_text is not None:
            texts = [msg["text"] for msg in messages]
            assert expected_text in texts, \
                f"Expected text '{expected_text}' not found in messages: {texts}"
        
        if contains is not None:
            last_text = messages[-1]["text"]
            assert contains in last_text, \
                f"Text '{contains}' not found in last message: {last_text}"
    
    def set_failure(self, should_fail: bool = True, status_code: int = 400, message: str = "Bad Request"):
        """
        Configure the mock to simulate API failures.
        
        Args:
            should_fail: Whether to fail requests
            status_code: HTTP status code to return
            message: Error message
        """
        self.should_fail = should_fail
        self.failure_status_code = status_code
        self.failure_message = message
    
    def clear(self):
        """Clear all sent messages."""
        self.sent_messages.clear()
    
    def reset(self):
        """Reset to default state."""
        self.sent_messages.clear()
        self.should_fail = False
        self.failure_status_code = 400
        self.failure_message = "Bad Request"
    
    def get_call_count(self) -> int:
        """Get number of messages sent."""
        return len(self.sent_messages)


class WhatsAppPayloadBuilder:
    """Builder for creating WhatsApp webhook payloads."""
    
    def __init__(self):
        self.payload = {
            "object": "whatsapp_business_account",
            "entry": []
        }
        self.current_entry = None
    
    def with_entry(self, entry_id: str = "test_entry_123"):
        """Start a new entry."""
        self.current_entry = {
            "id": entry_id,
            "changes": []
        }
        self.payload["entry"].append(self.current_entry)
        return self
    
    def with_message(
        self,
        phone_number_id: str,
        from_phone: str,
        message_id: str,
        text: str,
        user_name: str = "Test User"
    ):
        """Add a text message to the current entry."""
        if not self.current_entry:
            self.with_entry()
        
        change = {
            "value": {
                "messaging_product": "whatsapp",
                "metadata": {
                    "display_phone_number": phone_number_id,
                    "phone_number_id": phone_number_id
                },
                "contacts": [{
                    "profile": {
                        "name": user_name
                    },
                    "wa_id": from_phone
                }],
                "messages": [{
                    "from": from_phone,
                    "id": message_id,
                    "timestamp": "1234567890",
                    "text": {
                        "body": text
                    },
                    "type": "text"
                }]
            },
            "field": "messages"
        }
        
        self.current_entry["changes"].append(change)
        return self
    
    def with_status_update(
        self,
        phone_number_id: str,
        message_id: str,
        status: str = "delivered"
    ):
        """Add a status update to the current entry."""
        if not self.current_entry:
            self.with_entry()
        
        change = {
            "value": {
                "messaging_product": "whatsapp",
                "metadata": {
                    "phone_number_id": phone_number_id
                },
                "statuses": [{
                    "id": message_id,
                    "status": status,
                    "timestamp": "1234567890"
                }]
            },
            "field": "messages"
        }
        
        self.current_entry["changes"].append(change)
        return self
    
    def build(self) -> Dict[str, Any]:
        """Build the webhook payload."""
        return self.payload.copy()

================================================
File: tests/manual_flow_test.sh
================================================
#!/bin/bash

# Manual flow test script
# Tests the WhatsApp webhook flow end-to-end

# Load environment variables
set -a
source ../.env
set +a

# Test webhook payload (from Meta's test message)
PAYLOAD='{
  "object": "whatsapp_business_account",
  "entry": [{
    "id": "0",
    "changes": [{
      "field": "messages",
      "value": {
        "messaging_product": "whatsapp",
        "metadata": {
          "display_phone_number": "16505551111",
          "phone_number_id": "123456123"
        },
        "contacts": [{
          "profile": {"name": "test user name"},
          "wa_id": "16315551181"
        }],
        "messages": [{
          "from": "16315551181",
          "id": "ABGGFlA5Fpa",
          "timestamp": "1504902988",
          "type": "text",
          "text": {"body": "this is a text message"}
        }]
      }
    }]
  }]
}'

echo "Testing webhook endpoint..."
curl -X POST http://localhost:3000/ \
  -H "Content-Type: application/json" \
  -d "$PAYLOAD"

echo -e "\n\nDone. Check Docker logs for results:"
echo "docker logs webhook-ingress"
echo "docker logs windmill_server"


================================================
File: tests/requirements.txt
================================================
# Testing Framework
pytest>=7.4.0
pytest-cov>=4.1.0
pytest-xdist>=3.3.1  # Parallel test execution
pytest-timeout>=2.1.0  # Test timeouts
pytest-mock>=3.11.1  # Enhanced mocking

# Database Testing
psycopg2-binary>=2.9.11

# HTTP Mocking
responses>=0.23.1  # Mock requests library
requests-mock>=1.11.0  # Alternative request mocking

# Code Quality
pytest-flake8>=1.1.1  # Linting
pytest-mypy>=0.10.3  # Type checking
black>=23.7.0  # Code formatting
isort>=5.12.0  # Import sorting

# Coverage Reporting
coverage[toml]>=7.2.0

# Dependencies from main app (if testing imports them)
# Copy from main requirements if needed
openai>=1.0.0
google-generativeai>=0.3.0
requests>=2.31.0

# Optional but useful
pytest-html>=3.2.0  # HTML test reports
pytest-json-report>=1.5.0  # JSON test reports
pytest-benchmark>=4.0.0  # Performance benchmarking

================================================
File: tests/unit/test_rate_limiting.py
================================================
"""
Unit tests for rate limiting functionality.

Tests the Redis-based rate limiter in webhook-server/rateLimiter.js
using mocked Redis to avoid consuming actual API quota.

IMPORTANT: These tests use mocks to simulate 25 messages without consuming
the Gemini API free tier quota.
"""

import pytest
from unittest.mock import Mock, patch, MagicMock, AsyncMock
import sys
from pathlib import Path
import time

# Note: These are Python tests testing Node.js functionality via mocking
# We're testing the logic, not the actual implementation


@pytest.mark.unit
class TestRateLimiting:
    """Test rate limiting logic and scenarios."""

    @pytest.fixture
    def rate_limits(self):
        """Rate limits per plan tier."""
        return {
            "free": 20,
            "pro": 100,
            "enterprise": 500
        }

    @pytest.fixture
    def mock_redis_client(self):
        """Mock Redis client with sliding window functionality."""
        class MockRedisClient:
            def __init__(self):
                self.data = {}  # Sorted sets: {key: [(score, value), ...]}
                self.is_open = True
                self.expirations = {}

            def zRemRangeByScore(self, key, min_score, max_score):
                """Remove entries outside time window."""
                if key in self.data:
                    self.data[key] = [
                        (score, value) for score, value in self.data[key]
                        if not (min_score <= score <= max_score)
                    ]

            def zCard(self, key):
                """Count entries in sorted set."""
                return len(self.data.get(key, []))

            def zAdd(self, key, items):
                """Add entry to sorted set."""
                if key not in self.data:
                    self.data[key] = []

                # items is like [{"score": timestamp, "value": timestamp_str}]
                if isinstance(items, dict):
                    self.data[key].append((items["score"], items["value"]))
                elif isinstance(items, list):
                    for item in items:
                        self.data[key].append((item["score"], item["value"]))

                # Sort by score
                self.data[key].sort(key=lambda x: x[0])

            def expire(self, key, seconds):
                """Set expiration on key."""
                self.expirations[key] = seconds

            def zRange(self, key, start, end, options=None):
                """Get range of entries."""
                if key not in self.data:
                    return []

                entries = self.data[key]
                result = entries[start:end+1] if end >= 0 else entries[start:]

                # Return just values
                return [value for score, value in result]

            def multi(self):
                """Start transaction."""
                return MockRedisMulti(self)

        class MockRedisMulti:
            def __init__(self, client):
                self.client = client
                self.commands = []

            def zRemRangeByScore(self, key, min_score, max_score):
                self.commands.append(("zRemRangeByScore", key, min_score, max_score))
                return self

            def zCard(self, key):
                self.commands.append(("zCard", key))
                return self

            def zAdd(self, key, items):
                self.commands.append(("zAdd", key, items))
                return self

            def expire(self, key, seconds):
                self.commands.append(("expire", key, seconds))
                return self

            async def exec(self):
                """Execute transaction."""
                results = []
                for cmd in self.commands:
                    if cmd[0] == "zRemRangeByScore":
                        self.client.zRemRangeByScore(cmd[1], cmd[2], cmd[3])
                        results.append(None)
                    elif cmd[0] == "zCard":
                        results.append(self.client.zCard(cmd[1]))
                    elif cmd[0] == "zAdd":
                        self.client.zAdd(cmd[1], cmd[2])
                        results.append(None)
                    elif cmd[0] == "expire":
                        self.client.expire(cmd[1], cmd[2])
                        results.append(None)
                return results

        return MockRedisClient()

    def simulate_rate_limit_check(self, redis_client, phone_number_id, plan_tier, rate_limits, current_time=None):
        """
        Simulate the checkRateLimit logic from rateLimiter.js

        This is a Python implementation of the Node.js rate limiting logic
        for testing purposes.
        """
        if current_time is None:
            current_time = int(time.time() * 1000)  # milliseconds

        max_requests = rate_limits.get(plan_tier, rate_limits["free"])
        window_seconds = 3600  # 1 hour
        key = f"ratelimit:{phone_number_id}"

        window_start = current_time - (window_seconds * 1000)

        # Remove old entries
        redis_client.zRemRangeByScore(key, 0, window_start)

        # Count current requests
        current_count = redis_client.zCard(key)

        # Add current request
        redis_client.zAdd(key, {"score": current_time, "value": str(current_time)})

        # Set expiry
        redis_client.expire(key, window_seconds)

        if current_count >= max_requests:
            # Rate limit exceeded
            oldest_request = redis_client.zRange(key, 0, 0)
            reset_in = window_seconds
            if oldest_request:
                reset_in = max(0, int((int(oldest_request[0]) + (window_seconds * 1000) - current_time) / 1000))

            return {
                "allowed": False,
                "current": current_count,
                "max": max_requests,
                "resetIn": reset_in
            }

        return {
            "allowed": True,
            "current": current_count + 1,
            "max": max_requests,
            "resetIn": window_seconds
        }

    def test_free_tier_allows_20_messages(self, mock_redis_client, rate_limits):
        """Test that free tier allows exactly 20 messages per hour."""
        phone_number_id = "test_phone_free_tier"
        plan_tier = "free"
        current_time = int(time.time() * 1000)

        # Send 20 messages (should all be allowed)
        for i in range(20):
            result = self.simulate_rate_limit_check(
                mock_redis_client,
                phone_number_id,
                plan_tier,
                rate_limits,
                current_time + (i * 1000)  # 1 second apart
            )
            assert result["allowed"] is True, f"Message {i+1} should be allowed"
            assert result["current"] == i + 1

        # 21st message should be blocked
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            phone_number_id,
            plan_tier,
            rate_limits,
            current_time + (20 * 1000)
        )
        assert result["allowed"] is False, "21st message should be blocked"
        assert result["current"] == 20
        assert result["max"] == 20

    def test_simulate_25_messages_free_tier(self, mock_redis_client, rate_limits):
        """
        CRITICAL TEST: Simulate sending 25 messages to free tier chatbot.
        User specifically requested this to avoid consuming Gemini API quota.

        Expected: First 20 allowed, next 5 blocked.
        """
        phone_number_id = "test_phone_spam_attempt"
        plan_tier = "free"
        current_time = int(time.time() * 1000)

        allowed_count = 0
        blocked_count = 0

        # Simulate 25 messages
        for i in range(25):
            result = self.simulate_rate_limit_check(
                mock_redis_client,
                phone_number_id,
                plan_tier,
                rate_limits,
                current_time + (i * 100)  # 100ms apart (rapid fire)
            )

            if result["allowed"]:
                allowed_count += 1
            else:
                blocked_count += 1

        # Assert: Should allow exactly 20, block 5
        assert allowed_count == 20, f"Should allow exactly 20 messages, got {allowed_count}"
        assert blocked_count == 5, f"Should block exactly 5 messages, got {blocked_count}"

    def test_pro_tier_allows_100_messages(self, mock_redis_client, rate_limits):
        """Test that pro tier allows 100 messages per hour."""
        phone_number_id = "test_phone_pro_tier"
        plan_tier = "pro"
        current_time = int(time.time() * 1000)

        # Send 100 messages (should all be allowed)
        for i in range(100):
            result = self.simulate_rate_limit_check(
                mock_redis_client,
                phone_number_id,
                plan_tier,
                rate_limits,
                current_time + (i * 100)
            )
            assert result["allowed"] is True, f"Message {i+1} should be allowed"

        # 101st message should be blocked
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            phone_number_id,
            plan_tier,
            rate_limits,
            current_time + (100 * 100)
        )
        assert result["allowed"] is False
        assert result["current"] == 100
        assert result["max"] == 100

    def test_enterprise_tier_allows_500_messages(self, mock_redis_client, rate_limits):
        """Test that enterprise tier allows 500 messages per hour."""
        phone_number_id = "test_phone_enterprise"
        plan_tier = "enterprise"
        current_time = int(time.time() * 1000)

        # Send 500 messages (should all be allowed)
        for i in range(500):
            result = self.simulate_rate_limit_check(
                mock_redis_client,
                phone_number_id,
                plan_tier,
                rate_limits,
                current_time + (i * 10)
            )
            assert result["allowed"] is True, f"Message {i+1} should be allowed"

        # 501st message should be blocked
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            phone_number_id,
            plan_tier,
            rate_limits,
            current_time + (500 * 10)
        )
        assert result["allowed"] is False

    def test_sliding_window_resets_after_hour(self, mock_redis_client, rate_limits):
        """Test that rate limit sliding window resets after 1 hour."""
        phone_number_id = "test_phone_sliding_window"
        plan_tier = "free"
        current_time = int(time.time() * 1000)

        # Send 20 messages at T=0
        for i in range(20):
            result = self.simulate_rate_limit_check(
                mock_redis_client,
                phone_number_id,
                plan_tier,
                rate_limits,
                current_time + (i * 1000)
            )
            assert result["allowed"] is True

        # 21st message at T=0 should be blocked
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            phone_number_id,
            plan_tier,
            rate_limits,
            current_time + (20 * 1000)
        )
        assert result["allowed"] is False

        # After 1 hour + 1 second, should allow new messages
        one_hour_later = current_time + (3601 * 1000)
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            phone_number_id,
            plan_tier,
            rate_limits,
            one_hour_later
        )
        assert result["allowed"] is True, "Should allow message after 1 hour window"

    def test_different_chatbots_independent_limits(self, mock_redis_client, rate_limits):
        """Test that different chatbots have independent rate limits."""
        plan_tier = "free"
        current_time = int(time.time() * 1000)

        # Chatbot 1: Send 20 messages
        for i in range(20):
            result = self.simulate_rate_limit_check(
                mock_redis_client,
                "chatbot_1",
                plan_tier,
                rate_limits,
                current_time + (i * 100)
            )
            assert result["allowed"] is True

        # Chatbot 1: 21st blocked
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            "chatbot_1",
            plan_tier,
            rate_limits,
            current_time + (20 * 100)
        )
        assert result["allowed"] is False

        # Chatbot 2: Should still have full quota
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            "chatbot_2",
            plan_tier,
            rate_limits,
            current_time
        )
        assert result["allowed"] is True
        assert result["current"] == 1  # First message

    def test_reset_time_calculation(self, mock_redis_client, rate_limits):
        """Test that resetIn is correctly calculated."""
        phone_number_id = "test_phone_reset_time"
        plan_tier = "free"
        current_time = int(time.time() * 1000)

        # Fill up the quota
        for i in range(20):
            self.simulate_rate_limit_check(
                mock_redis_client,
                phone_number_id,
                plan_tier,
                rate_limits,
                current_time + (i * 1000)
            )

        # Next request should be blocked with resetIn
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            phone_number_id,
            plan_tier,
            rate_limits,
            current_time + (20 * 1000)
        )

        assert result["allowed"] is False
        assert "resetIn" in result
        assert result["resetIn"] > 0
        # Should be approximately 1 hour minus 20 seconds
        assert 3580 <= result["resetIn"] <= 3600

    def test_burst_traffic_handling(self, mock_redis_client, rate_limits):
        """Test handling of burst traffic (many messages in short time)."""
        phone_number_id = "test_phone_burst"
        plan_tier = "free"
        current_time = int(time.time() * 1000)

        # Send all 20 messages within 1 second (burst)
        for i in range(20):
            result = self.simulate_rate_limit_check(
                mock_redis_client,
                phone_number_id,
                plan_tier,
                rate_limits,
                current_time + i  # 1ms apart
            )
            assert result["allowed"] is True

        # Should still block 21st message
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            phone_number_id,
            plan_tier,
            rate_limits,
            current_time + 20
        )
        assert result["allowed"] is False

    def test_gradual_quota_recovery(self, mock_redis_client, rate_limits):
        """Test that quota gradually recovers in sliding window."""
        phone_number_id = "test_phone_gradual"
        plan_tier = "free"
        base_time = int(time.time() * 1000)

        # Send 20 messages at base_time
        for i in range(20):
            self.simulate_rate_limit_check(
                mock_redis_client,
                phone_number_id,
                plan_tier,
                rate_limits,
                base_time + (i * 60000)  # 1 minute apart
            )

        # At base_time + 61 minutes, first message should have expired
        # So we should be able to send 1 more message
        time_61_min_later = base_time + (61 * 60 * 1000)
        result = self.simulate_rate_limit_check(
            mock_redis_client,
            phone_number_id,
            plan_tier,
            rate_limits,
            time_61_min_later
        )
        assert result["allowed"] is True, "Should allow message after oldest entry expires"

    def test_fail_open_behavior_when_redis_unavailable(self, rate_limits):
        """Test that system allows requests when Redis is unavailable (fail-open)."""
        # This would be tested in the actual Node.js code
        # Here we just document the expected behavior

        # When Redis client is None or not connected:
        # - Should return {"allowed": True}
        # - Should not throw error
        # - Better to allow traffic than block legitimate users

        # This is a design decision test
        assert True  # Placeholder for documentation

    def test_current_count_accuracy(self, mock_redis_client, rate_limits):
        """Test that current count is accurately tracked."""
        phone_number_id = "test_phone_count"
        plan_tier = "free"
        current_time = int(time.time() * 1000)

        for i in range(15):
            result = self.simulate_rate_limit_check(
                mock_redis_client,
                phone_number_id,
                plan_tier,
                rate_limits,
                current_time + (i * 1000)
            )
            # Current count should match iteration (1-indexed)
            assert result["current"] == i + 1
            assert result["max"] == 20


================================================
File: utils/curl_windmill_endpoint.sh
================================================
#!/usr/bin/env bash
set -euo pipefail


# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Helper functions for colored output
error() {
  echo -e "${RED}✗ Error: $1${NC}" >&2
}

success() {
  echo -e "${GREEN}✓ $1${NC}"
}

info() {
  echo -e "${YELLOW}ℹ $1${NC}"
}

# 1. Load environment variables from the .env file in parent directory
load_env() {
  # Get the directory where this script is located
  local script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
  # .env should be in the parent of the script's directory (src/.env)
  local env_file="${script_dir}/../.env"
  
  if [ ! -f "$env_file" ]; then
    error ".env file not found at: $env_file"
    echo "  Expected location: $(dirname "$script_dir")/.env"
    exit 1
  fi
  
  # Use a more robust method: set -a enables automatic export
  # Filter out comments and empty lines, then source
  set -a
  # Create a temporary file with cleaned .env content
  local temp_env=$(mktemp)
  grep -v '^[[:space:]]*#' "$env_file" | grep -v '^[[:space:]]*$' > "$temp_env"
  
  # Source the cleaned .env file
  # This properly handles quoted values, spaces, and special characters
  . "$temp_env" 2>/dev/null || {
    # Fallback: read line by line if sourcing fails
    while IFS= read -r line || [ -n "$line" ]; do
      [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue
      # Remove inline comments (simple approach)
      line=$(echo "$line" | sed 's/[[:space:]]*#.*$//')
      [[ -z "$line" ]] && continue
      # Export the variable
      export "$line" 2>/dev/null || true
    done < "$env_file"
  }
  set +a
  rm -f "$temp_env"
  
  success ".env file loaded"
}

load_env

WINDMILL_URL="http://localhost:8081/api/w/development/jobs/run_wait_result/f/f/development/whatsapp_webhook_processor"
TOKEN="$WINDMILL_TOKEN"
WHATSAPP_PHONE_NUMBER_ID="$WHATSAPP_PHONE_NUMBER_ID"
WHATSAPP_USER_PHONE="5216441921909"
WHATSAPP_USER_NAME="JD"
WHATSAPP_MESSAGE_BODY="Hello, how are you?"
WHATSAPP_MESSAGE_ID="wamid.test.123"


info "curl command sent: \n
    curl -X POST \"$WINDMILL_URL\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" -d '{
        \"phone_number_id\": \"$WHATSAPP_PHONE_NUMBER_ID\",
        \"user_phone\": \"$WHATSAPP_USER_PHONE\",
        \"user_name\": \"$WHATSAPP_USER_NAME\",
        \"message_body\": \"$WHATSAPP_MESSAGE_BODY\",
        \"message_id\": \"$WHATSAPP_MESSAGE_ID\"
      }'"

curl -X POST "$WINDMILL_URL" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
        "phone_number_id": "911040768760384",
        "user_phone": "5216441921909",
        "user_name": "JD",
        "message_body": "Hello, how are you?",
        "message_id": "wamid.test.123"
      }'


================================================
File: webhook-server/app.js
================================================
// app.js (Simplified)
import express from 'express';
import axios from 'axios';
import crypto from 'crypto';
import pg from 'pg';
import { initializeRedis, checkRateLimit, closeRedis } from './rateLimiter.js';

const { Pool } = pg;

const app = express();
app.use(express.json());

// Database connection pool for business logic database
const pool = new Pool({
  host: process.env.DB_HOST || 'business_logic_db',
  port: process.env.DB_PORT || 5432,
  user: process.env.DB_USER || 'business_logic_user',
  password: process.env.DB_PASSWORD || 'business_logic_password',
  database: process.env.DB_NAME || 'business_logic_app',
  max: 10,
  idleTimeoutMillis: 30000,
});

// Message size limits per plan tier (in characters)
const MESSAGE_LIMITS = {
  free: 2000,
  pro: 5000,
  enterprise: 10000
};

// Initialize Redis for rate limiting
initializeRedis().catch(err => {
  console.error('Failed to initialize Redis:', err);
  console.warn('Rate limiting will be disabled (fail-open mode)');
});

app.use((req, res, next) => {
  // Log EVERYTHING
  console.log(`[${new Date().toISOString()}] ${req.method} ${req.url}`);
  if (req.method === 'POST') {
    console.log('Payload:', JSON.stringify(req.body, null, 2));
  }
  next();
});


// 1. Meta Verification (GET) - Standard Meta Requirement
app.get('/', (req, res) => {
  if (
    req.query['hub.mode'] === 'subscribe' &&
    req.query['hub.verify_token'] === process.env.WEBHOOK_VERIFY_TOKEN
  ) {
    console.log('Webhook verified');
    res.send(req.query['hub.challenge']);
  } else {
    console.log('Webhook verification failed');
    res.sendStatus(400);
  }
});

// 2. Message Handling (POST)
app.post('/', async (req, res) => {
  console.log('Webhook received');
  // A. Immediate 200 OK to Meta
  res.sendStatus(200);

  const body = req.body;

  // B. Basic Filtering: Only process actual user messages
  // (Ignore status updates like "sent", "delivered", "read")
  if (body.object === 'whatsapp_business_account') {
    body.entry?.forEach((entry) => {
      entry.changes?.forEach((change) => {
        const value = change.value;

        // Check if it's a message (not a status update)
        if (value.messages && value.messages[0]) {
          console.log('Processing message');
          // C. Trigger Windmill Asynchronously
          // Fire and forget - don't await this
          triggerWindmillFlow(value);
          console.log('Triggered Windmill flow');
        }
      });
    });
  }
});

async function triggerWindmillFlow(payload) {
  try {
    const message = payload.messages[0];
    const contact = payload.contacts[0];
    const phoneNumberId = payload.metadata.phone_number_id;
    const messageBody = message.text?.body || '';

    // Phase 7.1: Message Size Validation
    // Get chatbot's organization plan tier to enforce message limits
    let planTier = 'free'; // Default to most restrictive
    let maxMessageLength = MESSAGE_LIMITS.free;

    try {
      const result = await pool.query(`
        SELECT o.plan_tier
        FROM chatbots c
        JOIN organizations o ON c.organization_id = o.id
        WHERE c.phone_number_id = $1
        LIMIT 1
      `, [phoneNumberId]);

      if (result.rows.length > 0) {
        planTier = result.rows[0].plan_tier;
        maxMessageLength = MESSAGE_LIMITS[planTier] || MESSAGE_LIMITS.free;
      }
    } catch (dbError) {
      console.error('Failed to fetch plan tier, using default (free):', dbError.message);
    }

    // Validate message size
    if (messageBody.length > maxMessageLength) {
      console.warn(`Message too long: ${messageBody.length} chars (max: ${maxMessageLength} for ${planTier} plan)`);
      console.log(`Rejecting message from ${message.from} - exceeds ${planTier} plan limit`);

      // Don't trigger Windmill flow - message rejected for being too long
      // In production, you might want to send a WhatsApp reply explaining the limit
      return;
    }

    console.log(`Message validated: ${messageBody.length}/${maxMessageLength} chars (${planTier} plan)`);

    // Phase 7.2: Rate Limiting
    // Check if chatbot has exceeded rate limit for their plan tier
    const rateLimit = await checkRateLimit(phoneNumberId, planTier);

    if (!rateLimit.allowed) {
      console.warn(`Rate limit exceeded for ${phoneNumberId}: ${rateLimit.current}/${rateLimit.max} messages/hour (${planTier} plan)`);
      console.log(`Rejecting message from ${message.from} - rate limit exceeded, resets in ${rateLimit.resetIn}s`);

      // Don't trigger Windmill flow - rate limit exceeded
      // In production, you might want to send a WhatsApp reply explaining the limit
      return;
    }

    console.log(`Rate limit OK: ${rateLimit.current}/${rateLimit.max} messages/hour (${planTier} plan)`);

    // Use the environment variable for the URL to keep it flexible
    const WINDMILL_URL = process.env.WINDMILL_MESSAGE_PROCESSING_ENDPOINT || 'http://windmill_server:8000';
    console.log('Triggering Windmill flow with payload:', payload);
    console.log('WINDMILL_URL:', WINDMILL_URL);
    await axios.post(
      `${WINDMILL_URL}`,
      {
        phone_number_id: phoneNumberId,
        user_phone: message.from,
        user_name: contact.profile.name,
        message_body: messageBody,
        message_id: message.id
      },
      {
        headers: {
          // If you are using the default 'admin' user in local Windmill,
          // you might need an API token generated from the UI.
          Authorization: `Bearer ${process.env.WINDMILL_TOKEN}`
        }
      }
    );
    console.log('Windmill flow triggered successfully');
  } catch (error) {
    console.error("Failed to trigger Windmill:", error.message);
    console.error("Error details:", error.response?.data);
  }
}

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Webhook server is listening on port ${PORT}`);
});

// Graceful shutdown
process.on('SIGTERM', async () => {
  console.log('SIGTERM received, shutting down gracefully...');
  await closeRedis();
  process.exit(0);
});

process.on('SIGINT', async () => {
  console.log('SIGINT received, shutting down gracefully...');
  await closeRedis();
  process.exit(0);
});

================================================
File: docs/RAG_IMPLEMENTATION_GUIDE.md
================================================
# RAG Implementation Guide: pgvector Edition

## Executive Summary

**Recommendation: Use pgvector for long-term success**

### Why pgvector > Pinecone for Your SaaS

| Factor | pgvector | Pinecone |
|--------|----------|----------|
| **Cost at Scale** | ✅ $0 marginal cost | ❌ $70-200/month per index |
| **Multi-tenancy** | ✅ Natural with SQL | ⚠️ Complex namespace management |
| **Data Locality** | ✅ Everything in one DB | ❌ External service |
| **Setup Complexity** | ✅ Simple extension | ⚠️ Additional service |
| **Performance (< 1M vectors)** | ✅ < 100ms | ✅ < 50ms |
| **Vendor Lock-in** | ✅ Open source | ❌ Proprietary |
| **Maintenance** | ✅ Same as your DB | ⚠️ Another service to monitor |

### When to Use Each

**pgvector (Recommended):**
- ✅ Multi-tenant SaaS (your use case)
- ✅ Cost-sensitive business model
- ✅ < 1M vectors per tenant
- ✅ Want single database to manage

**Pinecone (Consider If):**
- ⚠️ > 5M vectors per tenant
- ⚠️ Need sub-10ms latency
- ⚠️ Don't want to manage infrastructure
- ⚠️ Budget for $500+/month in vector DB costs

## Architecture Overview

```
Document Upload Flow:
User uploads PDF/URL
    ↓
Windmill Job: Process Document
    ├─ Extract text (PyPDF2/trafilatura)
    ├─ Chunk text (1000 chars, 200 overlap)
    ├─ Generate embeddings (OpenAI ada-002)
    └─ Store in pgvector (document_chunks table)

Query Flow:
User sends message
    ↓
Step 1: Context Loading
    ↓
Step 2: LLM Processing
    ├─ Generate query embedding
    ├─ Search pgvector (top 5 similar chunks)
    ├─ Build prompt with context
    └─ Call LLM with enriched prompt
    ↓
Step 3: Send response
```

## Installation Steps

### 1. Install pgvector Extension

```bash
# In your PostgreSQL container
docker exec -it business_logic_db bash

# Inside container
apt-get update
apt-get install -y postgresql-16-pgvector

# Exit container
exit

# Or rebuild with pgvector in Dockerfile
```

**Alternative: Use postgres image with pgvector**
```yaml
# In docker-compose.yml
business_logic_db:
  image: pgvector/pgvector:pg16  # Instead of postgres:16
  # ... rest of config
```

### 2. Update Database Schema

```bash
cd db

# Add pgvector schema additions to create.sql
# (Already provided in artifacts above)

# Reset database
./manage_db.sh reset

# Verify pgvector is installed
docker exec business_logic_db psql -U business_logic_user -d business_logic_app -c "SELECT * FROM pg_extension WHERE extname = 'vector';"
```

### 3. Install Python Dependencies

```bash
# Add to your requirements.txt or install directly
pip install pgvector psycopg2-binary

# For document processing
pip install PyPDF2        # PDF extraction
pip install trafilatura   # Web scraping
pip install python-docx   # Word docs
pip install openai        # Embeddings
```

### 4. Add Document Processing Script to Windmill

Create new script `f/development/4_process_documents.py` (already provided above)

### 5. Update Step 2 with RAG

Replace your existing `2_whatsapp_llm_processing.py` with the RAG-enabled version (provided above)

## Document Processing Workflow

### Upload API Endpoint (to be created)

```python
# f/development/upload_document.py
def main(
    chatbot_id: str,
    file_upload: str,  # Base64 encoded file or URL
    source_type: str,  # "pdf", "url", "text"
    name: str,
    db_resource: str = "f/development/business_layer_db_postgreSQL"
):
    """
    Handle document upload.
    
    Steps:
    1. Validate user owns this chatbot
    2. Save file to storage
    3. Create knowledge_source record
    4. Trigger async processing job
    """
    # ... validation logic
    
    # Create knowledge source
    source_id = create_knowledge_source(chatbot_id, name, source_type, file_path)
    
    # Trigger async processing
    trigger_document_processing(source_id, chatbot_id)
    
    return {"success": True, "source_id": source_id}
```

### Processing Job

Triggered automatically after upload:

```python
# This runs in background
result = process_document(
    knowledge_source_id=source_id,
    chatbot_id=chatbot_id,
    chunk_size=1000,
    chunk_overlap=200
)

# Updates knowledge_sources.sync_status to:
# - "processing" (in progress)
# - "synced" (success)
# - "failed" (error)
```

## Configuration Options

### Chunking Strategy

```python
# Conservative (better context, more chunks)
chunk_size = 500
chunk_overlap = 100

# Balanced (recommended)
chunk_size = 1000
chunk_overlap = 200

# Aggressive (fewer chunks, may lose context)
chunk_size = 2000
chunk_overlap = 300
```

### Embedding Models

```python
# OpenAI (recommended for quality)
model = "text-embedding-ada-002"
dimensions = 1536
cost_per_1k_tokens = $0.0001

# OpenAI (cheaper, slightly lower quality)
model = "text-embedding-3-small"
dimensions = 1536
cost_per_1k_tokens = $0.00002

# OpenAI (best quality, pricier)
model = "text-embedding-3-large"
dimensions = 3072
cost_per_1k_tokens = $0.00013
```

**Important**: If you change embedding dimensions, update:
```sql
-- In create.sql
CREATE TABLE document_chunks (
    embedding vector(3072),  -- Update this
    ...
);

-- And the search function
CREATE OR REPLACE FUNCTION search_knowledge_base(
    p_query_embedding vector(3072),  -- Update this
    ...
)
```

### Retrieval Settings

```python
# In Step 2 (LLM Processing)

# Conservative (most relevant only)
top_k = 3
similarity_threshold = 0.8

# Balanced (recommended)
top_k = 5
similarity_threshold = 0.7

# Aggressive (more context, may include noise)
top_k = 10
similarity_threshold = 0.6
```

## Performance Optimization

### Index Types

**HNSW (Recommended):**
```sql
CREATE INDEX idx_document_chunks_embedding_hnsw 
ON document_chunks 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```
- **Pros**: Fast queries (10-50ms), good recall
- **Cons**: Higher memory usage, slower inserts
- **Use when**: < 10M vectors

**IVFFlat (Alternative):**
```sql
CREATE INDEX idx_document_chunks_embedding_ivfflat 
ON document_chunks 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```
- **Pros**: Lower memory, faster inserts
- **Cons**: Slower queries, needs VACUUM ANALYZE
- **Use when**: Memory constrained

### Query Optimization

```sql
-- Add partial index for active chatbots only
CREATE INDEX idx_active_chunks 
ON document_chunks(chatbot_id, embedding)
WHERE embedding IS NOT NULL;

-- Create materialized view for frequently accessed stats
CREATE MATERIALIZED VIEW chatbot_knowledge_stats AS
SELECT 
    chatbot_id,
    COUNT(*) as total_chunks,
    COUNT(DISTINCT knowledge_source_id) as total_sources,
    MAX(created_at) as last_updated
FROM document_chunks
GROUP BY chatbot_id;

-- Refresh periodically
REFRESH MATERIALIZED VIEW chatbot_knowledge_stats;
```

### Monitoring Queries

```sql
-- Check index usage
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    idx_tup_read as tuples_read
FROM pg_stat_user_indexes
WHERE tablename = 'document_chunks';

-- Check slow queries
SELECT 
    query,
    calls,
    mean_exec_time,
    max_exec_time
FROM pg_stat_statements
WHERE query LIKE '%document_chunks%'
ORDER BY mean_exec_time DESC
LIMIT 10;

-- Check table size
SELECT 
    pg_size_pretty(pg_total_relation_size('document_chunks')) as total_size,
    pg_size_pretty(pg_relation_size('document_chunks')) as table_size,
    pg_size_pretty(pg_indexes_size('document_chunks')) as indexes_size;
```

## Cost Analysis

### pgvector Costs

**Storage:**
- Vector: 1536 dimensions × 4 bytes = 6KB per vector
- Text: ~1KB average per chunk
- Total: ~7KB per chunk

**Example: 1000 tenants, 100 documents each**
- Documents: 100,000 total
- Chunks: 100,000 × 10 = 1,000,000 chunks
- Storage: 1M × 7KB = 7GB
- **Cost: ~$0-10/month** (part of your existing PostgreSQL)

**Processing:**
- Embeddings: 1M chunks × $0.0001/1K tokens = $100 one-time
- Ongoing: Only new documents

### Pinecone Costs (Comparison)

**For same workload:**
- 1M vectors in Pinecone
- Need: p1.x1 pod (~$70/month) or p2.x1 pod (~$200/month)
- Plus: API usage fees
- **Cost: $70-200/month recurring**

### Break-even Analysis

```
Year 1:
pgvector: $100 (embeddings) + $120 (storage) = $220
Pinecone: $840-2400

Savings: $620-2180 in first year
```

## Testing Your RAG Implementation

### Unit Test: Document Processing

```python
def test_document_processing(db_with_data, mock_wmill):
    """Test document chunking and embedding."""
    
    # Create test knowledge source
    source_id = create_test_source(db_with_data)
    
    # Process it
    result = process_document(
        knowledge_source_id=source_id,
        chatbot_id="test-bot-id",
        openai_api_key="test-key"
    )
    
    # Verify chunks created
    assert result["success"] is True
    assert result["chunks_created"] > 0
    
    # Check database
    chunks = get_chunks_for_source(db_with_data, source_id)
    assert len(chunks) == result["chunks_created"]
    assert chunks[0]["embedding"] is not None
```

### Integration Test: RAG Retrieval

```python
def test_rag_retrieval(db_with_data, mock_llm):
    """Test end-to-end RAG flow."""
    
    # Setup: Create document with known content
    create_test_document(
        db_with_data,
        content="The capital of France is Paris."
    )
    
    # Query
    result = step2.main(
        context_payload={...},
        user_message="What is the capital of France?"
    )
    
    # Verify RAG was used
    assert result["usage_info"]["rag_used"] is True
    assert result["usage_info"]["chunks_retrieved"] > 0
    assert "Paris" in result["reply_text"]
```

### Manual Testing

```bash
# 1. Upload a test document
curl -X POST http://localhost:8081/api/w/development/jobs/run/f/development/upload_document \
  -H "Authorization: Bearer $WINDMILL_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "chatbot_id": "your-bot-id",
    "source_type": "text",
    "name": "Test Document",
    "content": "This is test content about machine learning."
  }'

# 2. Wait for processing (check logs)
docker-compose logs -f windmill_worker

# 3. Query the chatbot
curl -X POST http://localhost:3000/ \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{
      "from": "test-user",
      "text": {"body": "Tell me about machine learning"}
    }]
  }'

# 4. Check if RAG was used (check Step 2 logs)
```

## Common Issues & Solutions

### Issue: Embeddings Not Generating

**Symptoms:**
- `document_chunks.embedding` is NULL
- Search returns no results

**Check:**
```sql
SELECT COUNT(*) as total, 
       COUNT(embedding) as with_embeddings 
FROM document_chunks 
WHERE chatbot_id = 'your-bot-id';
```

**Solutions:**
1. Check OpenAI API key is set
2. Check rate limits (OpenAI: 3,000 RPM for tier 1)
3. Check error logs: `SELECT error_message FROM knowledge_sources WHERE sync_status = 'failed'`

### Issue: Slow Queries

**Symptoms:**
- RAG retrieval takes > 500ms
- Database CPU high

**Check:**
```sql
EXPLAIN ANALYZE
SELECT * FROM search_knowledge_base(
    'chatbot-id',
    '[0.1, 0.2, ...]',  -- Sample embedding
    5,
    0.7
);
```

**Solutions:**
1. Ensure HNSW index exists: `\d document_chunks`
2. Run VACUUM ANALYZE: `VACUUM ANALYZE document_chunks;`
3. Increase shared_buffers in postgresql.conf
4. Consider reducing `top_k` or increasing `similarity_threshold`

### Issue: Low Quality Results

**Symptoms:**
- Retrieved chunks not relevant
- LLM response doesn't use context

**Check:**
```sql
-- See what's actually being retrieved
SELECT 
    content,
    1 - (embedding <=> query_embedding) as similarity
FROM document_chunks
WHERE chatbot_id = 'your-bot-id'
ORDER BY embedding <=> query_embedding
LIMIT 5;
```

**Solutions:**
1. Lower `similarity_threshold` (try 0.6)
2. Increase `top_k` (try 7-10)
3. Improve chunking (smaller chunks = more precise)
4. Add more context in system prompt about how to use retrieved information

### Issue: Out of Memory

**Symptoms:**
- PostgreSQL OOM kills
- Slow index builds

**Check:**
```sql
SELECT 
    pg_size_pretty(pg_total_relation_size('document_chunks')),
    COUNT(*) 
FROM document_chunks;
```

**Solutions:**
1. Switch from HNSW to IVFFlat (less memory)
2. Increase PostgreSQL memory settings
3. Archive old/unused documents
4. Use read replica for search queries


**Questions?** Check the test files in `tests/unit/` for examples of how to test RAG functionality.

================================================
File: api-server/routes/knowledge.js
================================================
/**
 * Knowledge Base API Routes
 *
 * Provides endpoints for managing chatbot knowledge bases:
 * - File uploads, URL ingestion, web crawling
 * - Document management and status tracking
 * - RAG search testing
 */

const express = require('express');
const multer = require('multer');
const { Pool } = require('pg');
const fetch = require('node-fetch');
const { checkQuota } = require('../middleware/quota');

const router = express.Router();

// Database connection pool
const pool = new Pool({
  host: process.env.DB_HOST || 'localhost',
  port: process.env.DB_PORT || 5432,
  user: process.env.DB_USER || 'windmill_user',
  password: process.env.DB_PASSWORD || 'changeme',
  database: process.env.DB_NAME || 'windmill',
  max: 20,
  idleTimeoutMillis: 30000,
});

// Windmill configuration
const WINDMILL_URL = process.env.WINDMILL_URL || 'http://localhost:8000';
const WINDMILL_TOKEN = process.env.WINDMILL_TOKEN;
const WINDMILL_WORKSPACE = process.env.WINDMILL_WORKSPACE || 'development';

// File upload configuration (10MB limit)
const upload = multer({
  storage: multer.memoryStorage(),
  limits: {
    fileSize: 10 * 1024 * 1024, // 10MB
  },
  fileFilter: (req, file, cb) => {
    const allowedMimeTypes = [
      'application/pdf',
      'application/vnd.openxmlformats-officedocument.wordprocessingml.document', // .docx
      'application/msword' // .doc
    ];

    if (allowedMimeTypes.includes(file.mimetype)) {
      cb(null, true);
    } else {
      cb(new Error('Invalid file type. Only PDF and DOCX files are allowed.'));
    }
  }
});

/**
 * Helper: Call Windmill script
 */
async function callWindmillScript(scriptPath, args) {
  const response = await fetch(
    `${WINDMILL_URL}/api/w/${WINDMILL_WORKSPACE}/jobs/run/p/${scriptPath}`,
    {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${WINDMILL_TOKEN}`
      },
      body: JSON.stringify(args)
    }
  );

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Windmill API error: ${error}`);
  }

  return await response.json();
}

/**
 * 1. POST /api/chatbots/:id/knowledge/upload
 * Upload PDF or DOCX file
 */
router.post('/:id/knowledge/upload', upload.single('file'), async (req, res, next) => {
  try {
    const { id: chatbotId } = req.params;
    const file = req.file;

    if (!file) {
      return res.status(400).json({ error: 'No file uploaded' });
    }

    const fileSizeMb = file.size / (1024 * 1024);

    // Check quota
    req.body.sourceType = file.mimetype.includes('pdf') ? 'pdf' : 'doc';
    req.body.fileSizeMb = fileSizeMb;
    req.params.chatbotId = chatbotId;

    await checkQuota(req, res, async () => {
      // Call Windmill upload script
      const result = await callWindmillScript('f/development/upload_document', {
        chatbot_id: chatbotId,
        file_content: file.buffer.toString('base64'),
        filename: file.originalname,
        content_type: file.mimetype
      });

      res.json({
        success: true,
        knowledge_source_id: result.knowledge_source_id,
        job_id: result.job_id,
        status: result.status,
        quota: req.quotaInfo
      });
    });

  } catch (error) {
    next(error);
  }
});

/**
 * 2. POST /api/chatbots/:id/knowledge/url
 * Add single URL to knowledge base
 */
router.post('/:id/knowledge/url', async (req, res, next) => {
  try {
    const { id: chatbotId } = req.params;
    const { url } = req.body;

    if (!url) {
      return res.status(400).json({ error: 'Missing URL' });
    }

    // Check quota
    req.body.sourceType = 'url';
    req.params.chatbotId = chatbotId;

    await checkQuota(req, res, async () => {
      // Call batch ingestion with single URL
      const result = await callWindmillScript('f/development/ingest_multiple_urls', {
        chatbot_id: chatbotId,
        urls: [url]
      });

      const urlResult = result.results[0];

      if (!urlResult.success) {
        return res.status(400).json({
          success: false,
          error: urlResult.error
        });
      }

      res.json({
        success: true,
        knowledge_source_id: urlResult.knowledge_source_id,
        job_id: urlResult.job_id,
        quota: req.quotaInfo
      });
    });

  } catch (error) {
    next(error);
  }
});

/**
 * 3. POST /api/chatbots/:id/knowledge/crawl
 * Discover links from base URL
 */
router.post('/:id/knowledge/crawl', async (req, res, next) => {
  try {
    const { id: chatbotId } = req.params;
    const { baseUrl, maxDepth = 2, maxPages = 50, filterKeywords } = req.body;

    if (!baseUrl) {
      return res.status(400).json({ error: 'Missing base URL' });
    }

    // Call web crawler
    const result = await callWindmillScript('f/development/utils/web_crawler', {
      base_url: baseUrl,
      max_depth: maxDepth,
      max_pages: maxPages,
      ...(filterKeywords && { filter_keywords: filterKeywords })
    });

    res.json({
      success: true,
      ...result
    });

  } catch (error) {
    next(error);
  }
});

/**
 * 4. POST /api/chatbots/:id/knowledge/ingest-batch
 * Ingest multiple URLs in batch
 */
router.post('/:id/knowledge/ingest-batch', async (req, res, next) => {
  try {
    const { id: chatbotId } = req.params;
    const { urls } = req.body;

    if (!urls || !Array.isArray(urls) || urls.length === 0) {
      return res.status(400).json({ error: 'Missing or invalid URLs array' });
    }

    // Call batch ingestion
    const result = await callWindmillScript('f/development/ingest_multiple_urls', {
      chatbot_id: chatbotId,
      urls: urls
    });

    res.json({
      success: true,
      ...result
    });

  } catch (error) {
    next(error);
  }
});

/**
 * 5. GET /api/chatbots/:id/knowledge/sources
 * List all knowledge sources (paginated)
 */
router.get('/:id/knowledge/sources', async (req, res, next) => {
  try {
    const { id: chatbotId } = req.params;
    const { page = 1, limit = 20, status, sourceType } = req.query;

    const offset = (page - 1) * limit;

    // Build query
    let query = `
      SELECT
        id,
        source_type,
        name,
        file_path,
        sync_status as status,
        error_message,
        file_size_bytes,
        chunks_count,
        created_at,
        updated_at
      FROM knowledge_sources
      WHERE chatbot_id = $1
    `;

    const params = [chatbotId];
    let paramIndex = 2;

    if (status) {
      query += ` AND status = $${paramIndex}`;
      params.push(status);
      paramIndex++;
    }

    if (sourceType) {
      query += ` AND source_type = $${paramIndex}`;
      params.push(sourceType);
      paramIndex++;
    }

    query += ` ORDER BY created_at DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
    params.push(limit, offset);

    const result = await pool.query(query, params);

    // Get total count
    const countQuery = `
      SELECT COUNT(*) as total
      FROM knowledge_sources
      WHERE chatbot_id = $1
      ${status ? `AND status = '${status}'` : ''}
      ${sourceType ? `AND source_type = '${sourceType}'` : ''}
    `;
    const countResult = await pool.query(countQuery, [chatbotId]);
    const total = parseInt(countResult.rows[0].total);

    res.json({
      success: true,
      sources: result.rows,
      pagination: {
        page: parseInt(page),
        limit: parseInt(limit),
        total,
        totalPages: Math.ceil(total / limit)
      }
    });

  } catch (error) {
    next(error);
  }
});

/**
 * 6. GET /api/chatbots/:id/knowledge/sources/:sourceId/status
 * Get processing status of a knowledge source
 */
router.get('/:id/knowledge/sources/:sourceId/status', async (req, res, next) => {
  try {
    const { sourceId } = req.params;

    const result = await pool.query(
      `SELECT
        id,
        status,
        error_message,
        chunk_count,
        processing_started_at,
        processing_completed_at,
        created_at,
        updated_at
      FROM knowledge_sources
      WHERE id = $1`,
      [sourceId]
    );

    if (result.rows.length === 0) {
      return res.status(404).json({ error: 'Knowledge source not found' });
    }

    const source = result.rows[0];

    // Calculate processing time if applicable
    let processingTimeSeconds = null;
    if (source.processing_started_at && source.processing_completed_at) {
      processingTimeSeconds =
        (new Date(source.processing_completed_at) - new Date(source.processing_started_at)) / 1000;
    }

    res.json({
      success: true,
      source_id: source.id,
      status: source.status,
      error_message: source.error_message,
      chunk_count: source.chunk_count,
      processing_time_seconds: processingTimeSeconds,
      created_at: source.created_at,
      updated_at: source.updated_at
    });

  } catch (error) {
    next(error);
  }
});

/**
 * 7. DELETE /api/chatbots/:id/knowledge/sources/:sourceId
 * Delete a knowledge source
 */
router.delete('/:id/knowledge/sources/:sourceId', async (req, res, next) => {
  try {
    const { id: chatbotId, sourceId } = req.params;

    const client = await pool.connect();

    try {
      await client.query('BEGIN');

      // Verify ownership
      const checkResult = await client.query(
        'SELECT id FROM knowledge_sources WHERE id = $1 AND chatbot_id = $2',
        [sourceId, chatbotId]
      );

      if (checkResult.rows.length === 0) {
        await client.query('ROLLBACK');
        return res.status(404).json({ error: 'Knowledge source not found' });
      }

      // Delete chunks first (foreign key constraint)
      await client.query('DELETE FROM document_chunks WHERE knowledge_source_id = $1', [sourceId]);

      // Delete source
      await client.query('DELETE FROM knowledge_sources WHERE id = $1', [sourceId]);

      await client.query('COMMIT');

      res.json({
        success: true,
        message: 'Knowledge source deleted successfully'
      });

    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }

  } catch (error) {
    next(error);
  }
});

/**
 * 8. POST /api/chatbots/:id/knowledge/search
 * Test RAG search
 */
router.post('/:id/knowledge/search', async (req, res, next) => {
  try {
    const { id: chatbotId } = req.params;
    const { query, limit = 5 } = req.body;

    if (!query) {
      return res.status(400).json({ error: 'Missing search query' });
    }

    // Call retrieve_knowledge function (this would be implemented in a Windmill script)
    // For now, return a placeholder response
    // TODO: Implement actual RAG search via Windmill script

    res.json({
      success: true,
      query,
      results: [],
      message: 'RAG search endpoint - implementation pending'
    });

  } catch (error) {
    next(error);
  }
});

/**
 * 9. GET /api/chatbots/:id/knowledge/quota
 * Get current quota usage
 */
router.get('/:id/knowledge/quota', async (req, res, next) => {
  try {
    const { id: chatbotId } = req.params;

    const result = await pool.query(
      `SELECT
        o.max_knowledge_pdfs,
        o.max_knowledge_urls,
        o.max_knowledge_ingestions_per_day,
        o.max_knowledge_storage_mb,
        o.current_knowledge_pdfs,
        o.current_knowledge_urls,
        o.current_storage_mb,
        COALESCE(dic.ingestion_count, 0) as today_ingestions
      FROM chatbots c
      JOIN organizations o ON c.organization_id = o.id
      LEFT JOIN daily_ingestion_counts dic
        ON dic.organization_id = o.id
        AND dic.date = CURRENT_DATE
      WHERE c.id = $1`,
      [chatbotId]
    );

    if (result.rows.length === 0) {
      return res.status(404).json({ error: 'Chatbot not found' });
    }

    const quota = result.rows[0];

    res.json({
      success: true,
      quota: {
        pdfs: {
          current: quota.current_knowledge_pdfs,
          max: quota.max_knowledge_pdfs,
          remaining: quota.max_knowledge_pdfs - quota.current_knowledge_pdfs
        },
        urls: {
          current: quota.current_knowledge_urls,
          max: quota.max_knowledge_urls,
          remaining: quota.max_knowledge_urls - quota.current_knowledge_urls
        },
        storage: {
          current_mb: parseFloat(quota.current_storage_mb),
          max_mb: quota.max_knowledge_storage_mb,
          remaining_mb: quota.max_knowledge_storage_mb - parseFloat(quota.current_storage_mb)
        },
        daily_ingestions: {
          today: quota.today_ingestions,
          max: quota.max_knowledge_ingestions_per_day,
          remaining: quota.max_knowledge_ingestions_per_day - quota.today_ingestions
        }
      }
    });

  } catch (error) {
    next(error);
  }
});

module.exports = router;


================================================
File: docker-compose.yml
================================================
version: "3.7"

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "${LOG_MAX_SIZE:-20m}"
    max-file: "${LOG_MAX_FILE:-10}"
    compress: "true"

services:
  db:
    deploy:
      # To use an external database, set replicas to 0 and set DATABASE_URL to the external database url in the .env file
      replicas: 1
    image: postgres:16
    shm_size: 1g
    restart: unless-stopped
    volumes:
      - db_data:/var/lib/postgresql/data
    expose:
      - 5432
    environment:
      POSTGRES_PASSWORD: ${WINDMILL_DB_PASSWORD}
      POSTGRES_DB: ${WINDMILL_DB_NAME}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging: *default-logging

  windmill_server:
    image: ${WM_IMAGE}

    deploy:
      replicas: 1
    restart: unless-stopped
    expose:
      - 8000
      - 2525
    environment:
      - DATABASE_URL=${WINDMILL_DATABASE_URL}
      - MODE=server
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - worker_logs:/tmp/windmill/logs

    logging: *default-logging

  windmill_worker:
    image: ${WM_IMAGE}

    deploy:
      replicas: 3
      resources:
        limits:
          cpus: "1"
          memory: 2048M
          # for GB, use syntax '2Gi'
    restart: unless-stopped
    # Uncomment to enable PID namespace isolation (recommended for security)
    # Requires privileged mode for --mount-proc flag
    # See: https://www.windmill.dev/docs/advanced/security_isolation
    # privileged: true
    environment:
      - DATABASE_URL=${WINDMILL_DATABASE_URL}
      - MODE=worker
      - WORKER_GROUP=default
      # Uncomment to enable PID namespace isolation (requires privileged: true above)
      # - ENABLE_UNSHARE_PID=true
    depends_on:
      db:
        condition: service_healthy
    # to mount the worker folder to debug, KEEP_JOB_DIR=true and mount /tmp/windmill
    volumes:
      # mount the docker socket to allow to run docker containers from within the workers
      - /var/run/docker.sock:/var/run/docker.sock
      - worker_dependency_cache:/tmp/windmill/cache
      - worker_logs:/tmp/windmill/logs

    logging: *default-logging

  ## This worker is specialized for "native" jobs. Native jobs run in-process and thus are much more lightweight than other jobs
  windmill_worker_native:
    # Use ghcr.io/windmill-labs/windmill-ee:main for the ee
    image: ${WM_IMAGE}

    deploy:
      replicas: 1
      resources:
        limits:
          cpus: "1"
          memory: 2048M
          # for GB, use syntax '2Gi'
    restart: unless-stopped
    # Uncomment to enable PID namespace isolation (recommended for security)
    # Requires privileged mode for --mount-proc flag
    # See: https://www.windmill.dev/docs/advanced/security_isolation
    # privileged: true
    environment:
      - DATABASE_URL=${WINDMILL_DATABASE_URL}
      - MODE=worker
      - WORKER_GROUP=native
      - NUM_WORKERS=8
      - SLEEP_QUEUE=200
      # Uncomment to enable PID namespace isolation (requires privileged: true above)
      # - ENABLE_UNSHARE_PID=true
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - worker_logs:/tmp/windmill/logs
    logging: *default-logging
  # This worker is specialized for reports or scraping jobs. It is assigned the "reports" worker group which has an init script that installs chromium and can be targeted by using the "chromium" worker tag.
  # windmill_worker_reports:
  #   image: ${WM_IMAGE}
  #   pull_policy: always
  #   deploy:
  #     replicas: 1
  #     resources:
  #       limits:
  #         cpus: "1"
  #         memory: 2048M
  #         # for GB, use syntax '2Gi'
  #   restart: unless-stopped
  #   # Uncomment to enable PID namespace isolation (recommended for security)
  #   # Requires privileged mode for --mount-proc flag
  #   # See: https://www.windmill.dev/docs/advanced/security_isolation
  #   # privileged: true
  #   environment:
  #     - DATABASE_URL=${DATABASE_URL}
  #     - MODE=worker
  #     - WORKER_GROUP=reports
  #     # Uncomment to enable PID namespace isolation (requires privileged: true above)
  #     # - ENABLE_UNSHARE_PID=true
  #   depends_on:
  #     db:
  #       condition: service_healthy
  #   # to mount the worker folder to debug, KEEP_JOB_DIR=true and mount /tmp/windmill
  #   volumes:
  #     # mount the docker socket to allow to run docker containers from within the workers
  #     - /var/run/docker.sock:/var/run/docker.sock
  #     - worker_dependency_cache:/tmp/windmill/cache
  #     - worker_logs:/tmp/windmill/logs

  # The indexer powers full-text job and log search, an EE feature.
  windmill_indexer:
    image: ${WM_IMAGE}

    deploy:
      replicas: 0 # set to 1 to enable full-text job and log search
    restart: unless-stopped
    expose:
      - 8002
    environment:
      - PORT=8002
      - DATABASE_URL=${WINDMILL_DATABASE_URL}
      - MODE=indexer
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - windmill_index:/tmp/windmill/search
      - worker_logs:/tmp/windmill/logs
    logging: *default-logging

  lsp:
    image: ghcr.io/windmill-labs/windmill-lsp:latest

    restart: unless-stopped
    expose:
      - 3001
    volumes:
      - lsp_cache:/pyls/.cache
    logging: *default-logging

  multiplayer:
    image: ghcr.io/windmill-labs/windmill-multiplayer:latest
    deploy:
      replicas: 0 # Set to 1 to enable multiplayer, only available on Enterprise Edition
    restart: unless-stopped
    expose:
      - 3002
    logging: *default-logging

  caddy:
    image: ghcr.io/windmill-labs/caddy-l4:latest
    restart: unless-stopped
    # Configure the mounted Caddyfile and the exposed ports or use another reverse proxy if needed
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      # - ./certs:/certs # Provide custom certificate files like cert.pem and key.pem to enable HTTPS - See the corresponding section in the Caddyfile
    ports:
      # To change the exposed port, simply change 80:80 to <desired_port>:80. No other changes needed
      - 8081:80
      - 25:25
      # - 443:443 # Uncomment to enable HTTPS handling by Caddy
    environment:
      - BASE_URL=":80"
      # - BASE_URL=":443" # uncomment and comment line above to enable HTTPS via custom certificate and key files
      # - BASE_URL=mydomain.com # Uncomment and comment line above to enable HTTPS handling by Caddy
    logging: *default-logging

  webhook-ingress:
    build: ./webhook-server
    container_name: webhook-ingress
    restart: unless-stopped
    ports:
      - "3000:3000" # Expose port 3000 to your host machine
    environment:
      - PORT=3000
      - WEBHOOK_VERIFY_TOKEN=${WEBHOOK_VERIFY_TOKEN} # Defined in your .env
      - WINDMILL_TOKEN=${WINDMILL_TOKEN}             # Defined in your .env
      - WINDMILL_MESSAGE_PROCESSING_ENDPOINT=${WINDMILL_MESSAGE_PROCESSING_ENDPOINT} # Defined in your .env
      - DB_HOST=business_logic_db
      - DB_PORT=5432
      - DB_USER=${BUSINESS_LOGIC_DB_USER}
      - DB_PASSWORD=${BUSINESS_LOGIC_DB_PASSWORD}
      - DB_NAME=${BUSINESS_LOGIC_DB_NAME}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      business_logic_db:
        condition: service_healthy
      windmill_server:
        condition: service_started
      redis:
        condition: service_healthy
  
  business_logic_db:
    build: ./docker/postgres
    container_name: business_logic_db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${BUSINESS_LOGIC_DB_USER}
      POSTGRES_PASSWORD: ${BUSINESS_LOGIC_DB_PASSWORD}
      POSTGRES_DB: ${BUSINESS_LOGIC_DB_NAME}
    volumes:
      - business_logic_db_data:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d
    ports:
      - "5433:5432" # Expose on port 5433 to avoid conflict with Windmill's DB on 5432
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${BUSINESS_LOGIC_DB_USER} -d ${BUSINESS_LOGIC_DB_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 5

  mcp_pricing_calculator:
    build: ./mcp-servers/pricing-calculator
    container_name: mcp_pricing_calculator
    restart: unless-stopped
    expose:
      - 3001
    environment:
      - PORT=3001
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3001/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 3s
      retries: 3
    logging: *default-logging

  mcp_lead_capture:
    build: ./mcp-servers/lead-capture
    container_name: mcp_lead_capture
    restart: unless-stopped
    expose:
      - 3002
    environment:
      - PORT=3002
      - DB_HOST=business_logic_db
      - DB_PORT=5432
      - DB_USER=${BUSINESS_LOGIC_DB_USER}
      - DB_PASSWORD=${BUSINESS_LOGIC_DB_PASSWORD}
      - DB_NAME=${BUSINESS_LOGIC_DB_NAME}
    depends_on:
      business_logic_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3002/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 3s
      retries: 3
    logging: *default-logging

  mcp_contact_owner:
    build: ./mcp-servers/contact-owner
    container_name: mcp_contact_owner
    restart: unless-stopped
    expose:
      - 3003
    environment:
      - PORT=3003
      - DB_HOST=business_logic_db
      - DB_PORT=5432
      - DB_USER=${BUSINESS_LOGIC_DB_USER}
      - DB_PASSWORD=${BUSINESS_LOGIC_DB_PASSWORD}
      - DB_NAME=${BUSINESS_LOGIC_DB_NAME}
    depends_on:
      business_logic_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3003/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 3s
      retries: 3
    logging: *default-logging

  whatsapp_chatbot_api:
    build: ./api-server
    container_name: whatsapp_chatbot_api
    restart: unless-stopped
    ports:
      - "4000:4000"
    environment:
      - PORT=4000
      - NODE_ENV=production
      - DB_HOST=business_logic_db
      - DB_PORT=5432
      - DB_USER=${BUSINESS_LOGIC_DB_USER}
      - DB_PASSWORD=${BUSINESS_LOGIC_DB_PASSWORD}
      - DB_NAME=${BUSINESS_LOGIC_DB_NAME}
      - WINDMILL_URL=http://windmill_server:8000
      - WINDMILL_TOKEN=${WINDMILL_TOKEN}
      - WINDMILL_WORKSPACE=development
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      business_logic_db:
        condition: service_healthy
      windmill_server:
        condition: service_started
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:4000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 3s
      retries: 3
    logging: *default-logging

  redis:
    image: redis:7-alpine
    container_name: whatsapp_chatbot_redis
    restart: unless-stopped
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    logging: *default-logging

volumes:
  db_data: null
  business_logic_db_data: null
  worker_dependency_cache: null
  worker_logs: null
  worker_memory: null
  windmill_index: null
  lsp_cache: null
  caddy_data: null


================================================
File: f/development/2_whatsapp_llm_processing.script.lock
================================================
# py: 3.12
annotated-types==0.7.0
anyio==4.12.0
beautifulsoup4==4.14.3
cachetools==6.2.4
certifi==2025.11.12
charset-normalizer==3.4.4
distro==1.9.0
google==3.0.0
google-auth==2.45.0
google-genai==1.56.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.11
jiter==0.12.0
openai==2.14.0
psycopg2-binary==2.9.11
pyasn1==0.6.1
pyasn1-modules==0.4.2
pydantic==2.12.5
pydantic-core==2.41.5
requests==2.32.5
rsa==4.9.1
sniffio==1.3.1
soupsieve==2.8.1
tenacity==9.1.2
tqdm==4.67.1
typing-extensions==4.15.0
typing-inspection==0.4.2
urllib3==2.6.2
websockets==15.0.1
wmill==1.601.1

================================================
File: f/development/utils/alert_on_failure.py
================================================
"""
Failure Alert Handler for Windmill Flows

This script is called by the flow's failure_module when any step fails.
It sends alerts to configured channels (Slack, email) and logs to database.

Usage:
Configure this as the failure_module in flow.yaml:

failure_module:
  id: failure
  value:
    type: script
    path: f/development/utils/alert_on_failure
    input_transforms:
      error_message:
        type: javascript
        expr: error.message
      step_id:
        type: javascript
        expr: error.step_id
      chatbot_id:
        type: javascript
        expr: flow_input.phone_number_id || 'unknown'
      user_phone:
        type: javascript
        expr: flow_input.user_phone || 'unknown'
"""

import wmill
import psycopg2
from typing import Dict, Any, Optional
import json
import os


def main(
    error_message: str,
    step_id: str,
    chatbot_id: Optional[str] = None,
    user_phone: Optional[str] = None,
    error_name: Optional[str] = None,
    slack_webhook_url: str = wmill.get_variable("u/admin/SLACK_ALERT_WEBHOOK"),
    db_resource: str = "f/development/business_layer_db_postgreSQL",
) -> Dict[str, Any]:
    """
    Handle flow failure by sending alerts and logging to database.

    Args:
        error_message: Error message from failed step
        step_id: ID of the step that failed
        chatbot_id: ID of the chatbot (if available)
        user_phone: Phone number of user (if available)
        error_name: Name of the error
        slack_webhook_url: Slack webhook URL for alerts
        db_resource: Database resource path

    Returns:
        Alert result
    """

    # Get flow context
    flow_id = os.environ.get("WM_ROOT_FLOW_JOB_ID", "unknown")
    job_id = os.environ.get("WM_JOB_ID", "unknown")

    print(f"Flow failure detected:")
    print(f"  Flow ID: {flow_id}")
    print(f"  Job ID: {job_id}")
    print(f"  Step ID: {step_id}")
    print(f"  Error: {error_message}")

    # Determine severity based on step and error
    severity = determine_severity(step_id, error_message)

    # Create alert record in database
    alert_id = log_to_database(
        error_message=error_message,
        step_id=step_id,
        chatbot_id=chatbot_id,
        severity=severity,
        metadata={
            "flow_id": flow_id,
            "job_id": job_id,
            "user_phone": user_phone,
            "error_name": error_name,
        },
        db_resource=db_resource
    )

    # Send Slack notification if configured
    slack_result = None
    if slack_webhook_url and slack_webhook_url != "":
        slack_result = send_slack_alert(
            error_message=error_message,
            step_id=step_id,
            chatbot_id=chatbot_id or "Unknown",
            user_phone=user_phone or "Unknown",
            flow_id=flow_id,
            severity=severity,
            webhook_url=slack_webhook_url
        )

    result = {
        "success": True,
        "alert_id": alert_id,
        "slack_sent": slack_result is not None,
        "severity": severity,
        "flow_id": flow_id,
        "recover": False,  # Don't retry automatically
    }

    # If called from Step 6 (conditional alert), raise exception to mark flow as FAILED
    # This makes rate-limited runs show as failed instead of successful
    if error_name in ["RATE_LIMIT_ERROR", "LLM_ERROR"]:
        print(f"Alert sent successfully. Raising exception to mark flow as failed.")
        raise Exception(f"[{severity.upper()}] {error_name}: {error_message[:200]}")

    return result


def determine_severity(step_id: str, error_message: str) -> str:
    """
    Determine alert severity based on step and error.

    Args:
        step_id: ID of failed step
        error_message: Error message

    Returns:
        Severity level: 'critical', 'error', 'warning', 'info'
    """
    error_lower = error_message.lower()

    # Critical errors
    if "quota" in error_lower or "limit" in error_lower:
        return "critical"

    if "database" in error_lower or "connection" in error_lower:
        return "critical"

    # Step-specific severity
    if step_id == "step1":
        # Context loading failures are critical
        return "critical"

    elif step_id == "step2":
        # LLM failures are errors but not critical
        return "error"

    elif step_id in ["step3a", "step3b", "step3c"]:
        # Output step failures are warnings
        return "warning"

    # Default
    return "error"


def log_to_database(
    error_message: str,
    step_id: str,
    chatbot_id: Optional[str],
    severity: str,
    metadata: Dict[str, Any],
    db_resource: str
) -> Optional[int]:
    """
    Log alert to system_alerts table.

    Args:
        error_message: Error message
        step_id: Failed step ID
        chatbot_id: Chatbot ID
        severity: Alert severity
        metadata: Additional metadata
        db_resource: Database resource path

    Returns:
        Alert ID or None if failed
    """
    try:
        raw_config = wmill.get_resource(db_resource)
        db_params = {
            "host": raw_config.get("host"),
            "port": raw_config.get("port"),
            "user": raw_config.get("user"),
            "password": raw_config.get("password"),
            "dbname": raw_config.get("dbname"),
            "sslmode": "disable",
        }

        conn = psycopg2.connect(**db_params)
        cur = conn.cursor()

        # Get organization_id from chatbot if available
        organization_id = None
        if chatbot_id and chatbot_id != "unknown":
            cur.execute(
                "SELECT organization_id FROM chatbots WHERE id = %s",
                (chatbot_id,)
            )
            result = cur.fetchone()
            if result:
                organization_id = result[0]

        # Insert alert
        cur.execute(
            """
            INSERT INTO system_alerts (
                organization_id,
                chatbot_id,
                type,
                severity,
                message,
                metadata,
                created_at
            ) VALUES (%s, %s, 'WEBHOOK_FAILURE', %s, %s, %s, NOW())
            RETURNING id
            """,
            (
                organization_id,
                chatbot_id if chatbot_id != "unknown" else None,
                severity,
                f"Flow step '{step_id}' failed: {error_message}",
                json.dumps(metadata)
            )
        )

        alert_id = cur.fetchone()[0]
        conn.commit()

        cur.close()
        conn.close()

        print(f"Alert logged to database: {alert_id}")
        return alert_id

    except Exception as e:
        print(f"Failed to log alert to database: {e}")
        return None


def send_slack_alert(
    error_message: str,
    step_id: str,
    chatbot_id: str,
    user_phone: str,
    flow_id: str,
    severity: str,
    webhook_url: str
) -> Optional[Dict[str, Any]]:
    """
    Send alert to Slack via webhook.

    Args:
        error_message: Error message
        step_id: Failed step ID
        chatbot_id: Chatbot ID
        user_phone: User phone number
        flow_id: Flow job ID
        severity: Alert severity
        webhook_url: Slack webhook URL

    Returns:
        Response dict or None if failed
    """
    try:
        import requests

        # Choose emoji and color based on severity
        emoji_map = {
            "critical": ":fire:",
            "error": ":x:",
            "warning": ":warning:",
            "info": ":information_source:",
        }

        color_map = {
            "critical": "#FF0000",  # Red
            "error": "#FF6B6B",     # Light red
            "warning": "#FFD93D",   # Yellow
            "info": "#6BCB77",      # Green
        }

        emoji = emoji_map.get(severity, ":question:")
        color = color_map.get(severity, "#808080")

        # Build Slack message
        payload = {
            "username": "Windmill Alert Bot",
            "icon_emoji": ":robot_face:",
            "attachments": [
                {
                    "color": color,
                    "title": f"{emoji} Flow Failure - {severity.upper()}",
                    "text": f"WhatsApp webhook processor flow failed",
                    "fields": [
                        {
                            "title": "Step",
                            "value": step_id,
                            "short": True
                        },
                        {
                            "title": "Chatbot ID",
                            "value": chatbot_id,
                            "short": True
                        },
                        {
                            "title": "User Phone",
                            "value": user_phone,
                            "short": True
                        },
                        {
                            "title": "Flow ID",
                            "value": flow_id,
                            "short": True
                        },
                        {
                            "title": "Error Message",
                            "value": f"```{error_message[:500]}```",  # Truncate long errors
                            "short": False
                        }
                    ],
                    "footer": "Windmill Error Handler",
                    "ts": int(os.environ.get("WM_JOB_STARTED_AT", "0")) or None
                }
            ]
        }

        response = requests.post(
            webhook_url,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=10
        )

        response.raise_for_status()

        print(f"Slack alert sent successfully")
        return {"status": "sent", "response_code": response.status_code}

    except Exception as e:
        print(f"Failed to send Slack alert: {e}")
        return None


================================================
File: mcp-servers/pricing-calculator/package.json
================================================
{
  "name": "pricing-calculator-mcp",
  "version": "1.0.0",
  "description": "MCP server for calculating WhatsApp chatbot pricing in Mexican pesos",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "nodemon server.js"
  },
  "keywords": ["mcp", "pricing", "whatsapp", "chatbot"],
  "author": "JD Labs",
  "license": "MIT",
  "dependencies": {
    "express": "^4.18.2"
  },
  "devDependencies": {
    "nodemon": "^3.0.1"
  }
}


================================================
File: tests/unit/test_quota_enforcement.py
================================================
"""
Unit tests for knowledge base quota enforcement.

Tests the check_knowledge_quota.py utility with all quota limit scenarios.
"""

import pytest
from unittest.mock import Mock, patch
import sys
import importlib.util
from pathlib import Path

# ============================================================================
# MOCK WMILL MODULE BEFORE IMPORTING MODULE UNDER TEST
# ============================================================================

# Create a mock wmill module
mock_wmill = Mock()
sys.modules['wmill'] = mock_wmill

# Now dynamically import the module under test
MODULE_PATH = Path(__file__).parent.parent.parent / "f" / "development" / "utils" / "check_knowledge_quota.py"
spec = importlib.util.spec_from_file_location("check_knowledge_quota", MODULE_PATH)
check_knowledge_quota_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(check_knowledge_quota_module)

# Get the main function
check_quota = check_knowledge_quota_module.main


@pytest.mark.unit
class TestQuotaEnforcement:
    """Test quota enforcement for knowledge base operations."""

    @pytest.fixture
    def mock_db_resource(self, test_db_config):
        """Mock database resource that returns test database config."""
        return {
            "host": test_db_config["host"],
            "port": test_db_config["port"],
            "user": test_db_config["user"],
            "password": test_db_config["password"],
            "dbname": test_db_config["dbname"]
        }

    @pytest.fixture
    def quota_data(self):
        """Sample organization quota data."""
        return {
            "org_id": "11111111-1111-1111-1111-111111111111",
            "max_pdfs": 50,
            "max_urls": 20,
            "max_daily_ingestions": 100,
            "max_storage": 500,
            "current_pdfs": 10,
            "current_urls": 5,
            "current_storage": 100,
            "today_ingestions": 25
        }

    def test_pdf_quota_available(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test PDF upload allowed when under quota."""
        # Setup: Insert test data
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_pdfs = %s,
                current_knowledge_pdfs = %s,
                max_knowledge_storage_mb = %s,
                current_storage_mb = %s,
                max_knowledge_ingestions_per_day = %s
            WHERE id = %s
        """, (
            quota_data["max_pdfs"],
            quota_data["current_pdfs"],
            quota_data["max_storage"],
            quota_data["current_storage"],
            quota_data["max_daily_ingestions"],
            quota_data["org_id"]
        ))

        # Execute: Mock wmill.get_resource and call check_quota
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="pdf",
                file_size_mb=10.0,
                db_resource="test_resource"
            )

        # Assert: Should be allowed
        assert result["allowed"] is True
        assert result["quota_type"] is None
        assert result["current"] == quota_data["current_pdfs"]
        assert result["max"] == quota_data["max_pdfs"]
        assert result["remaining"] == quota_data["max_pdfs"] - quota_data["current_pdfs"]

    def test_pdf_quota_exceeded(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test PDF upload blocked when quota exceeded."""
        # Setup: Set current PDFs to max
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_pdfs = %s,
                current_knowledge_pdfs = %s
            WHERE id = %s
        """, (
            quota_data["max_pdfs"],
            quota_data["max_pdfs"],  # At limit
            quota_data["org_id"]
        ))

        # Execute: Mock wmill.get_resource and call check_quota
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="pdf",
                file_size_mb=5.0,
                db_resource="test_resource"
            )

        # Assert: Should be blocked
        assert result["allowed"] is False
        assert result["quota_type"] == "PDF_LIMIT_EXCEEDED"
        assert result["current"] == quota_data["max_pdfs"]
        assert result["max"] == quota_data["max_pdfs"]
        assert result["remaining"] == 0

    def test_url_quota_exceeded(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test URL ingestion blocked when quota exceeded."""
        # Setup: Set current URLs to max
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_urls = %s,
                current_knowledge_urls = %s
            WHERE id = %s
        """, (
            quota_data["max_urls"],
            quota_data["max_urls"],  # At limit
            quota_data["org_id"]
        ))

        # Execute: Mock wmill.get_resource and call check_quota
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="url",
                file_size_mb=0.5,
                db_resource="test_resource"
            )

        # Assert: Should be blocked
        assert result["allowed"] is False
        assert result["quota_type"] == "URL_LIMIT_EXCEEDED"
        assert result["current"] == quota_data["max_urls"]
        assert result["max"] == quota_data["max_urls"]
        assert result["remaining"] == 0

    def test_storage_quota_exceeded(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test upload blocked when storage quota exceeded."""
        # Setup: Storage almost at limit
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_storage_mb = %s,
                current_storage_mb = %s
            WHERE id = %s
        """, (
            quota_data["max_storage"],
            quota_data["max_storage"] - 5,  # 5MB remaining
            quota_data["org_id"]
        ))

        # Execute: Mock wmill.get_resource and call check_quota
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="pdf",
                file_size_mb=10.0,  # Too large
                db_resource="test_resource"
            )

        # Assert: Should be blocked
        assert result["allowed"] is False
        assert result["quota_type"] == "STORAGE_LIMIT_EXCEEDED"
        assert result["max"] == quota_data["max_storage"]

    def test_daily_ingestion_quota_exceeded(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test ingestion blocked when daily limit exceeded."""
        # Setup: Add daily ingestion count at limit
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_ingestions_per_day = %s
            WHERE id = %s
        """, (
            quota_data["max_daily_ingestions"],
            quota_data["org_id"]
        ))

        db_with_autocommit.execute("""
            INSERT INTO daily_ingestion_counts (organization_id, date, ingestion_count)
            VALUES (%s, CURRENT_DATE, %s)
            ON CONFLICT (organization_id, date)
            DO UPDATE SET ingestion_count = %s
        """, (
            quota_data["org_id"],
            quota_data["max_daily_ingestions"],
            quota_data["max_daily_ingestions"]
        ))

        # Execute: Mock wmill.get_resource and call check_quota
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="pdf",
                file_size_mb=1.0,
                db_resource="test_resource"
            )

        # Assert: Should be blocked
        assert result["allowed"] is False
        assert result["quota_type"] == "DAILY_INGESTION_LIMIT_EXCEEDED"
        assert result["current"] == quota_data["max_daily_ingestions"]
        assert result["max"] == quota_data["max_daily_ingestions"]
        assert result["remaining"] == 0

    def test_chatbot_not_found(self, mock_db_resource):
        """Test error handling for non-existent chatbot."""
        # Execute: Mock wmill.get_resource and call check_quota
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="00000000-0000-0000-0000-000000000000",
                source_type="pdf",
                file_size_mb=1.0,
                db_resource="test_resource"
            )

        # Assert: Should return not found error
        assert result["allowed"] is False
        assert result["quota_type"] == "CHATBOT_NOT_FOUND"

    def test_remaining_calculation(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test correct calculation of remaining quota."""
        # Setup: Set specific quota values
        current = 15
        maximum = 50

        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_pdfs = %s,
                current_knowledge_pdfs = %s
            WHERE id = %s
        """, (maximum, current, quota_data["org_id"]))

        # Execute: Mock wmill.get_resource and call check_quota
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="pdf",
                file_size_mb=1.0,
                db_resource="test_resource"
            )

        # Assert: Remaining should be correctly calculated
        assert result["allowed"] is True
        assert result["remaining"] == maximum - current  # Should be 35

    def test_edge_case_exactly_at_storage_limit(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test storage quota when exactly at limit (edge case)."""
        # Setup: Storage at exactly max - 1MB
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_storage_mb = %s,
                current_storage_mb = %s
            WHERE id = %s
        """, (
            100,
            99,  # Exactly 1MB remaining
            quota_data["org_id"]
        ))

        # Execute: Test 1 - 0.5MB file should be allowed
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="pdf",
                file_size_mb=0.5,
                db_resource="test_resource"
            )
        assert result["allowed"] is True

        # Execute: Test 2 - 1.5MB file should be blocked
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="pdf",
                file_size_mb=1.5,
                db_resource="test_resource"
            )
        assert result["allowed"] is False
        assert result["quota_type"] == "STORAGE_LIMIT_EXCEEDED"

    def test_different_plan_tiers(self, mock_db_resource, db_with_autocommit):
        """Test that different organizations can have different quotas."""
        # Setup: Create org with free tier (low limits)
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_pdfs = 10,
                current_knowledge_pdfs = 5,
                plan_tier = 'free'
            WHERE id = '11111111-1111-1111-1111-111111111111'
        """)

        # Execute: Test free tier chatbot
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="pdf",
                file_size_mb=1.0,
                db_resource="test_resource"
            )

        # Assert: Should respect free tier limits
        assert result["allowed"] is True
        assert result["max"] == 10
        assert result["remaining"] == 5

    def test_doc_type_uses_pdf_quota(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test that 'doc' source type uses PDF quota limits."""
        # Setup: Set PDF quota
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_pdfs = %s,
                current_knowledge_pdfs = %s
            WHERE id = %s
        """, (
            quota_data["max_pdfs"],
            quota_data["current_pdfs"],
            quota_data["org_id"]
        ))

        # Execute: Check quota for 'doc' type
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="doc",
                file_size_mb=1.0,
                db_resource="test_resource"
            )

        # Assert: Should use PDF quota
        assert result["allowed"] is True
        assert result["current"] == quota_data["current_pdfs"]
        assert result["max"] == quota_data["max_pdfs"]

    def test_unknown_source_type_allows_with_default_remaining(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test that unknown source types are allowed with default remaining."""
        # Setup: Basic organization setup
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_pdfs = %s,
                current_knowledge_pdfs = %s
            WHERE id = %s
        """, (
            quota_data["max_pdfs"],
            quota_data["current_pdfs"],
            quota_data["org_id"]
        ))

        # Execute: Check quota for unknown type
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="unknown_type",
                file_size_mb=1.0,
                db_resource="test_resource"
            )

        # Assert: Should be allowed with default remaining
        assert result["allowed"] is True
        assert result["remaining"] == 999  # Default value for unknown types

    def test_url_quota_available(self, mock_db_resource, quota_data, db_with_autocommit):
        """Test URL ingestion allowed when under quota."""
        # Setup: Set URL quota well under limit
        db_with_autocommit.execute("""
            UPDATE organizations
            SET max_knowledge_urls = %s,
                current_knowledge_urls = %s
            WHERE id = %s
        """, (
            quota_data["max_urls"],
            quota_data["current_urls"],
            quota_data["org_id"]
        ))

        # Execute: Check quota for URL
        with patch.object(check_knowledge_quota_module, 'wmill') as mock_wmill_call:
            mock_wmill_call.get_resource.return_value = mock_db_resource

            result = check_quota(
                chatbot_id="22222222-2222-2222-2222-222222222222",
                source_type="url",
                file_size_mb=0.1,
                db_resource="test_resource"
            )

        # Assert: Should be allowed
        assert result["allowed"] is True
        assert result["quota_type"] is None
        assert result["current"] == quota_data["current_urls"]
        assert result["max"] == quota_data["max_urls"]
        assert result["remaining"] == quota_data["max_urls"] - quota_data["current_urls"]


================================================
File: webhook-server/package.json
================================================
{
    "name": "whatsapp-webhook-ingress",
    "version": "1.0.0",
    "type": "module",
    "main": "app.js",
    "scripts": {
        "start": "node app.js"
    },
    "dependencies": {
        "axios": "^1.6.0",
        "express": "^4.18.0",
        "pg": "^8.11.3",
        "redis": "^4.6.0"
    }
}

================================================
File: docs/ARCHITECTURE.md
================================================
Project Structure
    
src/
├── docker-compose.yml              # Main services (Windmill, Postgres, APIs)
├── webhook-server/                 # Node.js WhatsApp webhook handler (Port 3000)
├── api-server/                     # Knowledge Base Management API (Port 4000)
├── mcp-servers/                    # Model Context Protocol Servers
│   ├── lead-capture/               # CRM integration tool (Port 3002)
│   ├── pricing-calculator/         # Pricing logic tool (Port 3001)
│   └── contact-owner/              # Notification tool
├── db/                             # Database schema and migrations
├── f/development/                  # Windmill scripts (Business Logic)
│   ├── 1_whatsapp_context_loading.py
│   ├── 2_whatsapp_llm_processing.py
│   ├── RAG_process_documents.py
│   └── ...
├── tests/                          # Test Suite
│   ├── integration/                # End-to-end flow tests
│   ├── live/                       # Real API tests (OpenAI/Gemini)
│   └── unit/                       # Logic verification
└── docs/                           # Documentation

  

Quick Start
Prerequisites

    Docker & Docker Compose

    PostgreSQL 16+ with pgvector extension

    OpenAI API key (for embeddings)

    Google Gemini API key (for LLM)

    WhatsApp Business Account

1. Environment Setup
code Bash

    
cp .env.example .env
# Edit .env with your credentials

  

2. Start Services
code Bash

    
docker-compose up -d

# Services:
# - Windmill: http://localhost:8081
# - Webhook Server: http://localhost:3000
# - Knowledge API: http://localhost:4000
# - MCP Servers: http://localhost:3001, http://localhost:3002

  

3. Initialize Database
code Bash

    
# Full reset (Drop, Create, Seed)
./db/manage_db.sh reset

  

4. Configure Webhooks

Set your WhatsApp Webhook URL to: https://your-domain.com/webhook
Key Features Implementation
1. Agent Loop with MCP Tools

Located in f/development/2_whatsapp_llm_processing.py

The agent automatically discovers and calls tools defined in mcp-servers/.
Available Tools:

    Pricing Calculator: Calculates costs based on volume/tier (Port 3001).

    Lead Capture: Upserts contact info into the CRM table (Port 3002).

    Contact Owner: Notifies human agents.

2. RAG Knowledge Base API

A dedicated API (api-server) handles document ingestion.

Endpoints:

    POST /api/chatbots/:id/knowledge/upload (PDF/DOCX)

    POST /api/chatbots/:id/knowledge/url (Single URL)

    POST /api/chatbots/:id/knowledge/crawl (Web Crawler)

Processing Pipeline:
API Request → Windmill Job → Text Extraction → Chunking → OpenAI Embedding → pgvector Storage.
3. Usage Quota Enforcement

Automatic Enforcement:

    Real-time checking of Message and Token limits via 3_3_log_usage.py and middleware/quota.js.

    Configurable per-tier limits (Free, Pro, Enterprise).

    Automatic rejection of messages when over quota.

Testing

The project includes a comprehensive test suite in tests/.
code Bash

    
# Run Unit & Integration tests
pytest tests/

# Run Live tests (Real LLM calls - Costs money!)
pytest tests/live/ -m live

# Run specific integration flow
pytest tests/integration/test_full_flow.py

  

Monitoring

Key Tables:

    system_alerts: Critical failures and quota warnings.

    usage_logs: granular billing data.

    tool_executions: Performance and success/failure rates of MCP tools.

Roadmap
Completed ✅

Multi-tenant database schema with pgvector

WhatsApp webhook ingress & signature verification

3-step Windmill processing flow

Agent loop with MCP Tool support

Knowledge Base API (Upload, Crawl, Ingest)

Usage tracking and quota enforcement

"Lead Capture" & "Pricing" MCP Servers

    Live & Integration Test Suite

In Progress 🚧

Dashboard UI (Frontend)

Advanced RAG (Hybrid search with re-ranking)

    Google Docs integration

Future 🔮

WhatsApp media handling (images, voice notes)

Visual flow builder

    Real-time WebSocket monitoring
    
---

### 2. `docs/ARCHITECTURE.md`

Updated to include the API Server and MCP Servers in the system design, reflecting the microservices approach for tools and management.

```markdown
# Whatsaapp chatbot platform - Architecture

## 1. Overview
A Multi-tenant WhatsApp Chatbot Platform (SaaS) designed for scale and extensibility.
- **Tenants:** Businesses who upload data and configure bots.
- **Users:** End customers chatting via WhatsApp.
- **Goal:** Intelligent RAG-based answers + Agentic capabilities via standard protocols (MCP).

## 2. Tech Stack

### Core Services
- **Ingress Layer:** Node.js (Express)
  - Handles high-concurrency Webhook traffic from Meta.
- **Management Layer:** Node.js (Express)
  - `api-server` (Port 4000) for Knowledge Base management and RAG ingestion.
- **Orchestration:** Windmill
  - Manages the logic flow, state, and background processing (RAG jobs).
- **Tooling:** MCP Servers (Node.js)
  - Microservices exposing tools like Pricing Calculation and Lead Capture.

### Data & AI
- **Database:** PostgreSQL
  - Relational data: Tenants, Users, Logs.
  - Vector data: `pgvector` for embeddings (HNSW indexes).
- **AI/LLM:** Google Gemini / OpenAI
  - Handles reasoning, tool selection, and text generation.

## 3. System Architecture Diagram

```mermaid
flowchart TD
    User((WhatsApp User)) <--> Meta[Meta Cloud API]
    Admin((Admin User)) <--> API[Knowledge API :4000]
    
    Meta -- Webhook POST --> Ingress[Webhook Ingress :3000]
    
    subgraph "Orchestration & Logic"
        Ingress -- Async Trigger --> WM[Windmill Orchestrator]
        WM -- Tool Calls --> MCP[MCP Servers]
        WM -- Scripts --> Scripts[Python Scripts]
        API -- Async Jobs --> WM
    end
    
    subgraph "Data Persistence"
        Scripts --> DB[(PostgreSQL)]
        Scripts --> Vector[(pgvector)]
        WM --> S3[Object Storage]
    end
    
    subgraph "AI Services"
        Scripts --> LLM[OpenAI / Gemini]
    end
    
    WM -- Reply --> Meta

  

4. Data Flows
A. The "Agentic" Conversation Flow

    Ingestion: Node.js receives POST /webhook, verifies signature, pushes payload to Windmill.

    Context Loading (Step 1):

        Windmill loads Tenant config, User Profile, and Chat History.

        Checks Idempotency and Usage Quotas.

    AI Reasoning (Step 2):

        Inputs: History, System Prompt, RAG Context, Available Tools.

        Loop:

            LLM decides to call a tool (e.g., calculate_pricing).

            Windmill executes the tool via HTTP to the relevant MCP Server.

            Result is fed back to LLM.

    Response (Step 3): Final text sent to WhatsApp API.

    Persistence: Logs saved to messages and usage_logs.

B. The Knowledge Ingestion Flow

    Upload: Admin sends PDF/URL to Knowledge API (:4000).

    Job Creation: API validates quota and triggers a Windmill background job.

    Processing:

        Windmill script RAG_process_documents.py runs.

        Text extracted -> Chunked -> Embedded (OpenAI).

    Storage: Embeddings stored in document_chunks table with HNSW index.

5. Database Schema Strategy

    Tenants & Config: organizations, chatbots, org_integrations.

    RAG: knowledge_sources, document_chunks (vector).

    Metrics: usage_logs, usage_summary, tool_executions.

    Operational: webhook_events (idempotency), system_alerts.

code Code

    
---

### 3. `docs/DEVELOPMENT_WORKFLOW.md`

Added sections for MCP server development and the specific testing requirements for the new architecture.

```markdown
# Development Workflow

## Database Schema Changes

**CRITICAL:** Whenever you modify the database schema (create.sql, seed.sql), you MUST:

1. **Reset the database:**
   ```bash
   ./db/manage_db.sh reset

  

    Verify your changes:

        Check that all new tables/columns exist.

        Verify triggers (especially usage and timestamp triggers).

MCP Server Development

The platform uses the Model Context Protocol (MCP) for tools. MCP servers are located in mcp-servers/.

Adding a new MCP Tool:

    Create a new directory in mcp-servers/.

    Implement the server (Express.js recommended).

    Expose GET /tools to return tool definitions (JSON Schema).

    Implement POST /tools/:tool_name for execution.

    Register the tool in the database (table org_integrations) or via the setup script.

    Add the service to docker-compose.yml.

Windmill Script Changes

After creating or modifying scripts in f/development/:

    Generate metadata:
    code Bash

    
wmill script generate-metadata

  

Sync:
code Bash

        
    wmill sync push

      

Testing Workflow
1. Unit & Integration Tests

Run these frequently. They mock external services (LLM, WhatsApp) and test logic + database integrity.
code Bash

    
pytest tests/unit
pytest tests/integration

  

2. Live Tests (Cost Warning ⚠️)

Tests in tests/live/ make real API calls to OpenAI and Google Gemini.

    Requires OPENAI_API_KEY and GOOGLE_API_KEY in .env.

    Run only when necessary to verify API contracts.

code Bash

    
pytest tests/live/ -m live

  

3. API & Webhook Testing

    API Server: curl http://localhost:4000/health

    MCP Servers: curl http://localhost:3001/health

Git Workflow
Branch Strategy
code Code

    
master (production-ready)
  ├── feature/rag-knowledge-base (Backend API work)
  ├── feature/mcp-tools (Tool development)
  └── fix/issue-description

  

Pre-Commit Checklist

Database reset and verified (./db/manage_db.sh reset)

Windmill metadata generated

Docker containers build successfully (including new MCP servers)

pytest tests/integration passes

No secrets in code (use Windmill variables)

================================================
File: f/development/1_whatsapp_context_loading.py
================================================
import wmill
import psycopg2
from psycopg2.extras import RealDictCursor
from typing import Dict, Any, Optional


def main(
    whatsapp_phone_id: str,
    user_phone: str,
    message_id: str,  # NEW: WhatsApp message ID for idempotency
    user_name: str = "Unknown",
    db_resource: str = "f/development/business_layer_db_postgreSQL",
) -> Dict[str, Any]:
    """
    Step 1: Context Loading + Idempotency Check + Usage Limits
    
    This step:
    1. Checks for duplicate messages (idempotency)
    2. Validates tenant usage limits
    3. Loads chatbot configuration
    4. Fetches user context and history
    5. Loads enabled tools/integrations
    """

    # Setup DB Connection
    raw_config = wmill.get_resource(db_resource)
    db_params = {
        "host": raw_config.get("host"),
        "port": raw_config.get("port"),
        "user": raw_config.get("user"),
        "password": raw_config.get("password"),
        "dbname": raw_config.get("dbname"),
        "sslmode": "disable",
    }

    try:
        conn = psycopg2.connect(**db_params)
        cur = conn.cursor(cursor_factory=RealDictCursor)
    except Exception as e:
        print(f"DB Connection Error: {e}")
        return {
            "proceed": False,
            "reason": f"DB Connection Failed: {str(e)}",
            "notify_admin": True,
        }

    try:
        # ============================================================
        # STEP 1A: IDEMPOTENCY CHECK
        # ============================================================
        # Check if we've already processed this exact message
        idempotency_check = """
            SELECT id, status, processed_at
            FROM webhook_events
            WHERE whatsapp_message_id = %s
        """
        cur.execute(idempotency_check, (message_id,))
        existing_event = cur.fetchone()

        if existing_event:
            status = existing_event["status"]
            
            if status == "completed":
                print(f"Duplicate message detected: {message_id} (already completed)")
                return {
                    "proceed": False,
                    "reason": "Duplicate - Already Processed",
                    "webhook_event_id": existing_event["id"],
                }
            
            elif status == "processing":
                print(f"Duplicate message detected: {message_id} (currently processing)")
                return {
                    "proceed": False,
                    "reason": "Duplicate - Currently Processing",
                    "webhook_event_id": existing_event["id"],
                }
            
            elif status == "failed":
                print(f"Retrying previously failed message: {message_id}")
                # Allow retry of failed messages
                # Update status to processing
                cur.execute(
                    "UPDATE webhook_events SET status = 'processing', received_at = NOW() WHERE id = %s",
                    (existing_event["id"],)
                )
                conn.commit()
                webhook_event_id = existing_event["id"]
            else:
                webhook_event_id = existing_event["id"]
        else:
            # Create new webhook event record
            create_event = """
                INSERT INTO webhook_events (
                    whatsapp_message_id,
                    phone_number_id,
                    status,
                    received_at
                ) VALUES (%s, %s, 'processing', NOW())
                RETURNING id
            """
            cur.execute(create_event, (message_id, whatsapp_phone_id))
            webhook_event_id = cur.fetchone()["id"]
            conn.commit()

        # ============================================================
        # STEP 1B: FETCH CHATBOT + ORGANIZATION
        # ============================================================
        bot_query = """
            SELECT
                c.id,
                c.organization_id,
                c.name,
                c.system_prompt,
                c.persona,
                c.model_name,
                c.temperature,
                c.rag_enabled,
                c.whatsapp_access_token,
                c.is_active,
                c.fallback_message_error,
                c.fallback_message_limit,
                -- Organization info
                o.name as org_name,
                o.plan_tier,
                o.is_active as org_is_active,
                o.message_limit_monthly,
                o.token_limit_monthly,
                o.billing_period_start,
                o.billing_period_end
            FROM chatbots c
            JOIN organizations o ON c.organization_id = o.id
            WHERE c.whatsapp_phone_number_id = %s
        """
        cur.execute(bot_query, (whatsapp_phone_id,))
        bot = cur.fetchone()

        if not bot:
            print(f"No chatbot found for WhatsApp ID: {whatsapp_phone_id}")
            _mark_webhook_failed(cur, webhook_event_id, "Chatbot not found")
            conn.commit()
            return {
                "proceed": False,
                "reason": "Chatbot not found",
                "notify_admin": True,
            }

        # Check if org/chatbot is active
        if not bot["org_is_active"] or not bot["is_active"]:
            print(f"Chatbot or Organization is inactive")
            _mark_webhook_failed(cur, webhook_event_id, "Service inactive")
            conn.commit()
            return {
                "proceed": False,
                "reason": "Service Inactive",
                "notify_admin": True,
            }

        chatbot_id = bot["id"]
        org_id = bot["organization_id"]

        # Update webhook event with chatbot_id
        cur.execute(
            "UPDATE webhook_events SET chatbot_id = %s WHERE id = %s",
            (chatbot_id, webhook_event_id)
        )
        conn.commit()

        # ============================================================
        # STEP 1C: CHECK USAGE LIMITS
        # ============================================================
        usage_check_result = _check_usage_limits(
            cur, 
            org_id, 
            bot["message_limit_monthly"], 
            bot["token_limit_monthly"],
            bot["billing_period_start"],
            bot["billing_period_end"]
        )

        if not usage_check_result["has_quota"]:
            print(f"Organization {org_id} has exceeded usage limits")
            _mark_webhook_failed(cur, webhook_event_id, "Usage limit exceeded")
            conn.commit()
            
            return {
                "proceed": False,
                "reason": "Usage Limit Exceeded",
                "notify_admin": True,  # Send notification to org owner
                "usage_info": usage_check_result,
                "chatbot": {
                    "id": chatbot_id,
                    "organization_id": org_id,
                    "wa_token": bot["whatsapp_access_token"],
                },
            }

        # ============================================================
        # STEP 1D: UPSERT CONTACT
        # ============================================================
        contact_upsert = """
            INSERT INTO contacts (chatbot_id, phone_number, name, last_message_at)
            VALUES (%s, %s, %s, NOW())
            ON CONFLICT (chatbot_id, phone_number) 
            DO UPDATE SET 
                name = EXCLUDED.name, 
                last_message_at = NOW()
            RETURNING id, conversation_mode, variables, tags
        """
        cur.execute(contact_upsert, (chatbot_id, user_phone, user_name))
        contact = cur.fetchone()
        conn.commit()

        # Check for manual mode (human takeover)
        if contact["conversation_mode"] == "manual":
            print(f"User {user_phone} is in MANUAL mode")
            _mark_webhook_completed(cur, webhook_event_id)
            conn.commit()
            return {
                "proceed": False,
                "reason": "Manual Mode - Human Agent Required",
            }

        # ============================================================
        # STEP 1E: FETCH ACTIVE TOOLS/INTEGRATIONS
        # ============================================================
        tools_query = """
            SELECT 
                oi.id as integration_id,
                oi.provider, 
                oi.name, 
                oi.config, 
                oi.credentials,
                ci.settings_override,
                ci.is_enabled
            FROM chatbot_integrations ci
            JOIN org_integrations oi ON ci.integration_id = oi.id
            WHERE ci.chatbot_id = %s 
              AND ci.is_enabled = TRUE
              AND oi.is_active = TRUE
        """
        cur.execute(tools_query, (chatbot_id,))
        tools_rows = cur.fetchall()

        active_tools = []
        for t in tools_rows:
            base_config = t["config"] or {}
            override = t["settings_override"] or {}
            merged_config = {**base_config, **override}

            active_tools.append(
                {
                    "integration_id": t["integration_id"],
                    "provider": t["provider"],
                    "name": t["name"],
                    "config": merged_config,
                    "credentials": t["credentials"],
                }
            )

        # ============================================================
        # STEP 1F: FETCH CHAT HISTORY
        # ============================================================
        history_query = """
            SELECT role, content, tool_calls, tool_results, created_at
            FROM messages 
            WHERE contact_id = %s 
            ORDER BY created_at DESC 
            LIMIT 20
        """
        cur.execute(history_query, (contact["id"],))
        history_rows = cur.fetchall()

        # Reverse to chronological order
        history = [dict(row) for row in reversed(history_rows)]

        # ============================================================
        # RETURN CONTEXT PAYLOAD
        # ============================================================
        return {
            "proceed": True,
            "webhook_event_id": webhook_event_id,
            "chatbot": {
                "id": chatbot_id,
                "organization_id": org_id,
                "name": bot["name"],
                "system_prompt": bot["system_prompt"],
                "persona": bot["persona"],
                "model_name": bot["model_name"],
                "temperature": float(bot["temperature"]) if bot["temperature"] else 0.7,
                "wa_token": bot["whatsapp_access_token"],
                "rag_config": {
                    "enabled": bot["rag_enabled"],
                },
            },
            "user": {
                "id": contact["id"],
                "phone": user_phone,
                "name": user_name,
                "variables": contact["variables"] or {},
                "tags": contact["tags"] or [],
            },
            "history": history,
            "tools": active_tools,
            "usage_info": usage_check_result,
        }

    except Exception as e:
        print(f"Error in context loading: {e}")
        if conn:
            conn.rollback()
        raise e
    finally:
        if "cur" in locals():
            cur.close()
        if "conn" in locals():
            conn.close()


def _check_usage_limits(
    cur,
    org_id: str,
    message_limit: int,
    token_limit: int,
    period_start,
    period_end
) -> Dict[str, Any]:
    """
    Check if organization has remaining quota.
    
    Returns:
        {
            "has_quota": bool,
            "messages_used": int,
            "tokens_used": int,
            "messages_remaining": int,
            "tokens_remaining": int,
            "limit_type": str or None  # "messages" or "tokens" if exceeded
        }
    """
    
    # Get current usage using the database function
    usage_query = "SELECT * FROM get_current_usage(%s)"
    cur.execute(usage_query, (org_id,))
    usage = cur.fetchone()
    
    messages_used = usage["messages_used"]
    tokens_used = usage["tokens_used"]
    
    messages_remaining = message_limit - messages_used
    tokens_remaining = token_limit - tokens_used
    
    # Check if limits exceeded
    has_quota = True
    limit_type = None
    
    if messages_used >= message_limit:
        has_quota = False
        limit_type = "messages"
    elif tokens_used >= token_limit:
        has_quota = False
        limit_type = "tokens"
    
    return {
        "has_quota": has_quota,
        "messages_used": messages_used,
        "tokens_used": tokens_used,
        "messages_remaining": max(0, messages_remaining),
        "tokens_remaining": max(0, tokens_remaining),
        "message_limit": message_limit,
        "token_limit": token_limit,
        "limit_type": limit_type,
        "period_start": str(period_start),
        "period_end": str(period_end),
    }


def _mark_webhook_failed(cur, webhook_event_id: int, error_message: str):
    """Mark webhook event as failed"""
    cur.execute(
        """
        UPDATE webhook_events 
        SET status = 'failed', 
            error_message = %s,
            processed_at = NOW()
        WHERE id = %s
        """,
        (error_message, webhook_event_id)
    )


def _mark_webhook_completed(cur, webhook_event_id: int):
    """Mark webhook event as completed"""
    cur.execute(
        """
        UPDATE webhook_events 
        SET status = 'completed',
            processed_at = NOW()
        WHERE id = %s
        """,
        (webhook_event_id,)
    )

================================================
File: f/development/ingest_multiple_urls.script.lock
================================================
# py: 3.12
anyio==4.12.0
certifi==2025.11.12
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.11
psycopg2-binary==2.9.11
typing-extensions==4.15.0
wmill==1.601.1

================================================
File: f/development/utils/alert_on_failure.script.lock
================================================
# py: 3.12
anyio==4.12.0
certifi==2025.11.12
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.11
psycopg2-binary==2.9.11
typing-extensions==4.15.0
wmill==1.601.1

================================================
File: tests/unit/test_step2_llm_processing.py
================================================
"""
Unit tests for Step 2: LLM Processing

Tests Step 2's ability to:
- Call LLM providers (OpenAI, Google) without tools
- Format messages and system prompts correctly
- Handle RAG context injection
- Track token usage
- Return proper response structure
"""

import pytest
import sys
import os
from unittest.mock import Mock, patch, MagicMock
from collections import namedtuple

# Add parent directories to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../f/development'))

# Mock required modules before importing step2
mock_wmill = Mock()
mock_wmill.get_variable.return_value = "fake_google_api_key"
sys.modules['wmill'] = mock_wmill

# Mock Google GenAI SDK
mock_genai = Mock()
mock_genai_types = Mock()
sys.modules['google.genai'] = mock_genai
sys.modules['google.genai.types'] = mock_genai_types

# Import the module under test
import importlib.util
spec = importlib.util.spec_from_file_location(
    "step2",
    os.path.join(os.path.dirname(__file__), '../../f/development/2_whatsapp_llm_processing.py')
)
step2_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(step2_module)
step2_main = step2_module.main

# Simple class to hold usage metadata
UsageMetadata = namedtuple('UsageMetadata', ['prompt_token_count', 'candidates_token_count'])
OpenAIUsage = namedtuple('OpenAIUsage', ['prompt_tokens', 'completion_tokens'])


class TestStep2LLMProcessing:
    """Test Step 2's LLM processing functionality"""

    def test_simple_gemini_response_no_tools(self):
        """Test simple Gemini response without tools or RAG"""
        # Setup mock Gemini client
        mock_client = Mock()
        mock_models = Mock()

        # Create mock response
        mock_response = Mock()
        mock_response.text = "Hello! How can I help you today?"
        mock_response.usage_metadata = UsageMetadata(
            prompt_token_count=50,
            candidates_token_count=20
        )

        mock_models.generate_content = Mock(return_value=mock_response)
        mock_client.models = mock_models

        with patch.object(mock_genai, 'Client', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "chatbot-123",
                        "organization_id": "org-456",
                        "model_name": "gemini-pro",
                        "system_prompt": "You are a helpful assistant.",
                        "persona": "Friendly and professional",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}
                    },
                    "user": {
                        "id": "user-789",
                        "phone": "1234567890",
                        "name": "Test User",
                        "variables": {}
                    },
                    "history": [],
                    "tools": []
                },
                user_message="Hello",
                google_api_key="fake_key",
                default_provider="google"
            )

            # Assertions
            assert "error" not in result
            assert result["reply_text"] == "Hello! How can I help you today?"
            assert result["usage_info"]["provider"] == "google"
            assert result["usage_info"]["model"] == "gemini-pro"
            assert result["usage_info"]["tokens_input"] == 50
            assert result["usage_info"]["tokens_output"] == 20
            assert result["usage_info"]["rag_used"] is False
            assert result["usage_info"]["chunks_retrieved"] == 0
            assert len(result["tool_executions"]) == 0

    def test_simple_openai_response_no_tools(self):
        """Test simple OpenAI response without tools or RAG"""
        # Setup mock OpenAI client
        mock_client = Mock()

        # Create mock response
        mock_message = Mock()
        mock_message.content = "I'd be happy to help!"

        mock_choice = Mock()
        mock_choice.message = mock_message

        mock_response = Mock()
        mock_response.choices = [mock_choice]
        mock_response.usage = OpenAIUsage(
            prompt_tokens=100,
            completion_tokens=40
        )

        mock_client.chat = Mock()
        mock_client.chat.completions = Mock()
        mock_client.chat.completions.create = Mock(return_value=mock_response)

        # Patch OpenAI in the step2_module namespace where it's imported
        with patch.object(step2_module, 'OpenAI', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "chatbot-123",
                        "organization_id": "org-456",
                        "model_name": "gpt-4o",
                        "system_prompt": "You are a helpful assistant.",
                        "persona": "Concise and clear",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}
                    },
                    "user": {
                        "id": "user-789",
                        "phone": "1234567890",
                        "name": "Test User",
                        "variables": {}
                    },
                    "history": [],
                    "tools": []
                },
                user_message="Hello",
                openai_api_key="fake_openai_key",
                default_provider="openai"
            )

            # Assertions
            assert "error" not in result
            assert result["reply_text"] == "I'd be happy to help!"
            assert result["usage_info"]["provider"] == "openai"
            assert result["usage_info"]["model"] == "gpt-4o"
            assert result["usage_info"]["tokens_input"] == 100
            assert result["usage_info"]["tokens_output"] == 40
            assert result["usage_info"]["rag_used"] is False

    def test_provider_detection_from_model_name(self):
        """Test that provider is correctly detected from model_name"""
        # Test Gemini detection
        mock_client = Mock()
        mock_models = Mock()
        mock_response = Mock()
        mock_response.text = "Response"
        mock_response.usage_metadata = UsageMetadata(50, 20)
        mock_models.generate_content = Mock(return_value=mock_response)
        mock_client.models = mock_models

        with patch.object(mock_genai, 'Client', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test",
                        "model_name": "gemini-3-flash-preview",  # Contains "gemini"
                        "system_prompt": "Test",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [],
                    "tools": []
                },
                user_message="Test",
                google_api_key="fake_key",
                default_provider="openai"  # Default is openai, but should switch to google
            )

            assert result["usage_info"]["provider"] == "google"

    def test_conversation_history_formatting(self):
        """Test that conversation history is properly formatted for LLM"""
        mock_client = Mock()
        mock_models = Mock()
        mock_response = Mock()
        mock_response.text = "Response with history"
        mock_response.usage_metadata = UsageMetadata(150, 30)
        mock_models.generate_content = Mock(return_value=mock_response)
        mock_client.models = mock_models

        with patch.object(mock_genai, 'Client', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test",
                        "model_name": "gemini-pro",
                        "system_prompt": "You are helpful",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [
                        {"role": "user", "content": "Previous question"},
                        {"role": "assistant", "content": "Previous answer"}
                    ],
                    "tools": []
                },
                user_message="Follow-up question",
                google_api_key="fake_key"
            )

            # Verify generate_content was called
            assert mock_models.generate_content.called

            # The call should include conversation history
            call_args = mock_models.generate_content.call_args
            assert call_args is not None

    def test_user_context_injection(self):
        """Test that user context (name, phone, variables) is injected into prompt"""
        mock_client = Mock()
        mock_models = Mock()
        mock_response = Mock()
        mock_response.text = "Response"
        mock_response.usage_metadata = UsageMetadata(50, 20)
        mock_models.generate_content = Mock(return_value=mock_response)
        mock_client.models = mock_models

        with patch.object(mock_genai, 'Client', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test",
                        "model_name": "gemini-pro",
                        "system_prompt": "You are helpful",
                        "persona": "Professional",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}
                    },
                    "user": {
                        "id": "user-123",
                        "phone": "5551234567",
                        "name": "John Doe",
                        "variables": {"preferred_language": "Spanish", "tier": "premium"}
                    },
                    "history": [],
                    "tools": []
                },
                user_message="Hello",
                google_api_key="fake_key"
            )

            assert result["reply_text"] == "Response"
            # User context should be included in the system prompt
            # We can verify this by checking the generate_content call

    def test_rag_context_injection(self):
        """Test that RAG context is properly injected when enabled"""
        # Mock OpenAI for embeddings
        mock_openai_client = Mock()
        mock_embedding_response = Mock()
        mock_embedding_response.data = [Mock(embedding=[0.1] * 1536)]
        mock_openai_client.embeddings = Mock()
        mock_openai_client.embeddings.create = Mock(return_value=mock_embedding_response)

        # Mock database connection for RAG retrieval
        mock_conn = Mock()
        mock_cursor = Mock()
        mock_cursor.fetchall = Mock(return_value=[
            {
                "content": "Relevant information from knowledge base",
                "source_name": "Product Manual",
                "similarity": 0.85,
                "metadata": {"page": 5}
            }
        ])
        mock_conn.cursor = Mock(return_value=mock_cursor)

        # Mock Gemini response - simple response without tools (no agent loop)
        mock_gemini_client = Mock()
        mock_models = Mock()
        mock_response = Mock()
        mock_response.text = "Based on the knowledge base, here's the answer..."
        mock_response.usage_metadata = UsageMetadata(200, 50)
        mock_models.generate_content = Mock(return_value=mock_response)
        mock_gemini_client.models = mock_models

        # Patch OpenAI in the step2_module namespace
        with patch.object(step2_module, 'OpenAI', return_value=mock_openai_client), \
             patch.object(step2_module, 'psycopg2') as mock_psycopg2, \
             patch.object(mock_genai, 'Client', return_value=mock_gemini_client):

            # Configure psycopg2 mock
            mock_psycopg2.connect.return_value = mock_conn

            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "chatbot-123",
                        "model_name": "gemini-pro",
                        "system_prompt": "You are helpful",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": True}  # RAG enabled!
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [],
                    "tools": []
                },
                user_message="How do I use the product?",
                openai_api_key="fake_openai_key",  # For embeddings
                google_api_key="fake_google_key",
                db_resource="f/development/business_layer_db_postgreSQL"
            )

            # Assertions
            assert result["usage_info"]["rag_used"] is True
            assert result["usage_info"]["chunks_retrieved"] == 1
            assert len(result["retrieved_sources"]) == 1
            assert result["retrieved_sources"][0]["source_name"] == "Product Manual"
            assert result["retrieved_sources"][0]["similarity"] == 0.85

    def test_llm_error_handling(self):
        """Test LLM error handling with fallback messages"""
        mock_client = Mock()
        mock_models = Mock()

        # Simulate LLM error
        mock_models.generate_content = Mock(side_effect=Exception("API Error"))
        mock_client.models = mock_models

        with patch.object(mock_genai, 'Client', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test",
                        "model_name": "gemini-pro",
                        "system_prompt": "Test",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False},
                        "fallback_message_error": "Custom error message",
                        "fallback_message_limit": "Custom limit message"
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [],
                    "tools": []
                },
                user_message="Test",
                google_api_key="fake_key"
            )

            # Should return fallback message
            assert result["reply_text"] == "Custom error message"
            assert result["usage_info"]["error"] == "API Error"
            assert result["usage_info"]["is_limit_error"] is False

    def test_quota_limit_error_handling(self):
        """Test quota/limit error detection and appropriate fallback message"""
        mock_client = Mock()
        mock_models = Mock()

        # Simulate quota error
        mock_models.generate_content = Mock(side_effect=Exception("429 Quota exceeded"))
        mock_client.models = mock_models

        with patch.object(mock_genai, 'Client', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test",
                        "model_name": "gemini-pro",
                        "system_prompt": "Test",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False},
                        "fallback_message_error": "Error message",
                        "fallback_message_limit": "Quota exceeded message"
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [],
                    "tools": []
                },
                user_message="Test",
                google_api_key="fake_key"
            )

            # Should return limit fallback message
            assert result["reply_text"] == "Quota exceeded message"
            assert result["usage_info"]["is_limit_error"] is True

    def test_missing_api_key_error(self):
        """Test handling when API key is missing"""
        result = step2_main(
            context_payload={
                "proceed": True,
                "chatbot": {
                    "id": "test",
                    "model_name": "gpt-4",
                    "system_prompt": "Test",
                    "persona": "",
                    "temperature": 0.7,
                    "rag_config": {"enabled": False}
                },
                "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                "history": [],
                "tools": []
            },
            user_message="Test",
            openai_api_key="",  # Missing API key
            default_provider="openai"
        )

        assert result["error"] == "Missing OpenAI API Key"

    def test_empty_history_handling(self):
        """Test that empty history is handled correctly"""
        mock_client = Mock()
        mock_models = Mock()
        mock_response = Mock()
        mock_response.text = "First message response"
        mock_response.usage_metadata = UsageMetadata(50, 20)
        mock_models.generate_content = Mock(return_value=mock_response)
        mock_client.models = mock_models

        with patch.object(mock_genai, 'Client', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test",
                        "model_name": "gemini-pro",
                        "system_prompt": "Test",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [],  # Empty history
                    "tools": []
                },
                user_message="First message",
                google_api_key="fake_key"
            )

            assert "error" not in result
            assert result["reply_text"] == "First message response"

    def test_rag_disabled_no_retrieval(self):
        """Test that RAG retrieval is skipped when disabled"""
        mock_client = Mock()
        mock_models = Mock()
        mock_response = Mock()
        mock_response.text = "Response without RAG"
        mock_response.usage_metadata = UsageMetadata(50, 20)
        mock_models.generate_content = Mock(return_value=mock_response)
        mock_client.models = mock_models

        with patch.object(mock_genai, 'Client', return_value=mock_client), \
             patch.object(step2_module, 'OpenAI') as mock_openai:

            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test",
                        "model_name": "gemini-pro",
                        "system_prompt": "Test",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}  # RAG disabled
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [],
                    "tools": []
                },
                user_message="Question",
                openai_api_key="fake_key",
                google_api_key="fake_key"
            )

            # OpenAI should not be called for embeddings when RAG is disabled
            assert not mock_openai.called
            assert result["usage_info"]["rag_used"] is False
            assert result["usage_info"]["chunks_retrieved"] == 0
            assert len(result["retrieved_sources"]) == 0

    def test_openai_history_with_content(self):
        """
        GOAL: Test that OpenAI history includes only messages with content
        GIVEN: History with messages that have and don't have content
        WHEN: main is called with OpenAI
        THEN: Only messages with content are added to conversation
        """
        mock_client = Mock()
        mock_message = Mock()
        mock_message.content = "Response"
        mock_choice = Mock()
        mock_choice.message = mock_message
        mock_response = Mock()
        mock_response.choices = [mock_choice]
        mock_response.usage = OpenAIUsage(100, 40)
        mock_client.chat = Mock()
        mock_client.chat.completions = Mock()
        mock_client.chat.completions.create = Mock(return_value=mock_response)

        with patch.object(step2_module, 'OpenAI', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test",
                        "model_name": "gpt-4o",
                        "system_prompt": "Test",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [
                        {"role": "user", "content": "First message"},
                        {"role": "assistant", "content": ""},  # Empty content - should be skipped
                        {"role": "user", "content": "Second message"}
                    ],
                    "tools": []
                },
                user_message="Current message",
                openai_api_key="fake_key"
            )

            # Verify the call was made
            assert mock_client.chat.completions.create.called
            call_args = mock_client.chat.completions.create.call_args
            messages = call_args.kwargs['messages']

            # Should have: system + 2 history messages (skipping empty) + current user message
            assert len(messages) == 4
            assert messages[0]['role'] == 'system'
            assert messages[1]['content'] == 'First message'
            assert messages[2]['content'] == 'Second message'
            assert messages[3]['content'] == 'Current message'

    def test_missing_google_api_key(self):
        """
        GOAL: Test handling when Google API key is missing
        GIVEN: Google provider with no API key
        WHEN: main is called
        THEN: Returns error for missing API key
        """
        result = step2_main(
            context_payload={
                "proceed": True,
                "chatbot": {
                    "id": "test",
                    "model_name": "gemini-pro",
                    "system_prompt": "Test",
                    "persona": "",
                    "temperature": 0.7,
                    "rag_config": {"enabled": False}
                },
                "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                "history": [],
                "tools": []
            },
            user_message="Test",
            google_api_key="",  # Missing
            default_provider="google"
        )

        assert result["error"] == "Missing Google API Key"

    def test_google_no_usage_metadata_fallback(self):
        """
        GOAL: Test Google token estimation fallback when usage_metadata is missing
        GIVEN: Gemini response without usage_metadata
        WHEN: main processes the response
        THEN: Uses estimate_tokens fallback
        """
        mock_client = Mock()
        mock_models = Mock()
        mock_response = Mock()
        mock_response.text = "Response text"
        # No usage_metadata attribute
        del mock_response.usage_metadata

        mock_models.generate_content = Mock(return_value=mock_response)
        mock_client.models = mock_models

        with patch.object(mock_genai, 'Client', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test",
                        "model_name": "gemini-pro",
                        "system_prompt": "Test",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [],
                    "tools": []
                },
                user_message="Test",
                google_api_key="fake_key"
            )

            # Should use estimate_tokens
            assert result["usage_info"]["tokens_input"] > 0
            assert result["usage_info"]["tokens_output"] > 0

    def test_unknown_provider_error(self):
        """
        GOAL: Test error handling for unknown provider
        GIVEN: Invalid provider name
        WHEN: main is called
        THEN: Returns error for unknown provider
        """
        result = step2_main(
            context_payload={
                "proceed": True,
                "chatbot": {
                    "id": "test",
                    "model_name": "unknown-model",
                    "system_prompt": "Test",
                    "persona": "",
                    "temperature": 0.7,
                    "rag_config": {"enabled": False}
                },
                "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                "history": [],
                "tools": []
            },
            user_message="Test",
            default_provider="unknown_provider"
        )

        assert "error" in result
        assert "unknown_provider" in result["error"].lower()


class TestAgentLoop:
    """Test agent loop functionality for both OpenAI and Gemini"""

    def test_openai_agent_loop_with_tool_calls(self):
        """
        GOAL: Test OpenAI agent loop executes tools and returns response
        GIVEN: OpenAI client that returns tool calls then final response
        WHEN: execute_agent_loop_openai is called
        THEN: Tools are executed and final response is returned
        """
        mock_client = Mock()

        # First response: tool call
        mock_tool_call = Mock()
        mock_tool_call.id = "call_123"
        mock_tool_call.function.name = "search_knowledge_base"
        mock_tool_call.function.arguments = '{"query": "test query"}'

        mock_message_1 = Mock()
        mock_message_1.tool_calls = [mock_tool_call]
        mock_choice_1 = Mock()
        mock_choice_1.finish_reason = "tool_calls"
        mock_choice_1.message = mock_message_1
        mock_response_1 = Mock()
        mock_response_1.choices = [mock_choice_1]
        mock_response_1.usage = OpenAIUsage(100, 20)

        # Second response: final answer
        mock_message_2 = Mock()
        mock_message_2.content = "Based on the search results, here is the answer."
        mock_choice_2 = Mock()
        mock_choice_2.finish_reason = "stop"
        mock_choice_2.message = mock_message_2
        mock_response_2 = Mock()
        mock_response_2.choices = [mock_choice_2]
        mock_response_2.usage = OpenAIUsage(120, 30)

        mock_client.chat.completions.create = Mock(side_effect=[mock_response_1, mock_response_2])

        # Mock RAG search
        with patch.object(step2_module, 'execute_tool') as mock_execute_tool:
            mock_execute_tool.return_value = {"success": True, "results": [{"content": "info"}]}

            result = step2_module.execute_agent_loop_openai(
                client=mock_client,
                model_name="gpt-4o",
                messages=[{"role": "system", "content": "You are helpful"}],
                tools=[{
                    "type": "function",
                    "function": {
                        "name": "search_knowledge_base",
                        "description": "Search knowledge base"
                    }
                }],
                chatbot_id="chatbot-123",
                temperature=0.7,
                openai_api_key="fake_key",
                db_resource="f/development/db",
                max_iterations=5
            )

            # Assertions
            assert result["reply_text"] == "Based on the search results, here is the answer."
            assert len(result["tool_executions"]) == 1
            assert result["tool_executions"][0]["tool_name"] == "search_knowledge_base"
            assert result["tool_executions"][0]["status"] == "success"
            assert result["usage_info"]["tokens_input"] == 220  # 100 + 120
            assert result["usage_info"]["tokens_output"] == 50  # 20 + 30
            assert result["usage_info"]["iterations"] == 2

    def test_openai_agent_loop_multiple_tool_calls_in_one_response(self):
        """
        GOAL: Test OpenAI agent handles multiple tool calls in single response
        GIVEN: OpenAI response with multiple tool calls
        WHEN: execute_agent_loop_openai processes them
        THEN: All tools are executed and results fed back
        """
        mock_client = Mock()

        # Response with 2 tool calls
        mock_tool_call_1 = Mock()
        mock_tool_call_1.id = "call_1"
        mock_tool_call_1.function.name = "search_knowledge_base"
        mock_tool_call_1.function.arguments = '{"query": "query1"}'

        mock_tool_call_2 = Mock()
        mock_tool_call_2.id = "call_2"
        mock_tool_call_2.function.name = "search_knowledge_base"
        mock_tool_call_2.function.arguments = '{"query": "query2"}'

        mock_message_1 = Mock()
        mock_message_1.tool_calls = [mock_tool_call_1, mock_tool_call_2]
        mock_choice_1 = Mock()
        mock_choice_1.finish_reason = "tool_calls"
        mock_choice_1.message = mock_message_1
        mock_response_1 = Mock()
        mock_response_1.choices = [mock_choice_1]
        mock_response_1.usage = OpenAIUsage(100, 20)

        # Final response
        mock_message_2 = Mock()
        mock_message_2.content = "Combined answer from both searches"
        mock_choice_2 = Mock()
        mock_choice_2.finish_reason = "stop"
        mock_choice_2.message = mock_message_2
        mock_response_2 = Mock()
        mock_response_2.choices = [mock_choice_2]
        mock_response_2.usage = OpenAIUsage(150, 40)

        mock_client.chat.completions.create = Mock(side_effect=[mock_response_1, mock_response_2])

        with patch.object(step2_module, 'execute_tool') as mock_execute_tool:
            mock_execute_tool.return_value = {"success": True, "results": []}

            result = step2_module.execute_agent_loop_openai(
                client=mock_client,
                model_name="gpt-4o",
                messages=[{"role": "user", "content": "Test"}],
                tools=[{"type": "function", "function": {"name": "search_knowledge_base"}}],
                chatbot_id="chatbot-123",
                temperature=0.7,
                openai_api_key="fake_key",
                db_resource="f/development/db",
                max_iterations=5
            )

            # Should have executed both tools
            assert len(result["tool_executions"]) == 2
            assert mock_execute_tool.call_count == 2

    def test_openai_agent_loop_max_iterations(self):
        """
        GOAL: Test OpenAI agent loop stops at max iterations
        GIVEN: Agent that keeps requesting tools
        WHEN: Max iterations is reached
        THEN: Returns max iterations message
        """
        mock_client = Mock()

        # Always return tool calls
        mock_tool_call = Mock()
        mock_tool_call.id = "call_123"
        mock_tool_call.function.name = "search_knowledge_base"
        mock_tool_call.function.arguments = '{"query": "test"}'

        mock_message = Mock()
        mock_message.tool_calls = [mock_tool_call]
        mock_choice = Mock()
        mock_choice.finish_reason = "tool_calls"
        mock_choice.message = mock_message
        mock_response = Mock()
        mock_response.choices = [mock_choice]
        mock_response.usage = OpenAIUsage(100, 20)

        mock_client.chat.completions.create = Mock(return_value=mock_response)

        with patch.object(step2_module, 'execute_tool') as mock_execute_tool:
            mock_execute_tool.return_value = {"success": True, "results": []}

            result = step2_module.execute_agent_loop_openai(
                client=mock_client,
                model_name="gpt-4o",
                messages=[{"role": "user", "content": "Test"}],
                tools=[{"type": "function", "function": {"name": "search_knowledge_base"}}],
                chatbot_id="chatbot-123",
                temperature=0.7,
                openai_api_key="fake_key",
                db_resource="f/development/db",
                max_iterations=3  # Low limit
            )

            # Should hit max iterations
            assert "need more time" in result["reply_text"].lower() or "rephrase" in result["reply_text"].lower()
            assert result["usage_info"]["max_iterations_reached"] is True
            assert result["usage_info"]["iterations"] == 3

    def test_openai_agent_loop_unexpected_finish_reason(self):
        """
        GOAL: Test handling of unexpected finish_reason
        GIVEN: OpenAI returns unexpected finish_reason
        WHEN: execute_agent_loop_openai processes it
        THEN: Returns appropriate error message
        """
        mock_client = Mock()

        mock_message = Mock()
        mock_message.content = "Partial response"
        mock_choice = Mock()
        mock_choice.finish_reason = "length"  # Unexpected
        mock_choice.message = mock_message
        mock_response = Mock()
        mock_response.choices = [mock_choice]
        mock_response.usage = OpenAIUsage(100, 20)

        mock_client.chat.completions.create = Mock(return_value=mock_response)

        result = step2_module.execute_agent_loop_openai(
            client=mock_client,
            model_name="gpt-4o",
            messages=[{"role": "user", "content": "Test"}],
            tools=[],
            chatbot_id="chatbot-123",
            temperature=0.7,
            openai_api_key="fake_key",
            db_resource="f/development/db",
            max_iterations=5
        )

        # Should handle unexpected finish reason
        assert result["usage_info"]["finish_reason"] == "length"
        assert result["reply_text"] is not None

    def test_openai_agent_loop_exception_handling(self):
        """
        GOAL: Test OpenAI agent loop handles exceptions gracefully
        GIVEN: OpenAI client that raises exception
        WHEN: execute_agent_loop_openai is called
        THEN: Returns error message with exception info
        """
        mock_client = Mock()
        mock_client.chat.completions.create = Mock(side_effect=Exception("API Error"))

        result = step2_module.execute_agent_loop_openai(
            client=mock_client,
            model_name="gpt-4o",
            messages=[{"role": "user", "content": "Test"}],
            tools=[],
            chatbot_id="chatbot-123",
            temperature=0.7,
            openai_api_key="fake_key",
            db_resource="f/development/db",
            max_iterations=5
        )

        assert "error" in result["reply_text"].lower()
        assert result["usage_info"]["error"] == "API Error"
        assert result["usage_info"]["iterations"] == 1

    def test_openai_agent_loop_via_main(self):
        """
        GOAL: Test OpenAI agent loop is invoked via main when tools are present
        GIVEN: OpenAI chatbot with tools configured
        WHEN: main is called
        THEN: Agent loop is used and tools are available
        """
        mock_client = Mock()

        # Mock agent loop response
        mock_message = Mock()
        mock_message.content = "Response using tools"
        mock_choice = Mock()
        mock_choice.finish_reason = "stop"
        mock_choice.message = mock_message
        mock_response = Mock()
        mock_response.choices = [mock_choice]
        mock_response.usage = OpenAIUsage(100, 30)

        mock_client.chat.completions.create = Mock(return_value=mock_response)

        with patch.object(step2_module, 'OpenAI', return_value=mock_client):
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "chatbot-123",
                        "model_name": "gpt-4o",
                        "system_prompt": "Test",
                        "persona": "",
                        "temperature": 0.7,
                        "rag_config": {"enabled": False}
                    },
                    "user": {"id": "test", "phone": "123", "name": "Test", "variables": {}},
                    "history": [],
                    "tools": [
                        {
                            "enabled": True,
                            "provider": "mcp",
                            "name": "test_tool",
                            "config": {
                                "description": "Test tool",
                                "parameters": {}
                            }
                        }
                    ]
                },
                user_message="Test",
                openai_api_key="fake_key"
            )

            assert "error" not in result
            assert result["reply_text"] == "Response using tools"

    def test_gemini_agent_loop_max_iterations(self):
        """
        GOAL: Test Gemini agent loop stops at max iterations
        GIVEN: Gemini that keeps requesting function calls
        WHEN: Max iterations is reached
        THEN: Returns max iterations message
        """
        mock_client = Mock()
        mock_models = Mock()

        # Create function call part
        mock_fc = Mock()
        mock_fc.name = "search_knowledge_base"
        mock_fc.args = {"query": "test"}

        mock_part = Mock()
        mock_part.function_call = mock_fc

        mock_candidate = Mock()
        mock_candidate.content = Mock()
        mock_candidate.content.parts = [mock_part]

        mock_response = Mock()
        mock_response.candidates = [mock_candidate]
        mock_response.usage_metadata = UsageMetadata(100, 20)

        mock_models.generate_content = Mock(return_value=mock_response)
        mock_client.models = mock_models

        with patch.object(step2_module, 'execute_tool') as mock_execute_tool:
            mock_execute_tool.return_value = {"success": True}

            result = step2_module.execute_agent_loop_gemini(
                client=mock_client,
                model_name="gemini-pro",
                system_prompt="Test",
                user_message="Test",
                chat_history=[],
                tools=[{"function": {"name": "search_knowledge_base"}}],
                chatbot_id="chatbot-123",
                temperature=0.7,
                google_api_key="fake_key",
                db_resource="f/development/db",
                fallback_message_error="Error",
                fallback_message_limit="Limit",
                max_iterations=2
            )

            # Should hit max iterations
            assert "reformular" in result["reply_text"].lower() or "información" in result["reply_text"].lower()
            assert result["usage_info"]["max_iterations_reached"] is True


class TestToolExecution:
    """Test tool execution functionality"""

    def test_execute_tool_search_knowledge_base(self):
        """
        GOAL: Test built-in search_knowledge_base tool execution
        GIVEN: Tool call to search_knowledge_base
        WHEN: execute_tool is called
        THEN: RAG search is executed
        """
        with patch.object(step2_module, 'execute_rag_search') as mock_rag_search:
            mock_rag_search.return_value = {"success": True, "results": []}

            result = step2_module.execute_tool(
                tool_name="search_knowledge_base",
                arguments={"query": "test query"},
                tools=[],  # Not in tools list - it's built-in
                chatbot_id="chatbot-123",
                openai_api_key="fake_key",
                db_resource="f/development/db"
            )

            assert result["success"] is True
            mock_rag_search.assert_called_once_with(
                chatbot_id="chatbot-123",
                query="test query",
                openai_api_key="fake_key",
                db_resource="f/development/db"
            )

    def test_execute_tool_not_found(self):
        """
        GOAL: Test error when tool is not found
        GIVEN: Invalid tool name
        WHEN: execute_tool is called
        THEN: Returns tool not found error
        """
        result = step2_module.execute_tool(
            tool_name="nonexistent_tool",
            arguments={},
            tools=[],
            chatbot_id="chatbot-123",
            openai_api_key="fake_key",
            db_resource="f/development/db"
        )

        assert "error" in result
        assert "not found" in result["error"].lower()

    def test_execute_tool_mcp(self):
        """
        GOAL: Test MCP tool execution
        GIVEN: MCP tool definition
        WHEN: execute_tool is called
        THEN: MCP tool is executed via HTTP
        """
        tools = [{
            "type": "function",
            "function": {"name": "calculate_price"},
            "_metadata": {
                "tool_type": "mcp",
                "mcp_server_url": "http://mcp-server:3001"
            }
        }]

        with patch.object(step2_module, 'execute_mcp_tool') as mock_mcp:
            mock_mcp.return_value = {"result": "calculated"}

            result = step2_module.execute_tool(
                tool_name="calculate_price",
                arguments={"amount": 100},
                tools=tools,
                chatbot_id="chatbot-123",
                openai_api_key="fake_key",
                db_resource="f/development/db"
            )

            assert result["result"] == "calculated"
            mock_mcp.assert_called_once()

    def test_execute_tool_windmill(self):
        """
        GOAL: Test Windmill tool execution
        GIVEN: Windmill tool definition
        WHEN: execute_tool is called
        THEN: Windmill script is executed
        """
        tools = [{
            "type": "function",
            "function": {"name": "process_data"},
            "_metadata": {
                "tool_type": "windmill",
                "script_path": "f/scripts/process"
            }
        }]

        with patch.object(step2_module, 'execute_windmill_tool') as mock_windmill:
            mock_windmill.return_value = {"success": True, "data": "processed"}

            result = step2_module.execute_tool(
                tool_name="process_data",
                arguments={"input": "data"},
                tools=tools,
                chatbot_id="chatbot-123",
                openai_api_key="fake_key",
                db_resource="f/development/db"
            )

            assert result["success"] is True
            mock_windmill.assert_called_once()

    def test_execute_tool_unknown_type(self):
        """
        GOAL: Test error for unknown tool type
        GIVEN: Tool with unknown type
        WHEN: execute_tool is called
        THEN: Returns unknown tool type error
        """
        tools = [{
            "type": "function",
            "function": {"name": "test_tool"},
            "_metadata": {"tool_type": "unknown"}
        }]

        result = step2_module.execute_tool(
            tool_name="test_tool",
            arguments={},
            tools=tools,
            chatbot_id="chatbot-123",
            openai_api_key="fake_key",
            db_resource="f/development/db"
        )

        assert "error" in result
        assert "unknown tool type" in result["error"].lower()

    def test_execute_tool_exception(self):
        """
        GOAL: Test exception handling in tool execution
        GIVEN: Tool that raises exception
        WHEN: execute_tool is called
        THEN: Returns error message
        """
        tools = [{
            "type": "function",
            "function": {"name": "failing_tool"},
            "_metadata": {"tool_type": "mcp", "mcp_server_url": "http://server"}
        }]

        with patch.object(step2_module, 'execute_mcp_tool') as mock_mcp:
            mock_mcp.side_effect = Exception("Tool failed")

            result = step2_module.execute_tool(
                tool_name="failing_tool",
                arguments={},
                tools=tools,
                chatbot_id="chatbot-123",
                openai_api_key="fake_key",
                db_resource="f/development/db"
            )

            assert "error" in result
            assert "Tool failed" in result["error"]


class TestRAGSearch:
    """Test RAG search execution"""

    def test_execute_rag_search_success(self):
        """
        GOAL: Test successful RAG search execution
        GIVEN: Valid search parameters
        WHEN: execute_rag_search is called
        THEN: Returns formatted search results
        """
        with patch.object(step2_module, 'retrieve_knowledge') as mock_retrieve:
            mock_retrieve.return_value = [
                {
                    "content": "Information about product",
                    "source_name": "Manual",
                    "similarity": 0.9,
                    "metadata": {"page": 10}
                }
            ]

            result = step2_module.execute_rag_search(
                chatbot_id="chatbot-123",
                query="product info",
                openai_api_key="fake_key",
                db_resource="f/development/db"
            )

            assert result["success"] is True
            assert len(result["results"]) == 1
            assert result["results"][0]["content"] == "Information about product"
            assert result["results"][0]["relevance"] == "90%"
            assert result["count"] == 1

    def test_execute_rag_search_no_results(self):
        """
        GOAL: Test RAG search with no results
        GIVEN: Search query that returns no results
        WHEN: execute_rag_search is called
        THEN: Returns empty results with message
        """
        with patch.object(step2_module, 'retrieve_knowledge') as mock_retrieve:
            mock_retrieve.return_value = []

            result = step2_module.execute_rag_search(
                chatbot_id="chatbot-123",
                query="unknown topic",
                openai_api_key="fake_key",
                db_resource="f/development/db"
            )

            assert result["success"] is True
            assert len(result["results"]) == 0
            assert "no relevant information" in result["message"].lower()

    def test_execute_rag_search_exception(self):
        """
        GOAL: Test RAG search exception handling
        GIVEN: retrieve_knowledge raises exception
        WHEN: execute_rag_search is called
        THEN: Returns error message
        """
        with patch.object(step2_module, 'retrieve_knowledge') as mock_retrieve:
            mock_retrieve.side_effect = Exception("Database error")

            result = step2_module.execute_rag_search(
                chatbot_id="chatbot-123",
                query="test",
                openai_api_key="fake_key",
                db_resource="f/development/db"
            )

            assert "error" in result
            assert "Database error" in result["error"]

    def test_retrieve_knowledge_exception_handling(self):
        """
        GOAL: Test retrieve_knowledge exception handling
        GIVEN: Database or API error during retrieval
        WHEN: retrieve_knowledge is called
        THEN: Returns empty list
        """
        with patch.object(step2_module, 'OpenAI') as mock_openai:
            mock_openai.side_effect = Exception("OpenAI error")

            result = step2_module.retrieve_knowledge(
                chatbot_id="chatbot-123",
                query="test",
                openai_api_key="fake_key",
                db_resource="f/development/db"
            )

            assert result == []


class TestMCPToolExecution:
    """Test MCP tool execution"""

    def test_execute_mcp_tool_success(self):
        """
        GOAL: Test successful MCP tool execution
        GIVEN: Valid MCP server and tool
        WHEN: execute_mcp_tool is called
        THEN: HTTP request is made and response returned
        """
        metadata = {
            "mcp_server_url": "http://mcp-server:3001",
            "integration_id": "int-123"
        }

        with patch('requests.post') as mock_post:
            mock_response = Mock()
            mock_response.json.return_value = {"result": "success"}
            mock_post.return_value = mock_response

            result = step2_module.execute_mcp_tool(
                tool_name="calculate_price",
                metadata=metadata,
                arguments={"amount": 100},
                chatbot_id="chatbot-123"
            )

            assert result["result"] == "success"
            mock_post.assert_called_once()
            call_args = mock_post.call_args
            assert call_args.kwargs['json']['chatbot_id'] == "chatbot-123"
            assert call_args.kwargs['json']['amount'] == 100

    def test_execute_mcp_tool_no_url(self):
        """
        GOAL: Test MCP tool execution with missing URL
        GIVEN: Metadata without server URL
        WHEN: execute_mcp_tool is called
        THEN: Returns error for missing URL
        """
        metadata = {}  # No URL

        result = step2_module.execute_mcp_tool(
            tool_name="test_tool",
            metadata=metadata,
            arguments={},
            chatbot_id="chatbot-123"
        )

        assert "error" in result
        assert "not configured" in result["error"].lower()

    def test_execute_mcp_tool_timeout(self):
        """
        GOAL: Test MCP tool timeout handling
        GIVEN: MCP server that times out
        WHEN: execute_mcp_tool is called
        THEN: Returns timeout error
        """
        metadata = {"mcp_server_url": "http://mcp-server:3001"}

        with patch('requests.post') as mock_post:
            import requests
            mock_post.side_effect = requests.Timeout()

            result = step2_module.execute_mcp_tool(
                tool_name="slow_tool",
                metadata=metadata,
                arguments={},
                chatbot_id="chatbot-123"
            )

            assert "error" in result
            assert "timeout" in result["error"].lower()

    def test_execute_mcp_tool_request_exception(self):
        """
        GOAL: Test MCP tool request exception handling
        GIVEN: MCP server that returns error
        WHEN: execute_mcp_tool is called
        THEN: Returns error message
        """
        metadata = {"mcp_server_url": "http://mcp-server:3001"}

        with patch('requests.post') as mock_post:
            import requests
            mock_post.side_effect = requests.RequestException("Connection error")

            result = step2_module.execute_mcp_tool(
                tool_name="failing_tool",
                metadata=metadata,
                arguments={},
                chatbot_id="chatbot-123"
            )

            assert "error" in result
            assert "Connection error" in result["error"]


class TestWindmillToolExecution:
    """Test Windmill tool execution"""

    def test_execute_windmill_tool_success(self):
        """
        GOAL: Test successful Windmill tool execution
        GIVEN: Valid Windmill script path
        WHEN: execute_windmill_tool is called
        THEN: Script is executed and result returned
        """
        metadata = {"script_path": "f/scripts/process_data"}
        arguments = {"input": "test data"}

        mock_wmill.run_script_by_path = Mock(return_value={"processed": "data"})

        result = step2_module.execute_windmill_tool(
            metadata=metadata,
            arguments=arguments
        )

        assert result["success"] is True
        assert result["data"]["processed"] == "data"
        mock_wmill.run_script_by_path.assert_called_once_with(
            path="f/scripts/process_data",
            args=arguments,
            timeout=30
        )

    def test_execute_windmill_tool_no_script_path(self):
        """
        GOAL: Test Windmill tool with missing script path
        GIVEN: Metadata without script_path
        WHEN: execute_windmill_tool is called
        THEN: Returns error for missing path
        """
        metadata = {}  # No script_path

        result = step2_module.execute_windmill_tool(
            metadata=metadata,
            arguments={}
        )

        assert "error" in result
        assert "not configured" in result["error"].lower()

    def test_execute_windmill_tool_exception(self):
        """
        GOAL: Test Windmill tool exception handling
        GIVEN: Script that raises exception
        WHEN: execute_windmill_tool is called
        THEN: Returns error message
        """
        metadata = {"script_path": "f/scripts/failing"}

        mock_wmill.run_script_by_path = Mock(side_effect=Exception("Script failed"))

        result = step2_module.execute_windmill_tool(
            metadata=metadata,
            arguments={}
        )

        assert "error" in result
        assert "Script failed" in result["error"]


class TestToolPreparation:
    """Test tool definition preparation"""

    def test_prepare_tool_definitions_disabled_tools(self):
        """
        GOAL: Test that disabled tools are skipped
        GIVEN: Tool list with enabled and disabled tools
        WHEN: prepare_tool_definitions is called
        THEN: Only enabled tools are included
        """
        tools = [
            {
                "enabled": True,
                "provider": "mcp",
                "name": "enabled_tool",
                "config": {"description": "Enabled"}
            },
            {
                "enabled": False,
                "provider": "mcp",
                "name": "disabled_tool",
                "config": {"description": "Disabled"}
            }
        ]

        result = step2_module.prepare_tool_definitions(tools, "chatbot-123")

        assert len(result) == 1
        assert result[0]["function"]["name"] == "enabled_tool"

    def test_prepare_tool_definitions_windmill_tool(self):
        """
        GOAL: Test Windmill tool definition preparation
        GIVEN: Windmill tool configuration
        WHEN: prepare_tool_definitions is called
        THEN: Tool is formatted correctly
        """
        tools = [
            {
                "enabled": True,
                "provider": "windmill",
                "name": "windmill_script",
                "description": "Process data",
                "parameters": {"type": "object", "properties": {"input": {"type": "string"}}},
                "settings": {"script_path": "f/scripts/process"},
                "id": "tool-456"
            }
        ]

        result = step2_module.prepare_tool_definitions(tools, "chatbot-123")

        assert len(result) == 1
        assert result[0]["function"]["name"] == "windmill_script"
        assert result[0]["_metadata"]["tool_type"] == "windmill"
        assert result[0]["_metadata"]["script_path"] == "f/scripts/process"


class TestUtilityFunctions:
    """Test utility functions"""

    def test_estimate_tokens(self):
        """
        GOAL: Test token estimation function
        GIVEN: Text of various lengths
        WHEN: estimate_tokens is called
        THEN: Returns reasonable token estimate
        """
        # Short text
        short_tokens = step2_module.estimate_tokens("Hello world")
        assert short_tokens > 0
        assert short_tokens < 10

        # Long text (400 chars = ~100 tokens)
        long_text = "a" * 400
        long_tokens = step2_module.estimate_tokens(long_text)
        assert long_tokens == 100

        # Empty text should return at least 1
        empty_tokens = step2_module.estimate_tokens("")
        assert empty_tokens == 1

    def test_build_tool_instructions_with_llm_instructions(self):
        """
        GOAL: Test build_tool_instructions includes LLM instructions when present
        GIVEN: Tools with llm_instructions field
        WHEN: build_tool_instructions is called
        THEN: LLM instructions are included in output
        """
        tools = [
            {
                "name": "tool_with_instructions",
                "config": {
                    "description": "Test tool",
                    "llm_instructions": "Use this when user asks about pricing"
                }
            },
            {
                "name": "tool_without_instructions",
                "config": {
                    "description": "Another tool"
                }
            }
        ]

        result = step2_module.build_tool_instructions(tools)

        assert "tool_with_instructions" in result
        assert "Use this when user asks about pricing" in result
        assert "tool_without_instructions" in result

    def test_step1_failure_handling(self):
        """
        GOAL: Test proper error handling when Step 1 fails
        GIVEN: context_payload with proceed=False
        WHEN: main is called
        THEN: Returns error message and notify_admin flag
        """
        result = step2_main(
            context_payload={
                "proceed": False,
                "reason": "User is blocked",
                "notify_admin": True
            },
            user_message="Test"
        )

        assert "error" in result
        assert result["error"] == "User is blocked"
        assert result["should_notify_admin"] is True
        assert "unable to process" in result["reply_text"].lower()

    def test_retrieve_knowledge_no_api_key(self):
        """
        GOAL: Test retrieve_knowledge returns empty when no API key
        GIVEN: Empty API key
        WHEN: retrieve_knowledge is called
        THEN: Returns empty list without calling OpenAI
        """
        result = step2_module.retrieve_knowledge(
            chatbot_id="chatbot-123",
            query="test",
            openai_api_key="",  # No API key
            db_resource="f/development/db"
        )

        assert result == []

    def test_openai_agent_loop_json_decode_error(self):
        """
        GOAL: Test handling of malformed JSON in tool call arguments
        GIVEN: Tool call with invalid JSON arguments
        WHEN: execute_agent_loop_openai processes it
        THEN: Handles JSON decode error gracefully
        """
        mock_client = Mock()

        # First response: tool call with malformed JSON
        mock_tool_call = Mock()
        mock_tool_call.id = "call_123"
        mock_tool_call.function.name = "search_knowledge_base"
        mock_tool_call.function.arguments = '{invalid json}'  # Malformed

        mock_message_1 = Mock()
        mock_message_1.tool_calls = [mock_tool_call]
        mock_choice_1 = Mock()
        mock_choice_1.finish_reason = "tool_calls"
        mock_choice_1.message = mock_message_1
        mock_response_1 = Mock()
        mock_response_1.choices = [mock_choice_1]
        mock_response_1.usage = OpenAIUsage(100, 20)

        # Second response: final answer
        mock_message_2 = Mock()
        mock_message_2.content = "Here is the answer"
        mock_choice_2 = Mock()
        mock_choice_2.finish_reason = "stop"
        mock_choice_2.message = mock_message_2
        mock_response_2 = Mock()
        mock_response_2.choices = [mock_choice_2]
        mock_response_2.usage = OpenAIUsage(120, 30)

        mock_client.chat.completions.create = Mock(side_effect=[mock_response_1, mock_response_2])

        with patch.object(step2_module, 'execute_tool') as mock_execute_tool:
            mock_execute_tool.return_value = {"success": True}

            result = step2_module.execute_agent_loop_openai(
                client=mock_client,
                model_name="gpt-4o",
                messages=[{"role": "user", "content": "Test"}],
                tools=[{"type": "function", "function": {"name": "search_knowledge_base"}}],
                chatbot_id="chatbot-123",
                temperature=0.7,
                openai_api_key="fake_key",
                db_resource="f/development/db",
                max_iterations=5
            )

            # Should have handled the error and continued
            assert result["reply_text"] == "Here is the answer"
            # Tool should have been called with empty args
            mock_execute_tool.assert_called_once()
            call_args = mock_execute_tool.call_args
            assert call_args.kwargs['arguments'] == {}

    def test_gemini_agent_loop_with_tool_calls(self):
        """
        GOAL: Test Gemini agent loop executes tools and returns final response
        GIVEN: Gemini client that returns function calls then final response
        WHEN: execute_agent_loop_gemini is called
        THEN: Tools are executed and final response is returned
        """
        mock_client = Mock()
        mock_models = Mock()

        # First response: function call
        mock_fc = Mock()
        mock_fc.name = "search_knowledge_base"
        mock_fc.args = {"query": "test query"}

        mock_part_1 = Mock()
        mock_part_1.function_call = mock_fc

        mock_candidate_1 = Mock()
        mock_candidate_1.content = Mock()
        mock_candidate_1.content.parts = [mock_part_1]

        mock_response_1 = Mock()
        mock_response_1.candidates = [mock_candidate_1]
        mock_response_1.usage_metadata = UsageMetadata(100, 20)

        # Second response: final answer (no function calls)
        mock_part_2 = Mock()
        # Part without function_call attribute
        if hasattr(mock_part_2, 'function_call'):
            delattr(mock_part_2, 'function_call')

        mock_candidate_2 = Mock()
        mock_candidate_2.content = Mock()
        mock_candidate_2.content.parts = [mock_part_2]

        mock_response_2 = Mock()
        mock_response_2.candidates = [mock_candidate_2]
        mock_response_2.text = "Based on the search, here is the answer"
        mock_response_2.usage_metadata = UsageMetadata(120, 30)

        mock_models.generate_content = Mock(side_effect=[mock_response_1, mock_response_2])
        mock_client.models = mock_models

        with patch.object(step2_module, 'execute_tool') as mock_execute_tool:
            mock_execute_tool.return_value = {"success": True, "results": []}

            result = step2_module.execute_agent_loop_gemini(
                client=mock_client,
                model_name="gemini-pro",
                system_prompt="You are helpful",
                user_message="Test question",
                chat_history=[],
                tools=[{"function": {"name": "search_knowledge_base"}}],
                chatbot_id="chatbot-123",
                temperature=0.7,
                google_api_key="fake_key",
                db_resource="f/development/db",
                fallback_message_error="Error",
                fallback_message_limit="Limit",
                max_iterations=5
            )

            # Should have executed tool and returned final answer
            assert result["reply_text"] == "Based on the search, here is the answer"
            assert len(result["tool_executions"]) == 1
            assert result["usage_info"]["tokens_input"] == 220  # 100 + 120
            assert result["usage_info"]["tokens_output"] == 50  # 20 + 30
            assert result["usage_info"]["iterations"] == 2


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
File: api-server/package.json
================================================
{
  "name": "whatsapp-chatbot-server",
  "version": "1.0.0",
  "description": "Chatbot Knowledge Base API Server",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "nodemon server.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "pg": "^8.11.3",
    "multer": "^1.4.5-lts.1",
    "cors": "^2.8.5",
    "node-fetch": "^2.7.0",
    "uuid": "^9.0.1"
  },
  "devDependencies": {
    "nodemon": "^3.0.2"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}


================================================
File: f/development/3_3_log_usage.script.yaml
================================================
summary: ''
description: ''
lock: '!inline f/development/3_3_log_usage.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    context_payload:
      type: object
      description: ''
      default: null
      properties: {}
    db_resource:
      type: string
      description: ''
      default: f/development/business_layer_db_postgreSQL
      originalType: string
    llm_result:
      type: object
      description: ''
      default: null
      properties: {}
    send_result:
      type: object
      description: ''
      default: null
      properties: {}
    webhook_event_id:
      type: integer
      description: ''
      default: null
  required:
    - context_payload
    - llm_result
    - send_result


================================================
File: f/development/ingest_multiple_urls.py
================================================
"""
Batch URL Ingestion for Knowledge Base

This script ingests multiple URLs in batch, checking quota for each one
and creating knowledge_source records. It then triggers async processing
via RAG_process_documents.

Usage:
Called by the API server or frontend when user wants to ingest multiple
URLs discovered by the web crawler.

Features:
- Quota checking per URL
- Graceful handling of partial failures
- Async processing trigger
- Detailed per-URL status reporting
"""

import wmill
import psycopg2
from typing import Dict, List, Any
from datetime import datetime


def main(
    chatbot_id: str,
    urls: List[str],
    db_resource: str = "f/development/business_layer_db_postgreSQL"
) -> Dict[str, Any]:
    """
    Ingest multiple URLs in batch.

    For each URL:
    1. Check quota using check_knowledge_quota
    2. Create knowledge_source record (status: pending)
    3. Trigger RAG_process_documents async

    Args:
        chatbot_id: ID of the chatbot
        urls: List of URLs to ingest
        db_resource: Database resource path

    Returns:
        {
            "total_urls": int,
            "successful": int,
            "failed": int,
            "results": [
                {
                    "url": str,
                    "success": bool,
                    "knowledge_source_id": str (if success),
                    "job_id": str (if success),
                    "error": str (if failed),
                    "quota_info": dict
                }
            ]
        }
    """

    # Get database credentials
    raw_config = wmill.get_resource(db_resource)
    db_params = {
        "host": raw_config.get("host"),
        "port": raw_config.get("port"),
        "user": raw_config.get("user"),
        "password": raw_config.get("password"),
        "dbname": raw_config.get("dbname"),
        "sslmode": "disable",
    }

    results = []
    successful = 0
    failed = 0

    for url in urls:
        try:
            # Check quota for this URL
            quota_check = wmill.run_script_by_path(
                "f/development/utils/check_knowledge_quota",
                args={
                    "chatbot_id": chatbot_id,
                    "source_type": "url",
                    "file_size_mb": 0.0,  # Will be calculated during processing
                    "db_resource": db_resource
                }
            )

            if not quota_check.get("allowed", False):
                # Quota exceeded
                results.append({
                    "url": url,
                    "success": False,
                    "error": f"Quota exceeded: {quota_check.get('quota_type')}",
                    "quota_info": quota_check
                })
                failed += 1
                continue

            # Create knowledge_source record
            conn = psycopg2.connect(**db_params)
            cur = conn.cursor()

            try:
                cur.execute("""
                    INSERT INTO knowledge_sources (
                        chatbot_id,
                        source_type,
                        source_url,
                        status,
                        created_at,
                        updated_at
                    ) VALUES (%s, 'url', %s, 'pending', NOW(), NOW())
                    RETURNING id
                """, (chatbot_id, url))

                knowledge_source_id = cur.fetchone()[0]
                conn.commit()

                # Trigger async RAG processing
                job_id = wmill.run_script_by_path_async(
                    "f/development/RAG_process_documents",
                    args={
                        "knowledge_source_id": str(knowledge_source_id),
                        "db_resource": db_resource
                    }
                )

                results.append({
                    "url": url,
                    "success": True,
                    "knowledge_source_id": str(knowledge_source_id),
                    "job_id": job_id,
                    "quota_info": quota_check
                })
                successful += 1

                print(f"✓ Queued processing for {url} (source_id: {knowledge_source_id}, job: {job_id})")

            finally:
                cur.close()
                conn.close()

        except Exception as e:
            results.append({
                "url": url,
                "success": False,
                "error": str(e)
            })
            failed += 1
            print(f"✗ Failed to process {url}: {e}")

    return {
        "total_urls": len(urls),
        "successful": successful,
        "failed": failed,
        "results": results,
        "timestamp": datetime.now().isoformat()
    }


================================================
File: f/development/upload_document.script.lock
================================================
# py: 3.12
anyio==4.12.0
certifi==2025.11.12
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.11
psycopg2-binary==2.9.11
typing-extensions==4.15.0
wmill==1.601.1

================================================
File: f/development/utils/check_knowledge_quota.py
================================================
"""
Knowledge Base Quota Enforcement

This utility checks if an organization has remaining quota before
allowing new knowledge sources to be added. It prevents abuse and
controls costs by enforcing configurable per-tier limits.

Usage:
Called by upload_document.py and web_crawler.py before creating
new knowledge sources.
"""

import wmill
import psycopg2
from typing import Dict, Any


def main(
    chatbot_id: str,
    source_type: str,  # 'pdf', 'url', 'doc'
    file_size_mb: float = 0.0,
    db_resource: str = "f/development/business_layer_db_postgreSQL"
) -> Dict[str, Any]:
    """
    Check if organization has quota to add new knowledge source.

    Args:
        chatbot_id: ID of the chatbot
        source_type: Type of source ('pdf', 'url', 'doc')
        file_size_mb: Size of file in MB
        db_resource: Database resource path

    Returns:
        {
            "allowed": bool,
            "quota_type": str,  # which limit was hit (if not allowed)
            "current": int,     # current usage
            "max": int,         # max allowed
            "remaining": int    # how many more can be added
        }
    """

    # Get database credentials
    raw_config = wmill.get_resource(db_resource)
    db_params = {
        "host": raw_config.get("host"),
        "port": raw_config.get("port"),
        "user": raw_config.get("user"),
        "password": raw_config.get("password"),
        "dbname": raw_config.get("dbname"),
        "sslmode": "disable",
    }

    conn = psycopg2.connect(**db_params)
    cur = conn.cursor()

    try:
        # Get organization quota limits and current usage
        cur.execute("""
            SELECT
                o.id as organization_id,
                o.max_knowledge_pdfs,
                o.max_knowledge_urls,
                o.max_knowledge_ingestions_per_day,
                o.max_knowledge_storage_mb,
                o.current_knowledge_pdfs,
                o.current_knowledge_urls,
                o.current_storage_mb,
                COALESCE(dic.ingestion_count, 0) as today_ingestions
            FROM chatbots c
            JOIN organizations o ON c.organization_id = o.id
            LEFT JOIN daily_ingestion_counts dic
                ON dic.organization_id = o.id
                AND dic.date = CURRENT_DATE
            WHERE c.id = %s
        """, (chatbot_id,))

        row = cur.fetchone()
        if not row:
            return {
                "allowed": False,
                "quota_type": "CHATBOT_NOT_FOUND",
                "current": 0,
                "max": 0,
                "remaining": 0
            }

        (org_id, max_pdfs, max_urls, max_daily_ingestions, max_storage,
         current_pdfs, current_urls, current_storage, today_ingestions) = row

        # Convert Decimal types to float for arithmetic operations
        current_storage = float(current_storage)

        # Check storage limit
        if current_storage + file_size_mb > max_storage:
            return {
                "allowed": False,
                "quota_type": "STORAGE_LIMIT_EXCEEDED",
                "current": int(current_storage),
                "max": max_storage,
                "remaining": max(0, int(max_storage - current_storage))
            }

        # Check daily ingestion limit
        if today_ingestions >= max_daily_ingestions:
            return {
                "allowed": False,
                "quota_type": "DAILY_INGESTION_LIMIT_EXCEEDED",
                "current": today_ingestions,
                "max": max_daily_ingestions,
                "remaining": 0
            }

        # Check source type specific limits
        if source_type in ('pdf', 'doc'):
            if current_pdfs >= max_pdfs:
                return {
                    "allowed": False,
                    "quota_type": "PDF_LIMIT_EXCEEDED",
                    "current": current_pdfs,
                    "max": max_pdfs,
                    "remaining": 0
                }
            remaining = max_pdfs - current_pdfs

        elif source_type == 'url':
            if current_urls >= max_urls:
                return {
                    "allowed": False,
                    "quota_type": "URL_LIMIT_EXCEEDED",
                    "current": current_urls,
                    "max": max_urls,
                    "remaining": 0
                }
            remaining = max_urls - current_urls

        else:
            # Unknown source type - allow but warn
            remaining = 999

        # All checks passed
        return {
            "allowed": True,
            "quota_type": None,
            "current": current_pdfs if source_type in ('pdf', 'doc') else current_urls,
            "max": max_pdfs if source_type in ('pdf', 'doc') else max_urls,
            "remaining": remaining
        }

    finally:
        cur.close()
        conn.close()


================================================
File: tests/unit/test_message_validation.py
================================================
"""
Unit tests for message size validation.

Tests the message size validation logic in webhook-server/app.js
to prevent abuse from oversized messages.
"""

import pytest
from unittest.mock import Mock, patch
import sys
from pathlib import Path


@pytest.mark.unit
class TestMessageSizeValidation:
    """Test message size validation and plan-based limits."""

    @pytest.fixture
    def message_limits(self):
        """Message size limits per plan tier (in characters)."""
        return {
            "free": 2000,
            "pro": 5000,
            "enterprise": 10000
        }

    @pytest.fixture
    def mock_db_pool(self):
        """Mock PostgreSQL pool for database queries."""
        class MockPool:
            def __init__(self):
                self.plan_tier = "free"

            async def query(self, sql, params):
                """Mock database query."""
                if "SELECT o.plan_tier" in sql:
                    return {
                        "rows": [{"plan_tier": self.plan_tier}]
                    }
                return {"rows": []}

        return MockPool()

    def validate_message_size(self, message_body, plan_tier, message_limits):
        """
        Simulate message size validation logic from webhook-server/app.js

        Returns:
            dict with 'allowed' boolean and 'reason' if rejected
        """
        max_length = message_limits.get(plan_tier, message_limits["free"])

        if len(message_body) > max_length:
            return {
                "allowed": False,
                "reason": "MESSAGE_TOO_LONG",
                "current_length": len(message_body),
                "max_length": max_length,
                "plan_tier": plan_tier
            }

        return {
            "allowed": True,
            "current_length": len(message_body),
            "max_length": max_length,
            "plan_tier": plan_tier
        }

    def test_free_tier_accepts_2000_chars(self, message_limits):
        """Test that free tier accepts messages up to 2000 characters."""
        message = "A" * 2000  # Exactly 2000 chars
        result = self.validate_message_size(message, "free", message_limits)

        assert result["allowed"] is True
        assert result["current_length"] == 2000
        assert result["max_length"] == 2000

    def test_free_tier_rejects_2001_chars(self, message_limits):
        """Test that free tier rejects messages over 2000 characters."""
        message = "A" * 2001  # 1 char over limit
        result = self.validate_message_size(message, "free", message_limits)

        assert result["allowed"] is False
        assert result["reason"] == "MESSAGE_TOO_LONG"
        assert result["current_length"] == 2001
        assert result["max_length"] == 2000

    def test_free_tier_rejects_huge_message(self, message_limits):
        """Test that free tier rejects maliciously large messages."""
        message = "A" * 50000  # 50k chars (way over limit)
        result = self.validate_message_size(message, "free", message_limits)

        assert result["allowed"] is False
        assert result["reason"] == "MESSAGE_TOO_LONG"
        assert result["current_length"] == 50000
        assert result["max_length"] == 2000

    def test_pro_tier_accepts_5000_chars(self, message_limits):
        """Test that pro tier accepts messages up to 5000 characters."""
        message = "B" * 5000
        result = self.validate_message_size(message, "pro", message_limits)

        assert result["allowed"] is True
        assert result["current_length"] == 5000
        assert result["max_length"] == 5000

    def test_pro_tier_rejects_5001_chars(self, message_limits):
        """Test that pro tier rejects messages over 5000 characters."""
        message = "B" * 5001
        result = self.validate_message_size(message, "pro", message_limits)

        assert result["allowed"] is False
        assert result["current_length"] == 5001
        assert result["max_length"] == 5000

    def test_enterprise_tier_accepts_10000_chars(self, message_limits):
        """Test that enterprise tier accepts messages up to 10000 characters."""
        message = "C" * 10000
        result = self.validate_message_size(message, "enterprise", message_limits)

        assert result["allowed"] is True
        assert result["current_length"] == 10000
        assert result["max_length"] == 10000

    def test_enterprise_tier_rejects_10001_chars(self, message_limits):
        """Test that enterprise tier rejects messages over 10000 characters."""
        message = "C" * 10001
        result = self.validate_message_size(message, "enterprise", message_limits)

        assert result["allowed"] is False
        assert result["current_length"] == 10001

    def test_empty_message_allowed(self, message_limits):
        """Test that empty messages are allowed (edge case)."""
        message = ""
        result = self.validate_message_size(message, "free", message_limits)

        assert result["allowed"] is True
        assert result["current_length"] == 0

    def test_unicode_characters_counted_correctly(self, message_limits):
        """Test that Unicode/emoji characters are counted correctly."""
        # Mix of ASCII and Unicode
        message = "Hello 世界 🌍 " * 100  # ~1400 chars

        result = self.validate_message_size(message, "free", message_limits)

        assert result["allowed"] is True
        assert result["current_length"] < 2000

        # Now make it exceed limit
        long_message = "Hello 世界 🌍 " * 200  # ~2800 chars
        result = self.validate_message_size(long_message, "free", message_limits)

        assert result["allowed"] is False

    def test_newlines_and_spaces_counted(self, message_limits):
        """Test that newlines and spaces are counted in length."""
        # Message with many newlines and spaces
        message = "Line 1\n" * 250  # 7 chars * 250 = 1750 chars
        result = self.validate_message_size(message, "free", message_limits)

        assert result["allowed"] is True
        assert result["current_length"] == 1750

        # Exceed limit with newlines
        long_message = "Line 1\n" * 400  # 2800 chars
        result = self.validate_message_size(long_message, "free", message_limits)

        assert result["allowed"] is False

    def test_default_to_free_tier_if_unknown_plan(self, message_limits):
        """Test that unknown plan tier defaults to free tier limits."""
        message = "A" * 2001  # Over free limit

        # Use most restrictive limit for unknown plans
        result = self.validate_message_size(message, "unknown_plan", message_limits)

        # Should default to free tier limit
        assert result["allowed"] is False
        assert result["max_length"] == 2000

    def test_realistic_user_message_scenarios(self, message_limits):
        """Test realistic message scenarios."""
        # Scenario 1: Normal short message (free tier)
        normal_message = "Hello, I need help with my order #12345"
        result = self.validate_message_size(normal_message, "free", message_limits)
        assert result["allowed"] is True

        # Scenario 2: Long product description (free tier - should fail)
        long_description = """
        I am looking for a product that has the following specifications:
        - Size: Large
        - Color: Blue
        - Material: Cotton
        - Features: Waterproof, breathable, durable
        - Price range: $50-$100
        """ * 50  # Repeat to make it long

        result = self.validate_message_size(long_description, "free", message_limits)
        assert result["allowed"] is False

        # Scenario 3: Same long message on pro tier (should pass)
        result = self.validate_message_size(long_description, "pro", message_limits)
        # Check if it's under pro limit
        if len(long_description) <= 5000:
            assert result["allowed"] is True

    def test_attack_scenario_copy_paste_spam(self, message_limits):
        """Test protection against copy-paste spam attacks."""
        # Attacker copies a long text to exploit LLM costs
        spam_message = "SPAM " * 10000  # 50k chars (5 chars * 10000)

        # Free tier should block
        result = self.validate_message_size(spam_message, "free", message_limits)
        assert result["allowed"] is False
        assert result["current_length"] == 50000

        # Pro tier should also block
        result = self.validate_message_size(spam_message, "pro", message_limits)
        assert result["allowed"] is False

        # Even enterprise should block
        result = self.validate_message_size(spam_message, "enterprise", message_limits)
        assert result["allowed"] is False

    def test_exact_boundary_cases(self, message_limits):
        """Test exact boundary conditions for all tiers."""
        # Free tier boundary
        assert self.validate_message_size("A" * 1999, "free", message_limits)["allowed"] is True
        assert self.validate_message_size("A" * 2000, "free", message_limits)["allowed"] is True
        assert self.validate_message_size("A" * 2001, "free", message_limits)["allowed"] is False

        # Pro tier boundary
        assert self.validate_message_size("A" * 4999, "pro", message_limits)["allowed"] is True
        assert self.validate_message_size("A" * 5000, "pro", message_limits)["allowed"] is True
        assert self.validate_message_size("A" * 5001, "pro", message_limits)["allowed"] is False

        # Enterprise tier boundary
        assert self.validate_message_size("A" * 9999, "enterprise", message_limits)["allowed"] is True
        assert self.validate_message_size("A" * 10000, "enterprise", message_limits)["allowed"] is True
        assert self.validate_message_size("A" * 10001, "enterprise", message_limits)["allowed"] is False

    def test_plan_tier_lookup_from_database(self, db_with_data, message_limits):
        """Test that plan tier is correctly looked up from database."""
        # Setup: Verify organization plan tiers exist
        db_with_data.execute("""
            SELECT id, plan_tier FROM organizations
            WHERE id = '11111111-1111-1111-1111-111111111111'
        """)
        org = db_with_data.fetchone()

        assert org is not None
        assert org["plan_tier"] in ["free", "pro", "enterprise"]

        # Verify chatbot is linked to organization
        db_with_data.execute("""
            SELECT c.id, c.organization_id, o.plan_tier
            FROM chatbots c
            JOIN organizations o ON c.organization_id = o.id
            WHERE c.id = '22222222-2222-2222-2222-222222222222'
        """)
        chatbot = db_with_data.fetchone()

        assert chatbot is not None
        assert chatbot["plan_tier"] is not None

    def test_message_rejection_does_not_trigger_windmill(self, message_limits):
        """
        Test that rejected messages don't trigger Windmill flow.
        (Integration test - would be tested in actual webhook-server)
        """
        # This is a behavioral test documenting expected flow:
        # 1. Webhook receives message
        # 2. Checks message size
        # 3. If oversized: return early WITHOUT calling Windmill
        # 4. If valid: proceed to trigger Windmill

        # In actual implementation:
        # if (messageBody.length > maxMessageLength) {
        #     console.warn('Message rejected');
        #     return; // NO Windmill call
        # }
        # await triggerWindmillFlow(...); // Only if valid

        assert True  # Placeholder for integration test

    def test_logging_of_rejected_messages(self, message_limits):
        """Test that rejected messages are logged properly."""
        # Expected log format when message is rejected:
        # console.warn(`Message too long: ${length} chars (max: ${maxLength} for ${planTier} plan)`)

        message = "A" * 3000
        result = self.validate_message_size(message, "free", message_limits)

        # Should have enough info for logging
        assert result["allowed"] is False
        assert "current_length" in result
        assert "max_length" in result
        assert "plan_tier" in result

        # Could generate log message like:
        log_message = f"Message too long: {result['current_length']} chars (max: {result['max_length']} for {result['plan_tier']} plan)"
        assert "3000 chars" in log_message
        assert "2000" in log_message
        assert "free" in log_message


================================================
File: tests/unit/test_step3_1_send_reply.py
================================================
"""
Unit tests for Step 3a: Send reply to WhatsApp

Tests Step 3a's ability to:
- Send messages via WhatsApp Business API
- Handle missing text gracefully
- Format phone numbers correctly
- Handle API errors
"""

import pytest
import sys
import os
from unittest.mock import Mock, patch, MagicMock

# Add parent directories to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../f/development'))

# Import the module under test
import importlib.util
spec = importlib.util.spec_from_file_location(
    "step3a",
    os.path.join(os.path.dirname(__file__), '../../f/development/3_1_send_reply_to_whatsapp.py')
)
step3a_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(step3a_module)
step3a_main = step3a_module.main


class TestStep3aSendReply:
    """Test Step 3a's WhatsApp reply functionality"""

    @patch('requests.post')
    def test_successful_message_send(self, mock_post):
        """Test successful message sending"""
        # Mock successful API response
        mock_response = Mock()
        mock_response.ok = True
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "messaging_product": "whatsapp",
            "contacts": [{"input": "16315551181", "wa_id": "16315551181"}],
            "messages": [{"id": "wamid.ABC123"}]
        }
        mock_post.return_value = mock_response

        # Call function
        result = step3a_main(
            phone_number_id="123456123",
            context_payload={
                "proceed": True,
                "chatbot": {"wa_token": "test_token"},
                "user": {"phone": "16315551181"}
            },
            llm_result={"reply_text": "Hello! How can I help you?"}
        )

        # Assertions
        assert result["success"] is True
        assert "meta_response" in result

        # Verify API was called correctly
        mock_post.assert_called_once()
        call_args = mock_post.call_args
        assert "https://graph.facebook.com/v22.0/123456123/messages" in call_args[0][0]
        assert call_args[1]["headers"]["Authorization"] == "Bearer test_token"
        assert call_args[1]["json"]["text"]["body"] == "Hello! How can I help you?"

    def test_no_text_to_send(self):
        """Test handling when LLM result has no reply_text"""
        result = step3a_main(
            phone_number_id="123456123",
            context_payload={
                "proceed": True,
                "chatbot": {"wa_token": "test_token"},
                "user": {"phone": "16315551181"}
            },
            llm_result={}  # No reply_text
        )

        # Assertions
        assert result["success"] is False

    @patch('requests.post')
    def test_api_error_handling(self, mock_post):
        """Test handling of WhatsApp API errors"""
        import requests

        # Mock failed API response
        mock_response = Mock()
        mock_response.ok = False
        mock_response.status_code = 400
        mock_response.text = '{"error": {"message": "Invalid phone number"}}'
        mock_response.raise_for_status.side_effect = requests.exceptions.HTTPError("API Error")
        mock_post.return_value = mock_response

        result = step3a_main(
            phone_number_id="123456123",
            context_payload={
                "proceed": True,
                "chatbot": {"wa_token": "test_token"},
                "user": {"phone": "invalid"}
            },
            llm_result={"reply_text": "Test message"}
        )

        # Assertions
        assert result["success"] is False
        assert "error" in result

    @patch('requests.post')
    def test_phone_number_formatting(self, mock_post):
        """Test that phone numbers are formatted correctly"""
        mock_response = Mock()
        mock_response.ok = True
        mock_response.json.return_value = {}
        mock_post.return_value = mock_response

        # Test with + prefix (should be removed)
        result = step3a_main(
            phone_number_id="123456123",
            context_payload={
                "proceed": True,
                "chatbot": {"wa_token": "test_token"},
                "user": {"phone": "+16315551181"}  # Has + prefix
            },
            llm_result={"reply_text": "Test"}
        )

        # Verify phone was formatted correctly
        call_args = mock_post.call_args
        assert call_args[1]["json"]["to"] == "16315551181"  # No + prefix


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
File: .env.example
================================================
WINDMILL_DATABASE_URL=postgres://postgres:changeme@db/windmill?sslmode=disable #TBD - change!
WINDMILL_DB_PASSWORD=changeme #TBD - change!
WINDMILL_DB_NAME=windmill

BUSINESS_LOGIC_DB_USER=business_logic_user #TBD - change!
BUSINESS_LOGIC_DB_PASSWORD=business_logic_password #TBD - change!
BUSINESS_LOGIC_DB_NAME=business_logic_app
#connection string with sample credentials: postgres://business_logic_user:business_logic_password@localhost:5433/business_logic_app

#used only for seed.sql:
OWNER_EMAIL=your_email #TBD - change!
WHATSAPP_PHONE_NUMBER_ID=your_phone_number_ID #TBD - change!
WHATSAPP_ACCESS_TOKEN=your_meta_token #TBD - change!
SLACK_WEBHOOK_URL=your_slack_webhook_url #TBD - optional, for owner notifications (stored in organizations table)

# For Enterprise Edition, use:
# WM_IMAGE=ghcr.io/windmill-labs/windmill-ee:main
WM_IMAGE=ghcr.io/windmill-labs/windmill:main

WEBHOOK_VERIFY_TOKEN=
WINDMILL_TOKEN=
WINDMILL_MESSAGE_PROCESSING_ENDPOINT=

# API Server Configuration (Knowledge Base API)
# The WINDMILL_TOKEN is reused here for calling Windmill scripts from the API
# No additional configuration needed - API server uses same DB and Windmill instance

# Monitoring (optional)
# SLACK_ALERT_WEBHOOK=https://hooks.slack.com/services/...

# To use another port than :80, setup the Caddyfile and the caddy section of the docker-compose to your needs: https://caddyserver.com/docs/getting-started
# To have caddy take care of automatic TLS

# To rotate logs, set the following variables:
#LOG_MAX_SIZE=10m
#LOG_MAX_FILE=3

TEST_DB_HOST=localhost
TEST_DB_PORT=5434
TEST_DB_USER=test_user
TEST_DB_PASSWORD=test_password
TEST_DB_NAME=test_business_logic

================================================
File: .pytest_cache/CACHEDIR.TAG
================================================
Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html


================================================
File: api-server/middleware/quota.js
================================================
/**
 * Quota Enforcement Middleware
 *
 * Checks if a chatbot's organization has remaining quota before
 * allowing knowledge base operations.
 */

const fetch = require('node-fetch');

const WINDMILL_URL = process.env.WINDMILL_URL || 'http://localhost:8000';
const WINDMILL_TOKEN = process.env.WINDMILL_TOKEN;
const WINDMILL_WORKSPACE = process.env.WINDMILL_WORKSPACE || 'development';

/**
 * Middleware to check knowledge quota before upload/ingestion
 *
 * Expects req.params.chatbotId and req.body.sourceType
 * Optionally req.body.fileSizeMb
 */
async function checkQuota(req, res, next) {
  const { chatbotId } = req.params;
  const { sourceType, fileSizeMb = 0 } = req.body;

  if (!chatbotId) {
    return res.status(400).json({ error: 'Missing chatbot ID' });
  }

  if (!sourceType) {
    return res.status(400).json({ error: 'Missing source type' });
  }

  try {
    // Call Windmill script to check quota
    const response = await fetch(
      `${WINDMILL_URL}/api/w/${WINDMILL_WORKSPACE}/jobs/run/p/f/development/utils/check_knowledge_quota`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${WINDMILL_TOKEN}`
        },
        body: JSON.stringify({
          chatbot_id: chatbotId,
          source_type: sourceType,
          file_size_mb: fileSizeMb
        })
      }
    );

    if (!response.ok) {
      throw new Error(`Windmill API error: ${response.statusText}`);
    }

    const result = await response.json();

    // Check if allowed
    if (!result.allowed) {
      return res.status(403).json({
        error: 'Quota exceeded',
        quotaType: result.quota_type,
        current: result.current,
        max: result.max,
        remaining: result.remaining
      });
    }

    // Store quota info in request for later use
    req.quotaInfo = result;
    next();

  } catch (error) {
    console.error('Quota check error:', error);
    return res.status(500).json({
      error: 'Failed to check quota',
      message: error.message
    });
  }
}

module.exports = { checkQuota };


================================================
File: docs/AUTO_SCALING_COST_PROTECTION.md
================================================
# Auto-Scaling with Cost Protection

**CRITICAL:** This document outlines hard safety limits to prevent cost explosions from DDOS attacks, bugs, or misconfiguration.

## Core Principles

1. **Fail Safe:** Hard-coded limits that cannot be accidentally changed
2. **Manual Approval:** Human verification required for significant scaling
3. **Cost Caps:** Absolute maximum monthly budget
4. **Alert Early:** Notify before limits are reached
5. **Fail Closed:** When in doubt, stop scaling (not start)

---

## Hard Limits (NON-NEGOTIABLE)

```javascript
// These limits are HARD-CODED and should NEVER be changed without review
const SAFETY_LIMITS = {
  MAX_SERVERS: 5,                    // Absolute maximum server count
  MAX_MONTHLY_COST_EUR: 50,          // Hard budget cap
  MAX_SCALE_UP_PER_HOUR: 1,          // Max new servers per hour
  MANUAL_APPROVAL_THRESHOLD: 3,       // Servers requiring manual approval
  DDOS_DETECTION_THRESHOLD: 1000,     // Requests/min triggering DDOS mode
  COST_ALERT_THRESHOLD_EUR: 40,       // Warning threshold
};
```

### Why These Limits?

**MAX_SERVERS = 5:**
- 5x CPX21 servers = €34.50/month
- Leaves budget for database, Redis, storage
- More than enough for MVP → 1000 users

**MAX_MONTHLY_COST_EUR = 50:**
- Conservative budget for early stage
- Prevents billing surprises
- Can be increased later with proper monitoring

**MANUAL_APPROVAL_THRESHOLD = 3:**
- 1-2 servers: Auto-scale freely
- 3+ servers: Requires human review
- Prevents runaway scaling from bugs

---

## Auto-Scaling Strategy

### Scale-Up Triggers

1. **CPU Usage > 80%** for 5 consecutive minutes
2. **Memory Usage > 85%** for 5 consecutive minutes
3. **API Response Time p95 > 2000ms** for 10 minutes

### Scale-Down Triggers

1. **CPU Usage < 30%** for 15 consecutive minutes
2. **Memory Usage < 40%** for 15 consecutive minutes
3. **AND** current server count > minimum (1)

### Cooldown Periods

- **Scale-Up Cooldown:** 10 minutes (prevent flapping)
- **Scale-Down Cooldown:** 30 minutes (more conservative)
- **Between Scale Operations:** 5 minutes minimum

---

## DDOS Detection & Response

### Detection Criteria

```javascript
function isDDOSAttack() {
  const requestsPerMinute = getRequestRate();
  const uniqueIPCount = getUniqueIPs();
  const errorRate = getErrorRate();

  // DDOS if:
  // - >1000 req/min AND <10 unique IPs
  // - OR >5000 req/min from any source
  // - OR >50% error rate with high traffic
  return (
    (requestsPerMinute > 1000 && uniqueIPCount < 10) ||
    (requestsPerMinute > 5000) ||
    (requestsPerMinute > 500 && errorRate > 0.5)
  );
}
```

### DDOS Response (instead of scaling)

1. **Enable Rate Limiting** (aggressive mode)
   - 10 requests/minute per IP
   - Block IPs with >100 requests in 5 minutes

2. **Enable Cloudflare "Under Attack" Mode**
   - Challenge suspicious requests
   - Block known attack patterns

3. **Alert Team** via Slack
   - Don't auto-scale
   - Investigate before adding servers

4. **If legitimate traffic spike:**
   - Manually approve scaling
   - Monitor cost dashboard
   - Scale down after spike

---

## Implementation: Hetzner Auto-Scaling Script

### Prerequisites

```bash
# Install Hetzner CLI
brew install hcloud  # or apt-get install hcloud

# Authenticate
hcloud context create whatsapp_chatbot
hcloud context use whatsapp_chatbot

# Set token (from Hetzner Console → API Tokens)
export HCLOUD_TOKEN=your_token_here
```

### Auto-Scaling Script

```bash
#!/bin/bash
# File: infrastructure/autoscale.sh
# Description: Auto-scaling with hard safety limits

set -euo pipefail

# ============================================
# HARD-CODED SAFETY LIMITS
# DO NOT CHANGE WITHOUT TEAM REVIEW
# ============================================
readonly MAX_SERVERS=5
readonly MIN_SERVERS=1
readonly MAX_MONTHLY_COST_EUR=50
readonly COST_ALERT_THRESHOLD_EUR=40
readonly MANUAL_APPROVAL_THRESHOLD=3
readonly MAX_SCALE_EVENTS_PER_HOUR=1
readonly DDOS_THRESHOLD_RPM=1000

# Slack webhook for alerts
readonly SLACK_WEBHOOK="${SLACK_WEBHOOK_URL:-}"

# ============================================
# Helper Functions
# ============================================

send_slack_alert() {
  local message="$1"
  local severity="${2:-info}"  # info, warning, critical

  local emoji="ℹ️"
  case "$severity" in
    warning) emoji="⚠️" ;;
    critical) emoji="🚨" ;;
  esac

  if [[ -n "$SLACK_WEBHOOK" ]]; then
    curl -X POST "$SLACK_WEBHOOK" \
      -H 'Content-Type: application/json' \
      -d "{\"text\": \"$emoji [AutoScale] $message\"}"
  fi

  echo "[$(date -Iseconds)] $emoji $message"
}

get_current_server_count() {
  hcloud server list --selector role=api --output columns=id | wc -l
}

get_estimated_monthly_cost() {
  local server_count=$1
  # CPX21 = €6.90/month
  # Managed PostgreSQL Small = €39/month
  # Redis 256MB = €4/month
  # Load Balancer = €5.90/month (if >1 server)

  local app_servers_cost=$(echo "$server_count * 6.90" | bc)
  local db_cost=39
  local redis_cost=4
  local lb_cost=0
  [[ $server_count -gt 1 ]] && lb_cost=5.90

  local total=$(echo "$app_servers_cost + $db_cost + $redis_cost + $lb_cost" | bc)
  echo "$total"
}

check_ddos() {
  # Query Prometheus or logs for request rate
  local rpm=$(curl -s http://localhost:9090/api/v1/query?query=rate(http_requests_total[1m]) | \
    jq -r '.data.result[0].value[1]')

  if (( $(echo "$rpm > $DDOS_THRESHOLD_RPM" | bc -l) )); then
    return 0  # Is DDOS
  else
    return 1  # Not DDOS
  fi
}

# ============================================
# Main Auto-Scaling Logic
# ============================================

main() {
  local action="${1:-check}"  # check, scale-up, scale-down

  # Get current state
  local current_servers=$(get_current_server_count)
  local estimated_cost=$(get_estimated_monthly_cost "$current_servers")

  send_slack_alert "Current: $current_servers servers, Est. cost: €${estimated_cost}/month" "info"

  # Safety check: Cost approaching limit
  if (( $(echo "$estimated_cost >= $COST_ALERT_THRESHOLD_EUR" | bc -l) )); then
    send_slack_alert "⚠️ Cost approaching limit: €${estimated_cost}/${MAX_MONTHLY_COST_EUR}" "warning"
  fi

  # Safety check: Cost exceeded
  if (( $(echo "$estimated_cost >= $MAX_MONTHLY_COST_EUR" | bc -l) )); then
    send_slack_alert "🚨 CRITICAL: Monthly cost limit reached (€${estimated_cost}/${MAX_MONTHLY_COST_EUR})" "critical"
    send_slack_alert "Auto-scaling DISABLED. Manual intervention required." "critical"
    exit 1
  fi

  # Handle scale-up request
  if [[ "$action" == "scale-up" ]]; then
    # Safety check: Already at max
    if [[ $current_servers -ge $MAX_SERVERS ]]; then
      send_slack_alert "Cannot scale up: Already at MAX_SERVERS ($MAX_SERVERS)" "warning"
      exit 1
    fi

    # Safety check: DDOS detection
    if check_ddos; then
      send_slack_alert "🚨 DDOS detected! Enabling rate limiting instead of scaling" "critical"
      # Enable aggressive rate limiting via Redis
      redis-cli SET "rate_limit:ddos_mode" "true" EX 3600
      exit 0
    fi

    # Safety check: Manual approval required
    if [[ $current_servers -ge $MANUAL_APPROVAL_THRESHOLD ]]; then
      send_slack_alert "Manual approval required to add server #$((current_servers + 1))" "warning"
      send_slack_alert "Reply 'APPROVE_SCALE_UP' in Slack to proceed (expires in 5 min)"  "warning"

      # Wait for manual approval (in production, use a proper approval flow)
      # For now, just block scaling
      echo "Waiting for manual approval..."
      exit 0
    fi

    # Calculate cost after scale-up
    local new_cost=$(get_estimated_monthly_cost "$((current_servers + 1))")
    if (( $(echo "$new_cost >= $MAX_MONTHLY_COST_EUR" | bc -l) )); then
      send_slack_alert "Cannot scale up: Would exceed cost limit (€${new_cost})" "warning"
      exit 1
    fi

    # All checks passed - proceed with scale-up
    send_slack_alert "Scaling UP: $current_servers → $((current_servers + 1)) servers" "info"

    # Create new server
    hcloud server create \
      --name "api-server-$((current_servers + 1))" \
      --type cpx21 \
      --image ubuntu-22.04 \
      --location fsn1 \
      --label role=api \
      --user-data-from-file infrastructure/cloud-init.yml

    send_slack_alert "✅ Server #$((current_servers + 1)) created successfully" "info"

    # If this is the 2nd server, create load balancer
    if [[ $((current_servers + 1)) -eq 2 ]]; then
      hcloud load-balancer create \
        --name api-lb \
        --type lb11 \
        --location fsn1

      send_slack_alert "✅ Load balancer created" "info"
    fi
  fi

  # Handle scale-down request
  if [[ "$action" == "scale-down" ]]; then
    # Safety check: Already at minimum
    if [[ $current_servers -le $MIN_SERVERS ]]; then
      send_slack_alert "Cannot scale down: Already at MIN_SERVERS ($MIN_SERVERS)" "info"
      exit 0
    fi

    send_slack_alert "Scaling DOWN: $current_servers → $((current_servers - 1)) servers" "info"

    # Remove oldest server
    local oldest_server=$(hcloud server list --selector role=api --output columns=id | sort -n | head -1)
    hcloud server delete "$oldest_server"

    send_slack_alert "✅ Server removed successfully" "info"

    # If down to 1 server, remove load balancer
    if [[ $((current_servers - 1)) -eq 1 ]]; then
      hcloud load-balancer delete api-lb || true
      send_slack_alert "✅ Load balancer removed (back to single server)" "info"
    fi
  fi
}

# Run main function
main "$@"
```

### Cloud-Init Configuration

```yaml
# File: infrastructure/cloud-init.yml
#cloud-config

packages:
  - docker.io
  - docker-compose
  - git

runcmd:
  # Clone application repository
  - git clone https://github.com/your-org/whatsapp_chatbot.git /opt/whatsapp_chatbot
  - cd /opt/whatsapp_chatbot

  # Setup environment
  - cp .env.example .env
  - echo "DB_HOST=managed-db.hetzner.cloud" >> .env
  - echo "REDIS_HOST=managed-redis.hetzner.cloud" >> .env

  # Start application
  - docker-compose up -d whatsapp_chatbot_api

  # Register with load balancer (if exists)
  - curl -X POST http://load-balancer:9000/register \
      -d '{"server": "'$(hostname -I | awk '{print $1}')'", "port": 4000}'
```

---

## Monitoring Dashboard

### Grafana Dashboard: Cost & Scaling

**Panels:**

1. **Current Monthly Cost Estimate**
   - Gauge: €X / €50 (budget)
   - Alert at €40

2. **Server Count Over Time**
   - Graph: Number of servers
   - Max line at 5 (hard limit)

3. **Auto-Scaling Events**
   - Timeline of scale-up/down events
   - Color-coded by approval status

4. **Request Rate**
   - Graph: Requests per minute
   - DDOS threshold line at 1000 rpm

5. **Resource Utilization**
   - CPU usage (all servers)
   - Memory usage (all servers)
   - Trigger thresholds visible

---

## Cost Monitoring Script

```bash
#!/bin/bash
# File: infrastructure/check-costs.sh
# Run daily via cron to verify actual costs match estimates

HETZNER_API_TOKEN="${HCLOUD_TOKEN}"
CURRENT_MONTH=$(date +%Y-%m)

# Get actual Hetzner usage for current month
actual_cost=$(curl -H "Authorization: Bearer $HETZNER_API_TOKEN" \
  "https://api.hetzner.cloud/v1/pricing" | \
  jq '.pricing.monthly_costs.amount')

echo "Actual cost for $CURRENT_MONTH: €${actual_cost}"

# Alert if over threshold
if (( $(echo "$actual_cost >= 40" | bc -l) )); then
  send_slack_alert "⚠️ Monthly cost: €${actual_cost} (approaching €50 limit)" "warning"
fi

# Block scaling if over limit
if (( $(echo "$actual_cost >= 50" | bc -l) )); then
  send_slack_alert "🚨 COST LIMIT EXCEEDED: €${actual_cost}" "critical"
  # Disable auto-scaling
  touch /var/lock/autoscale-disabled
fi
```

---

## Testing Auto-Scaling

### Test 1: Normal Scale-Up

```bash
# Simulate high CPU load
stress --cpu 4 --timeout 600s

# Monitor logs
tail -f /var/log/autoscale.log

# Expected: New server created after 5 minutes
```

### Test 2: DDOS Protection

```bash
# Simulate traffic spike
ab -n 100000 -c 100 http://your-api.com/

# Expected: Rate limiting enabled, NO new servers created
```

### Test 3: Cost Limit Protection

```bash
# Manually set high estimated cost
export FORCE_COST_ESTIMATE=55

# Try to scale
./infrastructure/autoscale.sh scale-up

# Expected: Scaling blocked, alert sent
```

### Test 4: Manual Approval Flow

```bash
# Start with 2 servers
# Trigger scale-up
./infrastructure/autoscale.sh scale-up

# Expected: Approval request sent to Slack, scaling paused
```

---

## Deployment Checklist

- [ ] Set `HCLOUD_TOKEN` environment variable
- [ ] Configure `SLACK_WEBHOOK_URL` for alerts
- [ ] Test auto-scaling script in staging
- [ ] Configure monitoring dashboard
- [ ] Document manual approval process
- [ ] Schedule daily cost check (cron: `0 9 * * * /usr/local/bin/check-costs.sh`)
- [ ] Test DDOS detection
- [ ] Document scale-down procedure
- [ ] Set up PagerDuty/OpsGenie for critical alerts

---

## Manual Override Procedures

### Disable Auto-Scaling (Emergency)

```bash
# Create lock file
touch /var/lock/autoscale-disabled

# Or set environment variable
export AUTOSCALE_DISABLED=true

# To re-enable
rm /var/lock/autoscale-disabled
```

### Force Scale-Down (Cost Reduction)

```bash
# Manually scale down to minimum
./infrastructure/autoscale.sh scale-down

# Verify
hcloud server list --selector role=api
```

### Override Cost Limit (Approved Increase)

```bash
# ONLY after team approval
# Edit hard-coded limit in autoscale.sh
# Commit change with approval reference

# Before changing, document in:
# - Slack channel
# - Git commit message
# - Cost tracking spreadsheet
```

---

## Summary

**Key Takeaways:**

1. ✅ Hard limits prevent runaway costs
2. ✅ Manual approval for significant scaling
3. ✅ DDOS detection prevents wasteful scaling
4. ✅ Daily cost monitoring catches overages
5. ✅ Alerts notify before problems occur

**Cost Protection Layers:**

1. **Hard-coded maximum** (5 servers, €50/month)
2. **Manual approval threshold** (3+ servers)
3. **Alert threshold** (€40/month)
4. **Daily cost checks** (actual vs. estimate)
5. **Emergency kill switch** (disable auto-scaling)

**Next Steps:**

1. Deploy auto-scaling script to production
2. Test in staging environment
3. Configure Slack alerts
4. Setup monitoring dashboard
5. Document manual procedures
6. Train team on override procedures


================================================
File: docs/QUICK_REFERENCE.md
================================================
# Quick Reference - Developer Cheat Sheet

## Database Operations

### Reset Database
```bash
cd db
./manage_db.sh reset
```

### Check Database State
```bash
./manage_db.sh verify
```

### Common Queries
```sql
-- Check usage for an org
SELECT * FROM get_current_usage('org-uuid');

-- Find duplicate webhook events
SELECT whatsapp_message_id, COUNT(*) 
FROM webhook_events 
GROUP BY whatsapp_message_id 
HAVING COUNT(*) > 1;

-- Recent messages for a contact
SELECT * FROM messages 
WHERE contact_id = 'contact-uuid' 
ORDER BY created_at DESC LIMIT 10;

-- Current usage by org
SELECT 
  o.name,
  us.current_period_messages,
  us.current_period_tokens,
  o.message_limit_monthly,
  o.token_limit_monthly
FROM usage_summary us
JOIN organizations o ON us.organization_id = o.id;
```

## Testing

### Run All Tests
```bash
pytest
```

### Run Specific Tests
```bash
# By file
pytest tests/unit/test_step1_context_loading.py

# By marker
pytest -m unit
pytest -m integration
pytest -m db

# By keyword
pytest -k "idempotency"
pytest -k "usage_limits"
```

Refer to pytest.ini:
markers =
    unit: Unit tests that test individual functions in isolation
    integration: Integration tests that test multiple components together
    slow: Tests that take a long time to run
    db: Tests that require database access
    external: Tests that make real external API calls (should be rarely used)
    live_llm: Tests that make real LLM API calls (OpenAI, Gemini). Run with -m live_llm
    live_embeddings: Tests that generate real embeddings via OpenAI. Run with -m live_embeddings
    live: All live tests that use real external APIs. Run with -m live

### Coverage
```bash
pytest --cov=f/development --cov-report=html
open htmlcov/index.html
```

### Debug Mode
```bash
# Drop into debugger on failure
pytest --pdb

# Verbose output
pytest -vv
```

## Docker Commands

### Main Services
```bash
# Start all
docker-compose up -d

# View logs
docker-compose logs -f webhook-ingress
docker-compose logs -f windmill_worker

# Restart service
docker-compose restart webhook-ingress

# Stop all
docker-compose down
```

### Test Database
```bash
# Start test DB
docker-compose -f docker-compose.test.yml up -d

# Connect to test DB
docker exec -it test_business_logic_db psql -U test_user -d test_business_logic

# Reset test DB
use ./db/manage_db.sh --test reset
```

## Windmill Flows

### Trigger Flow Manually
```bash
# Using curl_windmill_endpoint.sh
cd utils
./curl_windmill_endpoint.sh
```

### Flow Structure
```
1. Context Loading
   ├─ Check idempotency
   ├─ Check usage limits
   ├─ Load chatbot config
   ├─ Fetch tools
   └─ Get history

2. LLM Processing
   ├─ Build prompt
   ├─ Call LLM
   └─ (Future: Tool use)

3. Parallel Execution
   ├─ Send WhatsApp reply
   ├─ Save chat history
   └─ Log usage
```

## Common Debugging

### No Response Sent?

**Check:**
```sql
-- Webhook event status
SELECT * FROM webhook_events ORDER BY created_at DESC LIMIT 5;

-- Recent messages
SELECT * FROM messages ORDER BY created_at DESC LIMIT 10;
```

**Logs:**
```bash
docker-compose logs webhook-ingress | grep -i error
docker-compose logs windmill_worker | grep -i error
```

### Duplicate Messages?

**Check:**
```sql
-- Find duplicates
SELECT whatsapp_message_id, status, COUNT(*) as count
FROM webhook_events
GROUP BY whatsapp_message_id, status
HAVING COUNT(*) > 1;
```

### Usage Limits Not Working?

**Check:**
```sql
-- Org limits
SELECT * FROM organizations WHERE id = 'org-uuid';

-- Current usage
SELECT * FROM usage_summary WHERE organization_id = 'org-uuid';

-- Recent logs
SELECT * FROM usage_logs 
WHERE organization_id = 'org-uuid' 
ORDER BY created_at DESC LIMIT 10;
```

## Environment Variables

### Required in .env
```bash
# Windmill DB
WINDMILL_DATABASE_URL=postgres://postgres:changeme@db/windmill?sslmode=disable
WINDMILL_DB_PASSWORD=changeme
WINDMILL_DB_NAME=windmill

# Business Logic DB
BUSINESS_LOGIC_DB_USER=business_logic_user
BUSINESS_LOGIC_DB_PASSWORD=business_logic_password
BUSINESS_LOGIC_DB_NAME=business_logic_app

# WhatsApp (for seed data)
WHATSAPP_PHONE_NUMBER_ID=your_phone_id
WHATSAPP_ACCESS_TOKEN=your_token
OWNER_EMAIL=your_email

# Webhook Server
WEBHOOK_VERIFY_TOKEN=your_verify_token
WINDMILL_TOKEN=your_windmill_api_token
WINDMILL_MESSAGE_PROCESSING_ENDPOINT=http://windmill_server:8000/api/w/development/jobs/run/f/development/whatsapp_webhook_processor
```

### Test Environment
```bash
# Test DB (in .env or .env.test)
TEST_DB_HOST=localhost
TEST_DB_PORT=5434
TEST_DB_USER=test_user
TEST_DB_PASSWORD=test_password
TEST_DB_NAME=test_business_logic
```

## File Locations

### Key Files
```
Configuration
├── .env                    # Environment variables
├── docker-compose.yml      # Main services
├── docker-compose.test.yml # Test database
└── pytest.ini              # Test configuration

Database
├── db/create.sql          # Schema definition
├── db/drop.sql            # Reset script
├── db/seed.sql            # Test data
└── db/manage_db.sh        # Helper script

Windmill Scripts
├── f/development/1_whatsapp_context_loading.py
├── f/development/2_whatsapp_llm_processing.py
├── f/development/3_1_send_reply_to_whatsapp.py
├── f/development/3_2_save_chat_history.py
└── f/development/3_3_log_usage.py

Testing
├── tests/conftest.py              # Fixtures
├── tests/test_harness/            # Mocks
└── tests/unit/                    # Unit tests
```

## API Endpoints

### WhatsApp Webhook
```bash
# Verification (GET)
GET http://localhost:3000/?hub.mode=subscribe&hub.verify_token=TOKEN&hub.challenge=CHALLENGE

# Message webhook (POST)
POST http://localhost:3000/
Content-Type: application/json

{
  "object": "whatsapp_business_account",
  "entry": [{
    "changes": [{
      "value": {
        "messages": [{
          "from": "phone",
          "id": "msg_id",
          "text": {"body": "Hello"}
        }]
      }
    }]
  }]
}
```

### Windmill Flow
```bash
# Direct trigger
POST http://localhost:8081/api/w/development/jobs/run/f/development/whatsapp_webhook_processor
Authorization: Bearer YOUR_TOKEN
Content-Type: application/json

{
  "phone_number_id": "123",
  "user_phone": "15551234567",
  "user_name": "Test",
  "message_body": "Hello",
  "message_id": "wamid.unique.001"
}
```

## Schema Reference

### Key Tables

**organizations**
- `id` - UUID primary key
- `message_limit_monthly` - Messages allowed
- `token_limit_monthly` - Tokens allowed
- `billing_period_start/end` - Current period

**chatbots**
- `id` - UUID primary key
- `whatsapp_phone_number_id` - Routing key
- `organization_id` - Owner
- `is_active` - Enable/disable

**webhook_events**
- `whatsapp_message_id` - Unique (idempotency)
- `status` - received/processing/completed/failed
- `chatbot_id` - Which bot

**usage_logs**
- `organization_id` - Who used it
- `tokens_input/output/total` - Usage
- `estimated_cost_usd` - Cost estimate

**usage_summary**
- `organization_id` - Cached usage
- `current_period_messages/tokens` - Quick check

## Common Tasks

### Add New Organization
```sql
INSERT INTO organizations (name, slug, plan_tier, message_limit_monthly, token_limit_monthly)
VALUES ('New Corp', 'new-corp', 'pro', 1000, 1000000);
```

### Add New Chatbot
```sql
INSERT INTO chatbots (
  organization_id, 
  name, 
  whatsapp_phone_number_id, 
  whatsapp_access_token,
  system_prompt
) VALUES (
  'org-uuid',
  'New Bot',
  'phone_id_123',
  'token_xyz',
  'You are a helpful assistant'
);
```

### Reset Usage for Testing
```sql
-- Reset usage summary
UPDATE usage_summary 
SET current_period_messages = 0, 
    current_period_tokens = 0
WHERE organization_id = 'org-uuid';

-- Delete usage logs (optional)
DELETE FROM usage_logs 
WHERE organization_id = 'org-uuid';
```

### Clear Old Webhook Events
```sql
-- Delete completed events older than 24 hours
DELETE FROM webhook_events
WHERE status = 'completed'
  AND processed_at < NOW() - INTERVAL '24 hours';

-- Or use the function
SELECT cleanup_old_webhook_events();
```

## Performance Tips

### Slow Queries?
```sql
-- Check query plan
EXPLAIN ANALYZE 
SELECT * FROM get_current_usage('org-uuid');

-- Check missing indexes
SELECT 
  schemaname,
  tablename,
  indexname
FROM pg_indexes
WHERE schemaname = 'public';
```

### Database Maintenance
```bash
# Connect to DB
docker exec -it business_logic_db psql -U business_logic_user -d business_logic_app

# Vacuum and analyze
VACUUM ANALYZE;

# Check table sizes
SELECT 
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

## Error Messages

| Error | Meaning | Fix |
|-------|---------|-----|
| "DB Connection Failed" | Can't reach database | Check docker-compose ps |
| "Chatbot not found" | Invalid phone_number_id | Check chatbots table |
| "Duplicate - Already Processed" | Message seen before | Normal, idempotency working |
| "Usage Limit Exceeded" | Out of quota | Check usage_summary, increase limits |
| "Service Inactive" | Chatbot/Org disabled | Check is_active flags |


================================================
File: f/development/2_whatsapp_llm_processing.script.yaml
================================================
summary: 2 Whatsapp LLM processing
description: >-
  step 2: Main processing stage where LLM uses tools/RAG if appropriate and
  outputs the final response to be sent back to the user
lock: '!inline f/development/2_whatsapp_llm_processing.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    context_payload:
      type: object
      description: ''
      default: null
      properties: {}
    db_resource:
      type: string
      description: ''
      default: f/development/business_layer_db_postgreSQL
      originalType: string
    default_provider:
      type: string
      description: ''
      default: google
      originalType: string
    google_api_key:
      type: string
      description: ''
      default: <function call>
      originalType: string
    openai_api_key:
      type: string
      description: ''
      default: ''
      originalType: string
    user_message:
      type: string
      description: ''
      default: null
      originalType: string
  required:
    - context_payload
    - user_message


================================================
File: tests/fixtures/generate_embeddings.py
================================================
"""
Generate Real Embeddings Fixture

This script generates real embeddings from OpenAI and saves them to a JSON file.
Run this script ONCE to create the fixture, then use the fixture in tests.

Usage:
    OPENAI_API_KEY=your_key python tests/fixtures/generate_embeddings.py

The generated fixture contains:
- Sample document embeddings
- Query embeddings for similarity tests
- Metadata about the embedding model used

This allows tests to use real embeddings without calling the API every time.
"""

import json
import os
from datetime import datetime
from pathlib import Path


def generate_embeddings():
    """Generate embeddings for test documents and save to fixture file."""
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("ERROR: OPENAI_API_KEY environment variable not set")
        print("Usage: OPENAI_API_KEY=your_key python generate_embeddings.py")
        return

    from openai import OpenAI

    client = OpenAI(api_key=api_key)

    # Test documents that simulate real knowledge base content
    test_documents = [
        {
            "id": "doc_1",
            "title": "Return Policy",
            "content": "Our return policy allows returns within 30 days of purchase. "
                       "Items must be in original condition with tags attached."
        },
        {
            "id": "doc_2",
            "title": "Shipping Information",
            "content": "We offer free shipping on orders over $50. "
                       "Standard shipping takes 5-7 business days. "
                       "Express shipping is available for an additional fee."
        },
        {
            "id": "doc_3",
            "title": "Product Warranty",
            "content": "All electronics come with a 1-year manufacturer warranty. "
                       "Extended warranty options are available at checkout."
        },
        {
            "id": "doc_4",
            "title": "Customer Support",
            "content": "Our customer support team is available 24/7. "
                       "You can reach us via chat, email, or phone."
        },
        {
            "id": "doc_5",
            "title": "Payment Methods",
            "content": "We accept all major credit cards, PayPal, and Apple Pay. "
                       "Installment payment options are available for orders over $100."
        }
    ]

    # Test queries for similarity search testing
    test_queries = [
        {
            "id": "query_1",
            "text": "How do I return an item?",
            "expected_match": "doc_1"  # Return Policy
        },
        {
            "id": "query_2",
            "text": "How long does shipping take?",
            "expected_match": "doc_2"  # Shipping Information
        },
        {
            "id": "query_3",
            "text": "What warranty do you offer?",
            "expected_match": "doc_3"  # Product Warranty
        },
        {
            "id": "query_4",
            "text": "How can I contact support?",
            "expected_match": "doc_4"  # Customer Support
        }
    ]

    print("Generating embeddings for test documents...")

    # Generate document embeddings
    doc_embeddings = []
    for doc in test_documents:
        response = client.embeddings.create(
            model="text-embedding-ada-002",
            input=doc["content"]
        )
        doc_embeddings.append({
            "id": doc["id"],
            "title": doc["title"],
            "content": doc["content"],
            "embedding": response.data[0].embedding
        })
        print(f"  Generated embedding for: {doc['title']}")

    print("\nGenerating embeddings for test queries...")

    # Generate query embeddings
    query_embeddings = []
    for query in test_queries:
        response = client.embeddings.create(
            model="text-embedding-ada-002",
            input=query["text"]
        )
        query_embeddings.append({
            "id": query["id"],
            "text": query["text"],
            "expected_match": query["expected_match"],
            "embedding": response.data[0].embedding
        })
        print(f"  Generated embedding for: {query['text']}")

    # Create fixture
    fixture = {
        "metadata": {
            "model": "text-embedding-ada-002",
            "dimensions": 1536,
            "generated_at": datetime.now().isoformat(),
            "description": "Pre-computed embeddings for testing RAG functionality"
        },
        "documents": doc_embeddings,
        "queries": query_embeddings
    }

    # Save to file
    fixture_path = Path(__file__).parent / "embeddings.json"
    with open(fixture_path, "w") as f:
        json.dump(fixture, f, indent=2)

    print(f"\nFixture saved to: {fixture_path}")
    print(f"Documents: {len(doc_embeddings)}")
    print(f"Queries: {len(query_embeddings)}")
    print("\nYou can now use this fixture in tests via the 'real_embeddings' fixture.")


if __name__ == "__main__":
    generate_embeddings()


================================================
File: db/init/01_enable_pgvector.sql
================================================
-- Enable pgvector extension for vector similarity search
-- This script runs automatically when the database container is first created

\c business_logic_app;

-- Create pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Verify installation
SELECT * FROM pg_extension WHERE extname = 'vector';

-- Display pgvector version
SELECT vector_version();

-- Note: HNSW indexes will be created automatically by create.sql
-- when the document_chunks table is created with the embedding column


================================================
File: docs/SCALING_STRATEGY.md
================================================
# Database Scaling Strategy

This document outlines the path for scaling Whatsapp Chatbot from MVP to production at scale.

## Current Architecture (MVP)

**Database:** Custom PostgreSQL 16 with pgvector (Docker container)
**Storage:** Docker volume (`business_logic_db_data`)
**Capacity:** ~1-100 users
**Cost:** €0 (local development)

### Characteristics
- ✅ Fast development iteration
- ✅ Full control over configuration
- ✅ Zero external dependencies
- ❌ Single point of failure
- ❌ Manual backups required
- ❌ Limited to single server

---

## Phase 1: Production Deploy (1-100 Users)

**Timeline:** Week 1
**Target:** Initial production deployment on Hetzner

### Infrastructure
- **Server:** Hetzner CPX21 (3 vCPU, 4GB RAM, 80GB SSD) - €6.90/month
- **Database:** Same Docker container, but with **Hetzner Block Storage**
- **Backup:** Automated daily snapshots to Hetzner Object Storage

### Migration Steps

1. **Attach Block Storage Volume** (20GB, €2.40/month):
   ```bash
   # On Hetzner Cloud Console:
   # Create volume: whatsapp-chatbot-db-volume (20GB, ext4)
   # Attach to server, mount at /mnt/db-storage

   # Update docker-compose.yml:
   volumes:
     business_logic_db_data:
       driver: local
       driver_opts:
         type: none
         o: bind
         device: /mnt/db-storage
   ```

2. **Setup Automated Backups**:
   ```bash
   # Create backup script: /usr/local/bin/backup-db.sh
   #!/bin/bash
   DATE=$(date +%Y%m%d_%H%M%S)
   docker exec business_logic_db pg_dump -U business_logic_user business_logic_app \
     | gzip > /tmp/backup_${DATE}.sql.gz

   # Upload to Hetzner Object Storage (S3-compatible)
   s3cmd put /tmp/backup_${DATE}.sql.gz \
     s3://whatsapp-chatbot-backups/db/backup_${DATE}.sql.gz

   # Cleanup old backups (keep 30 days)
   s3cmd ls s3://whatsapp-chatbot-backups/db/ | \
     awk '{print $4}' | \
     sort | \
     head -n -30 | \
     xargs -I {} s3cmd del {}

   # Add to crontab:
   0 2 * * * /usr/local/bin/backup-db.sh
   ```

3. **Test Restore Procedure**:
   ```bash
   # Download latest backup
   s3cmd get s3://whatsapp-chatbot-backups/db/backup_latest.sql.gz

   # Restore to test database
   gunzip backup_latest.sql.gz
   docker exec -i business_logic_db psql -U business_logic_user business_logic_app \
     < backup_latest.sql
   ```

### Benefits
- ✅ Database volume can be detached and reattached to new servers
- ✅ Automated backups with 30-day retention
- ✅ Point-in-time recovery
- ✅ Easy vertical scaling (upgrade server, reattach volume)

### Limitations
- ❌ Still single server (downtime during upgrades)
- ❌ No read replicas
- ❌ Manual intervention for scaling

**Estimated Cost:** €9.30/month (server + block storage)

---

## Phase 2: Managed Database (100-1000 Users)

**Timeline:** Month 2-3
**Target:** Zero-downtime deployments, better reliability

### Infrastructure
- **App Server:** Hetzner CPX21 (3 vCPU, 4GB RAM) - €6.90/month
- **Database:** Hetzner Managed PostgreSQL (2 vCPU, 4GB RAM) - €39/month
- **Redis:** Hetzner Managed Redis (256MB) - €4/month

### Migration Steps

1. **Provision Managed Database**:
   ```bash
   # Via Hetzner Cloud Console:
   # Create Managed Database: PostgreSQL 16
   # Plan: Small (2 vCPU, 4GB RAM)
   # Enable automated backups (included)
   # Enable pgvector extension
   ```

2. **Zero-Downtime Migration**:
   ```bash
   # 1. Setup replication from Docker DB to Managed DB
   docker exec business_logic_db pg_dump -U business_logic_user \
     business_logic_app | \
     psql -h managed-db.hetzner.cloud -U business_logic_user \
     business_logic_app

   # 2. Switch application to managed DB (update .env)
   DB_HOST=managed-db.hetzner.cloud

   # 3. Restart application
   docker-compose restart whatsapp_chatbot_api webhook-ingress

   # 4. Verify data consistency
   # 5. Decomission Docker database container
   ```

3. **Enable Connection Pooling**:
   ```yaml
   # Use PgBouncer (included with Hetzner Managed DB)
   # Update connection string:
   DATABASE_URL=postgres://user:pass@managed-db.hetzner.cloud:6432/db?pool_timeout=10
   ```

### Benefits
- ✅ Automated backups with point-in-time recovery
- ✅ High availability (99.95% SLA)
- ✅ Automatic minor version upgrades
- ✅ Connection pooling included
- ✅ Monitoring dashboards included
- ✅ Zero-downtime scaling (vertical)

### Limitations
- ❌ Higher cost
- ❌ Less control over configuration
- ❌ Still single region

**Estimated Cost:** €49.90/month

---

## Phase 3: Read Replicas (1000-5000 Users)

**Timeline:** Month 4-6
**Target:** Improved read performance for RAG queries

### Infrastructure
- **App Servers:** 2x Hetzner CPX21 (load balanced) - €13.80/month
- **Load Balancer:** Hetzner Load Balancer - €5.90/month
- **Database:** Hetzner Managed PostgreSQL with 1 Read Replica - €78/month (primary + replica)
- **Redis:** Hetzner Managed Redis (1GB) - €10/month

### Implementation

1. **Add Read Replica**:
   ```bash
   # Via Hetzner Console: Add read replica to existing database
   # Replica will be in same data center
   # Replication lag typically <100ms
   ```

2. **Route Read Queries to Replica**:
   ```javascript
   // In api-server/routes/knowledge.js
   // Use read replica for RAG search queries
   const readPool = new Pool({
     host: process.env.DB_READ_HOST || process.env.DB_HOST,
     port: process.env.DB_PORT,
     // ... other config
   });

   // Write operations use primary
   const writePool = new Pool({
     host: process.env.DB_HOST,
     // ...
   });

   // Example: RAG search uses read replica
   router.post('/:id/knowledge/search', async (req, res) => {
     const result = await readPool.query(`
       SELECT * FROM document_chunks
       WHERE chatbot_id = $1
       ORDER BY embedding <-> $2
       LIMIT 5
     `, [chatbotId, embedding]);
   });
   ```

3. **Load Balancer Configuration**:
   ```yaml
   # Hetzner Load Balancer
   algorithm: round_robin
   health_check:
     protocol: http
     port: 4000
     path: /health
     interval: 10s
   targets:
     - server: app-server-1 (10.0.0.2)
     - server: app-server-2 (10.0.0.3)
   ```

### Benefits
- ✅ Read queries don't impact write performance
- ✅ Horizontal read scalability
- ✅ Improved RAG search latency
- ✅ Better resource utilization

### Limitations
- ❌ Replication lag (eventual consistency)
- ❌ More complex application logic
- ❌ Higher costs

**Estimated Cost:** €107.70/month

---

## Phase 4: Sharding (5000-20000 Users)

**Timeline:** Month 7-12
**Target:** Horizontal write scalability

### Architecture
```
                     Load Balancer
                          |
      +-------------------+-------------------+
      |                   |                   |
  App Server 1       App Server 2       App Server 3
      |                   |                   |
      +-------------------+-------------------+
                          |
                    Shard Router
                          |
      +-------------------+-------------------+
      |                   |                   |
    Shard 1             Shard 2             Shard 3
 (Org ID: 0-333)    (Org ID: 334-666)   (Org ID: 667-999)
```

### Sharding Strategy

**Shard Key:** `organization_id`
**Reason:** Each organization's data is isolated, natural partitioning

1. **Shard Routing Logic**:
   ```javascript
   function getShardForOrganization(organizationId) {
     const hash = crypto.createHash('md5')
       .update(organizationId)
       .digest('hex');
     const shardNumber = parseInt(hash.slice(0, 8), 16) % NUM_SHARDS;
     return SHARD_POOLS[shardNumber];
   }

   // Usage
   const pool = getShardForOrganization(orgId);
   const result = await pool.query('SELECT ...');
   ```

2. **Cross-Shard Queries**:
   ```javascript
   // For admin queries spanning all organizations
   async function getAllOrganizations() {
     const results = await Promise.all(
       SHARD_POOLS.map(pool =>
         pool.query('SELECT * FROM organizations')
       )
     );
     return results.flat();
   }
   ```

### Migration to Sharded Architecture

**CRITICAL:** This is complex and requires significant planning. Consider using:
- Vitess (open-source sharding solution)
- Citus (PostgreSQL extension for sharding)
- Manual sharding with routing layer

**Estimated Cost:** €300-500/month (3 sharded databases + infrastructure)

---

## Phase 5: Multi-Region (20000+ Users)

**Timeline:** Year 2
**Target:** Global performance, disaster recovery

### Architecture
```
                        Global Load Balancer
                     (GeoDNS / Cloudflare)
                                |
                +---------------+---------------+
                |                               |
           EU Region                       US Region
                |                               |
        Hetzner Falkenstein             Hetzner Ashburn
        - App Servers (3x)              - App Servers (3x)
        - PostgreSQL Primary            - PostgreSQL Replica
        - Redis Cluster                 - Redis Cluster
```

### Implementation
- **Multi-region read replicas** for low-latency reads
- **Global write coordination** via primary in EU
- **CDN** for static assets (Cloudflare)
- **Cross-region backup replication**

**Estimated Cost:** €800-1200/month

---

## Decision Matrix

| User Count | Phase | Monthly Cost | Complexity | Recommended |
|------------|-------|--------------|------------|-------------|
| 1-100 | Phase 1: Block Storage | €9 | Low | ✅ Start here |
| 100-1000 | Phase 2: Managed DB | €50 | Medium | ✅ Production ready |
| 1000-5000 | Phase 3: Read Replicas | €108 | Medium | Optional |
| 5000-20000 | Phase 4: Sharding | €300-500 | High | As needed |
| 20000+ | Phase 5: Multi-Region | €800-1200 | Very High | Enterprise |

---

## Vector Database Considerations

**Current:** pgvector in PostgreSQL (works well up to ~1M vectors)

**When to migrate to dedicated vector DB:**

### Triggers for Migration:
1. **>1M document chunks** stored
2. **Search latency >200ms** consistently
3. **Complex embedding operations** (re-ranking, hybrid search)

### Options:
1. **Qdrant** (open-source, self-hosted)
   - Cost: €20-50/month (small instance)
   - Benefits: Purpose-built for vectors, better performance

2. **Weaviate** (open-source, self-hosted)
   - Cost: €30-60/month
   - Benefits: GraphQL API, hybrid search built-in

3. **Pinecone** (managed, cloud)
   - Cost: $70-140/month (production tier)
   - Benefits: Fully managed, excellent performance

**Recommendation:** Stay with pgvector until Phase 3, then evaluate Qdrant

---

## Backup Strategy

### Phase 1-2: Daily Backups
- Frequency: Daily at 2 AM UTC
- Retention: 30 days
- Storage: Hetzner Object Storage
- Cost: ~€1/month

### Phase 3+: Continuous Backups
- Frequency: Continuous WAL archiving
- Point-in-time recovery: Any point in last 7 days
- Storage: S3-compatible object storage
- Cost: ~€5/month

### Disaster Recovery Testing
- **Monthly:** Restore backup to test environment
- **Quarterly:** Full DR drill (simulate complete failure)
- **SLA Target:** RPO < 1 hour, RTO < 4 hours

---

## Monitoring & Alerting

### Key Metrics to Monitor:
1. **Database CPU/Memory** usage
2. **Query latency** (p50, p95, p99)
3. **Connection pool** utilization
4. **Disk I/O** and storage usage
5. **Replication lag** (if using replicas)
6. **Backup success/failure** rate

### Alert Thresholds:
- **CPU > 80%** for 5 minutes → Scale up
- **Disk > 85%** → Expand storage
- **Query latency p95 > 500ms** → Optimize queries
- **Backup failure** → Immediate investigation

---

## Cost Optimization Tips

1. **Right-size instances:** Don't over-provision early
2. **Use spot instances** for non-critical workers (Hetzner doesn't have spot, but can use smaller instances)
3. **Archive old data:** Move >90 day old messages to cold storage
4. **Compress backups:** gzip reduces storage by ~70%
5. **Monitor query performance:** Optimize slow queries before scaling hardware
6. **Connection pooling:** Reduce database connections (saves memory)

---

## Summary

Start simple (Phase 1), migrate to managed database when hitting 100 users (Phase 2), add read replicas when RAG performance degrades (Phase 3). Only consider sharding (Phase 4) if you reach 5000+ users.

**Key Principle:** Vertical scaling is cheaper and simpler than horizontal scaling until you absolutely need it.

**Next Steps:**
1. Implement Phase 1 (Block Storage) on production server
2. Setup automated backups
3. Document restore procedure
4. Test monthly backup restores
5. Plan Phase 2 migration when approaching 100 users


================================================
File: f/development/3_1_send_reply_to_whatsapp.py
================================================
import requests
import json

def main(
    phone_number_id: str,  # Map from Flow Input
    context_payload: dict,  # From Step 1
    llm_result: dict,  # From Step 2
):
    # Check if previous steps succeeded
    if not context_payload.get("proceed", False):
        print(f"Step 1 failed: {context_payload.get('reason', 'Unknown error')}")
        return {"success": False, "error": "Cannot send reply - previous steps failed"}

    if "error" in llm_result:
        print(f"Step 2 failed: {llm_result.get('error', 'Unknown error')}")
        return {"success": False, "error": "Cannot send reply - LLM processing failed"}

    token = context_payload["chatbot"]["wa_token"]
    to_phone = str(context_payload["user"]["phone"]).replace("+", "").strip()
    text_body = llm_result.get("reply_text")

    if not text_body:
        print("No text to send.")
        return {"success": False}

    # Ensure phone_number_id is a string and stripped of whitespace
    url = f"https://graph.facebook.com/v22.0/{str(phone_number_id).strip()}/messages"

    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    data = {
        "messaging_product": "whatsapp",
        "to": to_phone,
        "type": "text",
        "text": {"body": text_body},
    }

    print("--- Request Debug ---")
    print(f"POST URL: {url}")
    print(f"Headers: {{'Authorization': 'Bearer {token[:10]}...', 'Content-Type': 'application/json'}}")
    print(f"Payload: {json.dumps(data, indent=2)}")
    print("---------------------")

    try:
        response = requests.post(url, headers=headers, json=data)
        
        if not response.ok:
            print(f"Meta API Error Response ({response.status_code}):")
            print(response.text)
            
        response.raise_for_status()
        return {"success": True, "meta_response": response.json()}
    except requests.exceptions.RequestException as e:
        print(f"Request failed: {e}")
        return {"success": False, "error": str(e)}


================================================
File: mcp-servers/contact-owner/package.json
================================================
{
  "name": "mcp-contact-owner",
  "version": "1.0.0",
  "description": "MCP server for sending notifications to chatbot owners (purpose-agnostic)",
  "main": "server.js",
  "scripts": {
    "start": "node server.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "pg": "^8.11.3",
    "node-fetch": "^2.7.0"
  }
}


================================================
File: pytest.ini
================================================
[pytest]
# Pytest configuration file

# Test discovery patterns
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Directories to search for tests
testpaths = tests

# Minimum Python version
minversion = 3.12

# Additional command line options
addopts =
    # Verbose output
    -v
    # Show local variables in tracebacks
    --showlocals
    # Show summary of all test outcomes
    -ra
    # Strict markers (fail on unknown markers)
    --strict-markers
    # Strict config (fail on unknown config options)
    --strict-config
    # Show warnings
    -W default
    # Exclude live tests by default (run with pytest tests/live/ -m live)
    --ignore=tests/live
    # Coverage options (if pytest-cov is installed)
    # --cov=f/development
    # --cov-report=term-missing
    # --cov-report=html:htmlcov

# Custom markers
markers =
    unit: Unit tests that test individual functions in isolation
    integration: Integration tests that test multiple components together
    slow: Tests that take a long time to run
    db: Tests that require database access
    external: Tests that make real external API calls (should be rarely used)
    live_llm: Tests that make real LLM API calls (OpenAI, Gemini). Run with -m live_llm
    live_embeddings: Tests that generate real embeddings via OpenAI. Run with -m live_embeddings
    live: All live tests that use real external APIs. Run with -m live

# Test output options
console_output_style = progress
log_cli = false
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Timeout for tests (requires pytest-timeout)
# timeout = 30

# Warnings
filterwarnings =
    # Treat warnings as errors (comment out during development)
    # error
    # Ignore specific warnings
    ignore::DeprecationWarning:
    ignore::PendingDeprecationWarning:

# Environment variables
env =
    TEST_DB_HOST=localhost
    TEST_DB_PORT=5434
    TEST_DB_USER=test_user
    TEST_DB_PASSWORD=test_password
    TEST_DB_NAME=test_business_logic

================================================
File: f/development/2_whatsapp_llm_processing.py
================================================
import wmill
import os
import json
import psycopg2
from psycopg2.extras import RealDictCursor
from openai import OpenAI
from google import genai
from google.genai import types
from typing import Dict, Any, List, Optional


def build_tool_instructions(tools: List[Dict]) -> str:
    """
    Auto-generate tool usage instructions from tool configs.
    Each MCP provides its own llm_instructions that get injected into the system prompt.
    """
    if not tools:
        return ""

    instructions = "\n\n=== HERRAMIENTAS DISPONIBLES ===\n"
    instructions += "Tienes acceso a las siguientes herramientas. Úsalas cuando sea apropiado:\n\n"

    for tool in tools:
        config = tool.get("config", {})
        tool_name = tool.get("name")
        description = config.get("description", "")
        llm_instructions = config.get("llm_instructions", "")

        instructions += f"• {tool_name}: {description}\n"
        if llm_instructions:
            instructions += f"  CUÁNDO USAR: {llm_instructions}\n"
        instructions += "\n"

    return instructions


def main(
    context_payload: dict,
    user_message: str,
    openai_api_key: str = "",
    google_api_key: str = wmill.get_variable("u/admin/GoogleAPI_JD"),
    default_provider: str = "google",
    db_resource: str = "f/development/business_layer_db_postgreSQL",
):
    """
    Step 2: AI Reasoning with RAG (The Brain)

    Flow:
    1. Check if RAG is enabled for this chatbot
    2. If RAG enabled: Retrieve relevant context from knowledge base
    3. Build enhanced prompt with context
    4. Call LLM
    5. Return response with usage info
    """

    # Check if Step 1 succeeded
    if not context_payload.get("proceed", False):
        error_reason = context_payload.get("reason", "Unknown error in Step 1")
        print(f"Step 1 failed: {error_reason}")
        return {
            "error": error_reason,
            "reply_text": "Sorry, I'm unable to process your message at this time. Please try again later.",
            "should_notify_admin": context_payload.get("notify_admin", False)
        }

    # Unpack context
    chatbot = context_payload["chatbot"]
    user = context_payload["user"]
    history = context_payload["history"]
    tools = context_payload["tools"]
    
    # Determine provider
    provider = default_provider
    if "gemini" in chatbot.get("model_name", "").lower():
        provider = "google"
    elif "gpt" in chatbot.get("model_name", "").lower():
        provider = "openai"

    # Base system prompt
    base_prompt = chatbot.get("system_prompt", "You are a helpful assistant.")
    persona = chatbot.get("persona", "")
    
    # Inject user context
    user_context_str = (
        f"\n\nUser Context:\nName: {user.get('name')}\nPhone: {user.get('phone')}"
    )
    if user.get("variables"):
        user_context_str += f"\nKnown Info: {json.dumps(user['variables'])}"
    
    # =========================================================================
    # RAG RETRIEVAL
    # =========================================================================
    rag_context = ""
    retrieved_chunks = []
    
    if chatbot.get("rag_config", {}).get("enabled"):
        print("RAG is enabled, retrieving relevant context...")
        
        retrieved_chunks = retrieve_knowledge(
            chatbot_id=chatbot["id"],
            query=user_message,
            openai_api_key=openai_api_key,
            db_resource=db_resource,
            top_k=5,
            similarity_threshold=0.7
        )
        
        if retrieved_chunks:
            print(f"Retrieved {len(retrieved_chunks)} relevant chunks")
            
            # Format context for prompt
            rag_context = "\n\n=== KNOWLEDGE BASE CONTEXT ===\n"
            rag_context += "Use the following information to answer the user's question. "
            rag_context += "Only use this information if it's relevant to the query.\n\n"
            
            for i, chunk in enumerate(retrieved_chunks, 1):
                source_info = f"[Source: {chunk['source_name']}"
                if chunk.get("metadata", {}).get("page"):
                    source_info += f", Page {chunk['metadata']['page']}"
                source_info += f", Relevance: {chunk['similarity']:.0%}]"
                
                rag_context += f"{i}. {source_info}\n{chunk['content']}\n\n"
            
            rag_context += "=== END KNOWLEDGE BASE CONTEXT ===\n"
    
    # Build full system prompt
    full_system_prompt = f"{base_prompt}\n{persona}\n{user_context_str}"
    if rag_context:
        full_system_prompt += f"\n{rag_context}"

    # AUTO-INJECT tool instructions from MCP configs
    tool_instructions = build_tool_instructions(tools)
    if tool_instructions:
        full_system_prompt += tool_instructions

    # =========================================================================
    # AGENT LOOP: Tool Calling & Multi-Step Reasoning
    # =========================================================================
    # If tools are available, use agent loop for multi-step reasoning
    # Otherwise, fall back to simple LLM call

    reply_text = ""
    updated_variables = {}
    usage_info = {}
    tool_executions = []

    # Prepare tool definitions for LLM
    tool_definitions = prepare_tool_definitions(tools, chatbot["id"])

    # Add built-in RAG search tool if RAG is enabled
    if chatbot.get("rag_config", {}).get("enabled"):
        tool_definitions.append({
            "type": "function",
            "function": {
                "name": "search_knowledge_base",
                "description": "Search the chatbot's knowledge base for relevant information to answer user questions",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "The search query to find relevant information"
                        }
                    },
                    "required": ["query"]
                }
            }
        })

    try:
        if provider == "openai":
            if not openai_api_key:
                return {"error": "Missing OpenAI API Key"}

            model_name = chatbot.get("model_name", "gpt-4o")
            print(f"Using OpenAI with model: {model_name}")

            client = OpenAI(api_key=openai_api_key)

            # Format Messages
            messages = [{"role": "system", "content": full_system_prompt}]

            # Add History
            for msg in history:
                if msg.get("content"):
                    messages.append({"role": msg["role"], "content": msg["content"]})

            # Add Current User Message
            messages.append({"role": "user", "content": user_message})

            # Use agent loop if tools are available
            if tool_definitions:
                print(f"Agent loop enabled with {len(tool_definitions)} tools")
                result = execute_agent_loop_openai(
                    client=client,
                    model_name=model_name,
                    messages=messages,
                    tools=tool_definitions,
                    chatbot_id=chatbot["id"],
                    temperature=chatbot.get("temperature", 0.7),
                    openai_api_key=openai_api_key,
                    db_resource=db_resource,
                    max_iterations=5
                )
                reply_text = result["reply_text"]
                tool_executions = result["tool_executions"]
                usage_info = result["usage_info"]
                usage_info["rag_used"] = bool(rag_context)
                usage_info["chunks_retrieved"] = len(retrieved_chunks)
            else:
                # Simple LLM call without tools
                print(f"Calling OpenAI without tools (RAG: {bool(rag_context)})")

                response = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=chatbot.get("temperature", 0.7),
                )

                reply_text = response.choices[0].message.content

                # Extract usage info
                usage_info = {
                    "provider": "openai",
                    "model": model_name,
                    "tokens_input": response.usage.prompt_tokens,
                    "tokens_output": response.usage.completion_tokens,
                    "rag_used": bool(rag_context),
                    "chunks_retrieved": len(retrieved_chunks),
                }

        elif provider == "google":
            if not google_api_key:
                return {"error": "Missing Google API Key"}

            model_name = chatbot.get("model_name", "gemini-3-flash-preview")
            print(f"Using Google with model: {model_name}")

            client = genai.Client(api_key=google_api_key)

            # Format Messages for Gemini (new SDK uses different format)
            chat_history = []
            for msg in history:
                role = "user" if msg["role"] == "user" else "model"
                if msg.get("content"):
                    chat_history.append(types.Content(role=role, parts=[types.Part(text=msg["content"])]))

            # Use agent loop if tools are available
            if tool_definitions:
                print(f"Gemini agent loop enabled with {len(tool_definitions)} tools")
                result = execute_agent_loop_gemini(
                    client=client,
                    model_name=model_name,
                    system_prompt=full_system_prompt,
                    user_message=user_message,
                    chat_history=chat_history,
                    tools=tool_definitions,
                    chatbot_id=chatbot["id"],
                    temperature=chatbot.get("temperature", 0.7),
                    google_api_key=google_api_key,
                    db_resource=db_resource,
                    fallback_message_error=chatbot.get("fallback_message_error", "Lo siento, estoy teniendo problemas técnicos. Por favor intenta de nuevo más tarde."),
                    fallback_message_limit=chatbot.get("fallback_message_limit", "Lo siento, he alcanzado mi límite de uso. El administrador ha sido notificado."),
                    max_iterations=5
                )
                reply_text = result["reply_text"]
                tool_executions = result["tool_executions"]
                usage_info = result["usage_info"]
                usage_info["rag_used"] = bool(rag_context)
                usage_info["chunks_retrieved"] = len(retrieved_chunks)
            else:
                # Simple LLM call without tools
                print(f"Calling Google Gemini without tools (RAG: {bool(rag_context)})")

                # Combine system prompt + user message
                final_input = f"{full_system_prompt}\n\nUser Message: {user_message}"

                # Add current message to history
                messages = chat_history + [types.Content(role="user", parts=[types.Part(text=final_input)])]

                # Call Gemini with new SDK
                response = client.models.generate_content(
                    model=model_name,
                    contents=messages,
                    config=types.GenerateContentConfig(
                        temperature=chatbot.get("temperature", 0.7)
                    )
                )

                reply_text = response.text

                # Extract usage info (new SDK structure)
                usage_metadata = getattr(response, 'usage_metadata', None)
                if usage_metadata:
                    usage_info = {
                        "provider": "google",
                        "model": model_name,
                        "tokens_input": usage_metadata.prompt_token_count,
                        "tokens_output": usage_metadata.candidates_token_count,
                        "rag_used": bool(rag_context),
                        "chunks_retrieved": len(retrieved_chunks),
                    }
                else:
                    # Fallback if metadata not available
                    usage_info = {
                        "provider": "google",
                        "model": model_name,
                        "tokens_input": estimate_tokens(final_input),
                        "tokens_output": estimate_tokens(reply_text),
                        "rag_used": bool(rag_context),
                        "chunks_retrieved": len(retrieved_chunks),
                    }

        else:
            return {"error": f"Unknown provider: {provider}"}

    except Exception as e:
        print(f"LLM Error: {e}")
        error_str = str(e).lower()

        # Determine if this is a quota/limit error
        is_limit_error = any(keyword in error_str for keyword in ['quota', 'limit', 'rate', 'exhausted', '429'])

        # Use appropriate fallback message
        if is_limit_error:
            reply_text = chatbot.get("fallback_message_limit", "Lo siento, he alcanzado mi límite de uso. El administrador ha sido notificado.")
        else:
            reply_text = chatbot.get("fallback_message_error", "Lo siento, estoy teniendo problemas técnicos. Por favor intenta de nuevo más tarde.")

        usage_info = {
            "provider": provider,
            "model": chatbot.get("model_name"),
            "error": str(e),
            "is_limit_error": is_limit_error,
        }

    return {
        "reply_text": reply_text,
        "updated_variables": updated_variables,
        "usage_info": usage_info,
        "tool_executions": tool_executions,
        "retrieved_sources": [
            {
                "source_name": chunk["source_name"],
                "similarity": chunk["similarity"],
                "metadata": chunk.get("metadata", {}),
            }
            for chunk in retrieved_chunks
        ] if retrieved_chunks else [],
    }


def retrieve_knowledge(
    chatbot_id: str,
    query: str,
    openai_api_key: str,
    db_resource: str,
    top_k: int = 5,
    similarity_threshold: float = 0.7
) -> List[Dict[str, Any]]:
    """
    Retrieve relevant knowledge from the vector database.
    
    Args:
        chatbot_id: ID of the chatbot
        query: User's query
        openai_api_key: OpenAI API key for embeddings
        db_resource: Database resource path
        top_k: Number of chunks to retrieve
        similarity_threshold: Minimum similarity score (0-1)
    
    Returns:
        List of relevant chunks with content and metadata
    """
    if not openai_api_key:
        print("No OpenAI API key provided, skipping RAG")
        return []
    
    try:
        # 1. Generate embedding for the query
        client = OpenAI(api_key=openai_api_key)
        
        response = client.embeddings.create(
            model="text-embedding-ada-002",
            input=query
        )
        
        query_embedding = response.data[0].embedding
        
        # 2. Search vector database
        raw_config = wmill.get_resource(db_resource)
        db_params = {
            "host": raw_config.get("host"),
            "port": raw_config.get("port"),
            "user": raw_config.get("user"),
            "password": raw_config.get("password"),
            "dbname": raw_config.get("dbname"),
            "sslmode": "disable",
        }
        
        conn = psycopg2.connect(**db_params)
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        # Use the search function we created
        cur.execute(
            "SELECT * FROM search_knowledge_base(%s::uuid, %s::vector(1536), %s, %s)",
            (chatbot_id, query_embedding, top_k, similarity_threshold)
        )
        
        results = cur.fetchall()
        
        cur.close()
        conn.close()
        
        return [dict(row) for row in results]
        
    except Exception as e:
        print(f"RAG retrieval error: {e}")
        return []


def estimate_tokens(text: str) -> int:
    """
    Rough token estimation: ~4 characters per token.
    Used as fallback when actual token counts aren't available.
    """
    return max(len(text) // 4, 1)


# =========================================================================
# AGENT LOOP IMPLEMENTATION
# =========================================================================

def prepare_tool_definitions(tools: List[Dict], chatbot_id: str) -> List[Dict]:
    """
    Convert tool records from database into OpenAI function calling format.

    Args:
        tools: List of tool records from chatbot_integrations table
        chatbot_id: ID of the chatbot

    Returns:
        List of tool definitions in OpenAI format
    """
    tool_defs = []

    for tool in tools:
        # Skip disabled tools
        if not tool.get("enabled", True):
            continue

        # Format depends on tool type
        tool_type = tool.get("provider", "custom")

        if tool_type == "mcp":
            # MCP tools from external servers
            # Extract description and parameters from config
            config = tool.get("config", {})
            tool_defs.append({
                "type": "function",
                "function": {
                    "name": tool.get("name", "unknown_tool"),
                    "description": config.get("description", ""),
                    "parameters": config.get("parameters", {
                        "type": "object",
                        "properties": {}
                    })
                },
                "_metadata": {
                    "tool_type": "mcp",
                    "mcp_server_url": config.get("server_url"),
                    "integration_id": tool.get("integration_id")
                }
            })

        elif tool_type == "windmill":
            # Windmill script tools
            tool_defs.append({
                "type": "function",
                "function": {
                    "name": tool.get("name", "unknown_tool"),
                    "description": tool.get("description", ""),
                    "parameters": tool.get("parameters", {
                        "type": "object",
                        "properties": {}
                    })
                },
                "_metadata": {
                    "tool_type": "windmill",
                    "script_path": tool.get("settings", {}).get("script_path"),
                    "integration_id": tool.get("id")
                }
            })

    return tool_defs


def execute_agent_loop_openai(
    client: OpenAI,
    model_name: str,
    messages: List[Dict],
    tools: List[Dict],
    chatbot_id: str,
    temperature: float,
    openai_api_key: str,
    db_resource: str,
    max_iterations: int = 5
) -> Dict[str, Any]:
    """
    Execute agent loop with tool calling for OpenAI.

    The agent iteratively:
    1. Calls LLM with tool definitions
    2. If LLM wants to use a tool, execute it
    3. Feed result back to LLM
    4. Repeat until LLM returns final answer or max iterations reached

    Args:
        client: OpenAI client
        model_name: Model to use
        messages: Conversation messages
        tools: Tool definitions
        chatbot_id: ID of chatbot
        temperature: Sampling temperature
        openai_api_key: API key for embeddings
        db_resource: Database resource path
        max_iterations: Maximum tool call iterations

    Returns:
        Dict with reply_text, tool_executions, and usage_info
    """
    iteration = 0
    tool_executions = []
    total_tokens_input = 0
    total_tokens_output = 0

    while iteration < max_iterations:
        iteration += 1
        print(f"Agent iteration {iteration}/{max_iterations}")

        # Call LLM with tools
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                tools=tools,
                tool_choice="auto",  # Let model decide when to use tools
                temperature=temperature
            )

            # Track token usage
            total_tokens_input += response.usage.prompt_tokens
            total_tokens_output += response.usage.completion_tokens

            choice = response.choices[0]
            finish_reason = choice.finish_reason

            # Check if model wants to call a tool
            if finish_reason == "tool_calls" and choice.message.tool_calls:
                print(f"LLM requested {len(choice.message.tool_calls)} tool calls")

                # Add assistant message with tool calls
                messages.append(choice.message)

                # Execute each tool call
                for tool_call in choice.message.tool_calls:
                    tool_name = tool_call.function.name
                    try:
                        tool_args = json.loads(tool_call.function.arguments)
                    except json.JSONDecodeError:
                        tool_args = {}

                    print(f"Executing tool: {tool_name} with args: {tool_args}")

                    # Execute the tool
                    tool_result = execute_tool(
                        tool_name=tool_name,
                        arguments=tool_args,
                        tools=tools,
                        chatbot_id=chatbot_id,
                        openai_api_key=openai_api_key,
                        db_resource=db_resource
                    )

                    # Track execution
                    tool_executions.append({
                        "tool_name": tool_name,
                        "arguments": tool_args,
                        "result": tool_result,
                        "status": "success" if not tool_result.get("error") else "failed",
                        "iteration": iteration
                    })

                    # Add tool result to messages
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": tool_name,
                        "content": json.dumps(tool_result)
                    })

                # Continue loop to get next LLM response
                continue

            elif finish_reason == "stop":
                # Model returned final answer
                reply_text = choice.message.content

                return {
                    "reply_text": reply_text,
                    "tool_executions": tool_executions,
                    "usage_info": {
                        "provider": "openai",
                        "model": model_name,
                        "tokens_input": total_tokens_input,
                        "tokens_output": total_tokens_output,
                        "tool_calls": len(tool_executions),
                        "iterations": iteration
                    }
                }

            else:
                # Unexpected finish reason
                print(f"Unexpected finish_reason: {finish_reason}")
                reply_text = choice.message.content or "I encountered an issue processing your request."

                return {
                    "reply_text": reply_text,
                    "tool_executions": tool_executions,
                    "usage_info": {
                        "provider": "openai",
                        "model": model_name,
                        "tokens_input": total_tokens_input,
                        "tokens_output": total_tokens_output,
                        "tool_calls": len(tool_executions),
                        "iterations": iteration,
                        "finish_reason": finish_reason
                    }
                }

        except Exception as e:
            print(f"Agent loop error: {e}")
            return {
                "reply_text": "I encountered an error while processing your request.",
                "tool_executions": tool_executions,
                "usage_info": {
                    "provider": "openai",
                    "model": model_name,
                    "tokens_input": total_tokens_input,
                    "tokens_output": total_tokens_output,
                    "tool_calls": len(tool_executions),
                    "iterations": iteration,
                    "error": str(e)
                }
            }

    # Max iterations reached
    print(f"Max iterations ({max_iterations}) reached")
    return {
        "reply_text": "I need more time to think about this. Could you please rephrase your question?",
        "tool_executions": tool_executions,
        "usage_info": {
            "provider": "openai",
            "model": model_name,
            "tokens_input": total_tokens_input,
            "tokens_output": total_tokens_output,
            "tool_calls": len(tool_executions),
            "iterations": iteration,
            "max_iterations_reached": True
        }
    }


def execute_agent_loop_gemini(
    client: Any,
    model_name: str,
    system_prompt: str,
    user_message: str,
    chat_history: List[Any],
    tools: List[Dict],
    chatbot_id: str,
    temperature: float,
    google_api_key: str,
    db_resource: str,
    fallback_message_error: str,
    fallback_message_limit: str,
    max_iterations: int = 5
) -> Dict[str, Any]:
    """
    Execute agent loop with tool calling for Google Gemini using new SDK.

    Args:
        client: Gemini Client instance (new SDK)
        model_name: Model name (e.g., "gemini-3-flash-preview")
        system_prompt: System prompt
        user_message: Current user message
        chat_history: Conversation history as list of types.Content
        tools: Tool definitions
        chatbot_id: ID of chatbot
        temperature: Sampling temperature
        google_api_key: API key for embeddings
        db_resource: Database resource path
        max_iterations: Maximum tool call iterations

    Returns:
        Dict with reply_text, tool_executions, and usage_info
    """
    iteration = 0
    tool_executions = []
    total_tokens_input = 0
    total_tokens_output = 0

    # Convert tool definitions to Gemini function declarations format (new SDK)
    function_declarations = []
    for tool in tools:
        if "function" in tool:
            func = tool["function"]
            function_declarations.append(
                types.FunctionDeclaration(
                    name=func["name"],
                    description=func.get("description", ""),
                    parameters=func.get("parameters", {})
                )
            )

    # Create tool config (new SDK)
    tool_config = None
    if function_declarations:
        tool_config = types.Tool(function_declarations=function_declarations)

    # Build message history - start with system prompt + user message
    first_message = f"{system_prompt}\n\nUser Message: {user_message}"
    messages = chat_history + [types.Content(role="user", parts=[types.Part(text=first_message)])]

    while iteration < max_iterations:
        iteration += 1
        print(f"Gemini agent iteration {iteration}/{max_iterations}")

        try:
            # Call Gemini with tools (new SDK)
            config_params = {
                "temperature": temperature
            }
            if tool_config:
                config_params["tools"] = [tool_config]

            response = client.models.generate_content(
                model=model_name,
                contents=messages,
                config=types.GenerateContentConfig(**config_params)
            )

            # Track token usage
            usage_metadata = getattr(response, 'usage_metadata', None)
            if usage_metadata:
                total_tokens_input += usage_metadata.prompt_token_count
                total_tokens_output += usage_metadata.candidates_token_count

            # Check if model wants to call functions
            candidate = response.candidates[0]
            parts = candidate.content.parts

            # Check for function calls in parts
            function_calls = [part for part in parts if hasattr(part, 'function_call') and part.function_call]

            if function_calls:
                print(f"Gemini requested {len(function_calls)} function calls")

                # Add assistant message with function calls to history
                messages.append(candidate.content)

                # Execute each function call
                function_response_parts = []
                for part in function_calls:
                    fc = part.function_call
                    tool_name = fc.name

                    # Convert function call args to dict (new SDK)
                    tool_args = {}
                    if hasattr(fc, 'args') and fc.args:
                        # fc.args is a dict-like object in new SDK
                        tool_args = dict(fc.args)

                    print(f"Executing tool: {tool_name} with args: {tool_args}")

                    # Execute the tool
                    tool_result = execute_tool(
                        tool_name=tool_name,
                        arguments=tool_args,
                        tools=tools,
                        chatbot_id=chatbot_id,
                        openai_api_key=google_api_key,  # Used for RAG embeddings
                        db_resource=db_resource
                    )

                    # Track execution
                    tool_executions.append({
                        "tool_name": tool_name,
                        "arguments": tool_args,
                        "result": tool_result,
                        "status": "success" if not tool_result.get("error") else "failed",
                        "iteration": iteration
                    })

                    # Create function response (new SDK)
                    function_response_parts.append(
                        types.Part(
                            function_response=types.FunctionResponse(
                                name=tool_name,
                                response=tool_result
                            )
                        )
                    )

                # Add function responses to messages
                messages.append(
                    types.Content(
                        role="function",
                        parts=function_response_parts
                    )
                )

                # Continue loop to get next response
                continue

            # No function calls - this is the final response
            reply_text = response.text
            print(f"Gemini returned final answer after {iteration} iterations")

            return {
                "reply_text": reply_text,
                "tool_executions": tool_executions,
                "usage_info": {
                    "provider": "google",
                    "model": model_name,
                    "tokens_input": total_tokens_input,
                    "tokens_output": total_tokens_output,
                    "tool_calls": len(tool_executions),
                    "iterations": iteration
                }
            }

        except Exception as e:
            import traceback
            print(f"Gemini agent loop error: {e}")
            print(f"Traceback: {traceback.format_exc()}")

            # Determine if this is a quota/limit error
            error_str = str(e).lower()
            is_limit_error = any(keyword in error_str for keyword in ['quota', 'limit', 'rate', 'exhausted', '429'])

            # Use appropriate fallback message
            reply_text = fallback_message_limit if is_limit_error else fallback_message_error

            return {
                "reply_text": reply_text,
                "tool_executions": tool_executions,
                "usage_info": {
                    "provider": "google",
                    "model": model_name,
                    "tokens_input": total_tokens_input,
                    "tokens_output": total_tokens_output,
                    "tool_calls": len(tool_executions),
                    "iterations": iteration,
                    "error": str(e),
                    "is_limit_error": is_limit_error,
                }
            }

    # Max iterations reached
    print(f"Max iterations ({max_iterations}) reached")
    return {
        "reply_text": "Necesito más información para responder. ¿Podrías reformular tu pregunta?",
        "tool_executions": tool_executions,
        "usage_info": {
            "provider": "google",
            "model": model_name,
            "tokens_input": total_tokens_input,
            "tokens_output": total_tokens_output,
            "tool_calls": len(tool_executions),
            "iterations": iteration,
            "max_iterations_reached": True
        }
    }


def execute_tool(
    tool_name: str,
    arguments: Dict[str, Any],
    tools: List[Dict],
    chatbot_id: str,
    openai_api_key: str,
    db_resource: str
) -> Dict[str, Any]:
    """
    Execute a single tool call.

    Args:
        tool_name: Name of the tool to execute
        arguments: Tool arguments
        tools: List of available tool definitions
        chatbot_id: ID of chatbot
        openai_api_key: API key for OpenAI
        db_resource: Database resource path

    Returns:
        Tool execution result
    """
    # Find tool definition
    tool = next((t for t in tools if t.get("function", {}).get("name") == tool_name), None)

    if not tool:
        # Check if it's a built-in tool
        if tool_name == "search_knowledge_base":
            return execute_rag_search(
                chatbot_id=chatbot_id,
                query=arguments.get("query", ""),
                openai_api_key=openai_api_key,
                db_resource=db_resource
            )

        return {"error": f"Tool '{tool_name}' not found"}

    metadata = tool.get("_metadata", {})
    tool_type = metadata.get("tool_type", "unknown")

    try:
        if tool_type == "mcp":
            # Execute MCP tool via HTTP
            return execute_mcp_tool(tool_name, metadata, arguments, chatbot_id)

        elif tool_type == "windmill":
            # Execute Windmill script
            return execute_windmill_tool(metadata, arguments)

        else:
            return {"error": f"Unknown tool type: {tool_type}"}

    except Exception as e:
        print(f"Tool execution error ({tool_name}): {e}")
        return {"error": str(e)}


def execute_rag_search(
    chatbot_id: str,
    query: str,
    openai_api_key: str,
    db_resource: str
) -> Dict[str, Any]:
    """
    Execute RAG search as a tool.

    Args:
        chatbot_id: ID of chatbot
        query: Search query
        openai_api_key: API key for embeddings
        db_resource: Database resource path

    Returns:
        Search results
    """
    try:
        chunks = retrieve_knowledge(
            chatbot_id=chatbot_id,
            query=query,
            openai_api_key=openai_api_key,
            db_resource=db_resource,
            top_k=5,
            similarity_threshold=0.7
        )

        if not chunks:
            return {
                "success": True,
                "results": [],
                "message": "No relevant information found in knowledge base."
            }

        # Format results for LLM
        formatted_results = []
        for chunk in chunks:
            formatted_results.append({
                "content": chunk["content"],
                "source": chunk["source_name"],
                "relevance": f"{chunk['similarity']:.0%}",
                "metadata": chunk.get("metadata", {})
            })

        return {
            "success": True,
            "results": formatted_results,
            "count": len(formatted_results)
        }

    except Exception as e:
        return {"error": f"RAG search failed: {str(e)}"}


def execute_mcp_tool(tool_name: str, metadata: Dict, arguments: Dict, chatbot_id: str) -> Dict[str, Any]:
    """
    Execute MCP tool by calling external MCP server via HTTP.

    Args:
        tool_name: Name of the tool to execute
        metadata: Tool metadata with MCP server URL
        arguments: Tool arguments from LLM
        chatbot_id: ID of the chatbot (auto-injected for MCP servers that need it)

    Returns:
        Tool result
    """
    import requests

    mcp_server_url = metadata.get("mcp_server_url")
    if not mcp_server_url:
        return {"error": "MCP server URL not configured"}

    # Construct full tool endpoint URL: server_url + /tools/ + tool_name
    # e.g., http://mcp_pricing_calculator:3001/tools/calculate_pricing
    tool_endpoint = f"{mcp_server_url.rstrip('/')}/tools/{tool_name}"

    # Auto-inject chatbot_id into all MCP tool calls
    # Some MCPs (like contact_owner) need it to look up org/notification settings
    payload = {**arguments, "chatbot_id": chatbot_id}

    try:
        # Call MCP server at the specific tool endpoint
        response = requests.post(
            tool_endpoint,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=30  # 30 second timeout
        )

        response.raise_for_status()
        return response.json()

    except requests.Timeout:
        return {"error": "MCP server timeout (30s)"}
    except requests.RequestException as e:
        return {"error": f"MCP server error: {str(e)}"}


def execute_windmill_tool(metadata: Dict, arguments: Dict) -> Dict[str, Any]:
    """
    Execute Windmill script tool.

    Args:
        metadata: Tool metadata with script path
        arguments: Tool arguments

    Returns:
        Tool result
    """
    script_path = metadata.get("script_path")
    if not script_path:
        return {"error": "Windmill script path not configured"}

    try:
        # Execute Windmill script synchronously
        result = wmill.run_script_by_path(
            path=script_path,
            args=arguments,
            timeout=30  # 30 second timeout
        )

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        return {"error": f"Windmill script execution failed: {str(e)}"}

================================================
File: f/development/3_3_log_usage.script.lock
================================================
# py: 3.12
anyio==4.12.0
certifi==2025.11.12
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.11
psycopg2-binary==2.9.11
typing-extensions==4.15.0
wmill==1.601.1

================================================
File: .claude/settings.local.json
================================================
{
  "permissions": {
    "allow": [
      "Bash(wc:*)",
      "Bash(grep:*)",
      "Bash(git add:*)",
      "Bash(python3 -m pytest:*)",
      "Bash(git commit:*)",
      "Bash(wmill script generate-metadata:*)",
      "Bash(wmill sync push:*)",
      "Bash(docker compose:*)",
      "Bash(./manage_db.sh reset:*)",
      "Bash(git checkout:*)",
      "Bash(ls:*)",
      "Bash(python3:*)",
      "Bash(docker exec:*)",
      "Bash(curl:*)",
      "Bash(./db/manage_db.sh:*)",
      "Bash(docker ps:*)",
      "Bash(tee:*)",
      "Bash(done)",
      "Bash(python -m pytest:*)",
      "Bash(find:*)"
    ]
  }
}


================================================
File: f/development/1_whatsapp_context_loading.script.lock
================================================
# py: 3.12
anyio==4.12.0
certifi==2025.11.12
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.11
psycopg2-binary==2.9.11
typing-extensions==4.15.0
wmill==1.601.1

================================================
File: mcp-servers/lead-capture/server.js
================================================
const express = require('express');
const { Client } = require('pg');
const app = express();
const PORT = process.env.PORT || 3002;

app.use(express.json());

// Database connection config from environment
const DB_CONFIG = {
  host: process.env.DB_HOST || 'windmill_db',
  port: process.env.DB_PORT || 5432,
  user: process.env.DB_USER || 'postgres',
  password: process.env.DB_PASSWORD || 'changeme',
  database: process.env.DB_NAME || 'business_logic'
};

// Health check endpoint
app.get('/health', (req, res) => {
  res.json({ status: 'healthy', service: 'lead-capture' });
});

// MCP tool endpoint - capture lead
app.post('/tools/capture_lead', async (req, res) => {
  const client = new Client(DB_CONFIG);

  try {
    const {
      name,
      phone,
      email,
      company,
      estimated_messages,
      notes,
      chatbot_id
    } = req.body;

    // Validate required fields
    if (!phone) {
      return res.status(400).json({
        error: 'El número de teléfono es requerido'
      });
    }

    await client.connect();

    // Check if lead already exists
    const checkQuery = `
      SELECT id, created_at
      FROM leads
      WHERE phone = $1
      ORDER BY created_at DESC
      LIMIT 1
    `;
    const existingLead = await client.query(checkQuery, [phone]);

    if (existingLead.rows.length > 0) {
      // Update existing lead
      const updateQuery = `
        UPDATE leads
        SET
          name = COALESCE($1, name),
          email = COALESCE($2, email),
          company = COALESCE($3, company),
          estimated_messages = COALESCE($4, estimated_messages),
          notes = COALESCE($5, notes),
          chatbot_id = COALESCE($6, chatbot_id),
          updated_at = NOW(),
          contact_count = contact_count + 1
        WHERE id = $7
        RETURNING *
      `;

      const result = await client.query(updateQuery, [
        name,
        email,
        company,
        estimated_messages,
        notes,
        chatbot_id,
        existingLead.rows[0].id
      ]);

      await client.end();

      return res.json({
        success: true,
        action: 'actualizado',
        mensaje: `Lead actualizado exitosamente. Este contacto ha interactuado ${result.rows[0].contact_count} veces.`,
        lead: {
          id: result.rows[0].id,
          nombre: result.rows[0].name,
          telefono: result.rows[0].phone,
          email: result.rows[0].email,
          empresa: result.rows[0].company,
          mensajes_estimados: result.rows[0].estimated_messages,
          veces_contactado: result.rows[0].contact_count
        }
      });
    }

    // Insert new lead
    const insertQuery = `
      INSERT INTO leads (
        name,
        phone,
        email,
        company,
        estimated_messages,
        notes,
        chatbot_id,
        contact_count,
        created_at,
        updated_at
      )
      VALUES ($1, $2, $3, $4, $5, $6, $7, 1, NOW(), NOW())
      RETURNING *
    `;

    const result = await client.query(insertQuery, [
      name || null,
      phone,
      email || null,
      company || null,
      estimated_messages || null,
      notes || null,
      chatbot_id || null
    ]);

    await client.end();

    res.json({
      success: true,
      action: 'creado',
      mensaje: 'Lead guardado exitosamente. Un representante te contactará pronto.',
      lead: {
        id: result.rows[0].id,
        nombre: result.rows[0].name,
        telefono: result.rows[0].phone,
        email: result.rows[0].email,
        empresa: result.rows[0].company,
        mensajes_estimados: result.rows[0].estimated_messages
      }
    });

  } catch (error) {
    console.error('Error capturing lead:', error);

    if (client._connected) {
      await client.end();
    }

    res.status(500).json({
      error: 'Error al guardar la información del lead',
      details: error.message
    });
  }
});

// MCP tool endpoint - get lead info
app.post('/tools/get_lead', async (req, res) => {
  const client = new Client(DB_CONFIG);

  try {
    const { phone } = req.body;

    if (!phone) {
      return res.status(400).json({
        error: 'El número de teléfono es requerido'
      });
    }

    await client.connect();

    const query = `
      SELECT * FROM leads
      WHERE phone = $1
      ORDER BY created_at DESC
      LIMIT 1
    `;

    const result = await client.query(query, [phone]);
    await client.end();

    if (result.rows.length === 0) {
      return res.json({
        found: false,
        mensaje: 'No se encontró información de este contacto'
      });
    }

    const lead = result.rows[0];
    res.json({
      found: true,
      lead: {
        nombre: lead.name,
        telefono: lead.phone,
        email: lead.email,
        empresa: lead.company,
        mensajes_estimados: lead.estimated_messages,
        veces_contactado: lead.contact_count,
        primer_contacto: lead.created_at,
        ultimo_contacto: lead.updated_at
      }
    });

  } catch (error) {
    console.error('Error retrieving lead:', error);

    if (client._connected) {
      await client.end();
    }

    res.status(500).json({
      error: 'Error al buscar la información del lead',
      details: error.message
    });
  }
});

// MCP tool discovery endpoint
app.get('/tools', (req, res) => {
  res.json({
    tools: [
      {
        name: 'capture_lead',
        description: 'Guarda la información de un cliente potencial interesado en el chatbot de WhatsApp',
        input_schema: {
          type: 'object',
          properties: {
            phone: {
              type: 'string',
              description: 'Número de teléfono del cliente (requerido)'
            },
            name: {
              type: 'string',
              description: 'Nombre del cliente'
            },
            email: {
              type: 'string',
              description: 'Correo electrónico del cliente'
            },
            company: {
              type: 'string',
              description: 'Nombre de la empresa'
            },
            estimated_messages: {
              type: 'number',
              description: 'Volumen estimado de mensajes por mes'
            },
            notes: {
              type: 'string',
              description: 'Notas adicionales sobre el cliente'
            },
            chatbot_id: {
              type: 'string',
              description: 'ID del chatbot que capturó el lead'
            }
          },
          required: ['phone']
        }
      },
      {
        name: 'get_lead',
        description: 'Busca información de un cliente potencial por número de teléfono',
        input_schema: {
          type: 'object',
          properties: {
            phone: {
              type: 'string',
              description: 'Número de teléfono a buscar'
            }
          },
          required: ['phone']
        }
      }
    ]
  });
});

app.listen(PORT, () => {
  console.log(`Lead Capture MCP server running on port ${PORT}`);
  console.log(`Health check: http://localhost:${PORT}/health`);
  console.log(`Tool discovery: http://localhost:${PORT}/tools`);
});


================================================
File: db/migrations/001_add_tool_tables.sql
================================================
-- Migration: Add tool execution tracking tables
-- Purpose: Track agent tool calls for debugging, billing, and analytics
-- Date: 2025-12-29

-- =====================================================================
-- Tool Executions Table
-- =====================================================================
-- Tracks every tool call made by the agent during message processing

CREATE TABLE IF NOT EXISTS tool_executions (
    id BIGSERIAL PRIMARY KEY,

    -- Linkage
    message_id BIGINT REFERENCES messages(id) ON DELETE CASCADE,
    chatbot_id UUID NOT NULL REFERENCES chatbots(id) ON DELETE CASCADE,
    organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,

    -- Tool identification
    tool_name VARCHAR(100) NOT NULL,
    tool_type VARCHAR(20) NOT NULL CHECK (tool_type IN ('mcp', 'windmill', 'builtin')),
    integration_id UUID REFERENCES org_integrations(id) ON DELETE SET NULL,

    -- Execution details
    arguments JSONB NOT NULL DEFAULT '{}',
    result JSONB,
    status VARCHAR(20) DEFAULT 'pending' CHECK (status IN ('pending', 'success', 'failed', 'timeout')),
    error_message TEXT,

    -- Performance tracking
    execution_time_ms INT,
    iteration INT DEFAULT 1, -- Which agent loop iteration this was

    -- Timestamps
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    completed_at TIMESTAMP WITH TIME ZONE
);

-- Indexes for performance
CREATE INDEX idx_tool_executions_message ON tool_executions(message_id);
CREATE INDEX idx_tool_executions_chatbot ON tool_executions(chatbot_id);
CREATE INDEX idx_tool_executions_org ON tool_executions(organization_id);
CREATE INDEX idx_tool_executions_integration ON tool_executions(integration_id);
CREATE INDEX idx_tool_executions_created_at ON tool_executions(created_at DESC);
CREATE INDEX idx_tool_executions_status ON tool_executions(status) WHERE status != 'success';

-- Composite index for analytics queries
CREATE INDEX idx_tool_executions_analytics ON tool_executions(
    organization_id,
    tool_name,
    status,
    created_at DESC
);

-- =====================================================================
-- Trigger: Auto-update completed_at on status change
-- =====================================================================

CREATE OR REPLACE FUNCTION update_tool_execution_completed_at()
RETURNS TRIGGER AS $$
BEGIN
    IF NEW.status IN ('success', 'failed', 'timeout') AND OLD.status = 'pending' THEN
        NEW.completed_at = NOW();
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_update_tool_execution_completed_at
BEFORE UPDATE ON tool_executions
FOR EACH ROW
EXECUTE FUNCTION update_tool_execution_completed_at();

-- =====================================================================
-- System Alerts Table (if not exists)
-- =====================================================================
-- Used for quota alerts, tool failures, etc.

CREATE TABLE IF NOT EXISTS system_alerts (
    id BIGSERIAL PRIMARY KEY,
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    chatbot_id UUID REFERENCES chatbots(id) ON DELETE CASCADE,

    type VARCHAR(50) NOT NULL CHECK (type IN (
        'QUOTA_EXCEEDED',
        'TOOL_FAILURE',
        'RAG_FAILURE',
        'WEBHOOK_FAILURE',
        'SECURITY_WARNING',
        'OTHER'
    )),

    severity VARCHAR(20) DEFAULT 'warning' CHECK (severity IN ('info', 'warning', 'error', 'critical')),
    message TEXT NOT NULL,
    metadata JSONB DEFAULT '{}',

    -- Alert state
    acknowledged BOOLEAN DEFAULT FALSE,
    acknowledged_by VARCHAR(255),
    acknowledged_at TIMESTAMP WITH TIME ZONE,

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_system_alerts_org ON system_alerts(organization_id);
CREATE INDEX idx_system_alerts_chatbot ON system_alerts(chatbot_id);
CREATE INDEX idx_system_alerts_type ON system_alerts(type);
CREATE INDEX idx_system_alerts_unacknowledged ON system_alerts(acknowledged) WHERE acknowledged = FALSE;
CREATE INDEX idx_system_alerts_created_at ON system_alerts(created_at DESC);

-- =====================================================================
-- Helpful Views
-- =====================================================================

-- View: Tool usage statistics per organization
CREATE OR REPLACE VIEW tool_usage_stats AS
SELECT
    organization_id,
    tool_name,
    tool_type,
    COUNT(*) as total_executions,
    COUNT(*) FILTER (WHERE status = 'success') as successful_executions,
    COUNT(*) FILTER (WHERE status = 'failed') as failed_executions,
    COUNT(*) FILTER (WHERE status = 'timeout') as timeout_executions,
    AVG(execution_time_ms) FILTER (WHERE execution_time_ms IS NOT NULL) as avg_execution_time_ms,
    MAX(created_at) as last_used
FROM tool_executions
GROUP BY organization_id, tool_name, tool_type;

-- View: Recent tool failures for monitoring
CREATE OR REPLACE VIEW recent_tool_failures AS
SELECT
    te.id,
    te.organization_id,
    o.name as organization_name,
    te.chatbot_id,
    c.name as chatbot_name,
    te.tool_name,
    te.tool_type,
    te.error_message,
    te.execution_time_ms,
    te.created_at
FROM tool_executions te
JOIN organizations o ON te.organization_id = o.id
JOIN chatbots c ON te.chatbot_id = c.id
WHERE te.status IN ('failed', 'timeout')
  AND te.created_at > NOW() - INTERVAL '24 hours'
ORDER BY te.created_at DESC;

-- =====================================================================
-- Comments
-- =====================================================================

COMMENT ON TABLE tool_executions IS 'Tracks all agent tool executions for debugging, analytics, and billing';
COMMENT ON COLUMN tool_executions.iteration IS 'Which iteration of the agent loop this tool was called in (1-5)';
COMMENT ON COLUMN tool_executions.execution_time_ms IS 'Time taken to execute the tool in milliseconds';

COMMENT ON TABLE system_alerts IS 'System-generated alerts for quota limits, failures, security warnings';

COMMENT ON VIEW tool_usage_stats IS 'Aggregated tool usage statistics per organization';
COMMENT ON VIEW recent_tool_failures IS 'Recent tool execution failures for monitoring dashboard';


================================================
File: f/webhooks/folder.meta.yaml
================================================
summary: ''
display_name: webhooks
extra_perms:
  u/admin: true
  u/jdrqlabs: true
owners:
  - u/jdrqlabs
  - u/admin


================================================
File: tests/live/conftest.py
================================================
"""
Live Test Configuration

This module configures live tests that make real API calls.
Live tests are skipped by default unless explicitly enabled.

Usage:
    # Run all tests EXCEPT live tests (default behavior)
    pytest tests/

    # Run ONLY live LLM tests
    pytest tests/live/ -m live_llm

    # Run ONLY live embedding tests
    pytest tests/live/ -m live_embeddings

    # Run ALL live tests
    pytest tests/live/ -m live

Environment Variables Required:
    OPENAI_API_KEY: For embedding and GPT tests
    GOOGLE_API_KEY: For Gemini tests (optional)
"""

import pytest
import os
import sys


def pytest_configure(config):
    """Add custom markers and configure live tests."""
    # Register markers
    config.addinivalue_line("markers", "live_llm: Tests that call real LLM APIs")
    config.addinivalue_line("markers", "live_embeddings: Tests that generate real embeddings")
    config.addinivalue_line("markers", "live: All live API tests")


@pytest.fixture(scope="session")
def openai_api_key():
    """
    Get OpenAI API key from environment.
    Skip test if not available.
    """
    key = os.environ.get("OPENAI_API_KEY")
    if not key:
        pytest.skip("OPENAI_API_KEY not set - skipping live test")
    return key


@pytest.fixture(scope="session")
def google_api_key():
    """
    Get Google API key from environment.
    Skip test if not available.
    """
    key = os.environ.get("GOOGLE_API_KEY")
    if not key:
        pytest.skip("GOOGLE_API_KEY not set - skipping live test")
    return key


@pytest.fixture(scope="session")
def live_test_warning():
    """
    Print warning before running live tests.
    """
    print("\n" + "=" * 60)
    print("WARNING: Running LIVE tests with real API calls")
    print("This will incur API costs!")
    print("=" * 60 + "\n")
    return True


================================================
File: wmill.yaml
================================================
defaultTs: bun
includes:
  - f/**
excludes: []
codebases: []
skipVariables: false
skipResources: false
skipResourceTypes: false
skipSecrets: true
skipScripts: false
skipFlows: false
skipApps: false
skipFolders: false
includeSchedules: false
includeTriggers: false
includeUsers: false
includeGroups: false
includeSettings: false
includeKey: false
skipWorkspaceDependencies: false
nonDottedPaths: true
gitBranches:
  master:
    overrides: {}
  claude-development:
    overrides: {}
  bugfix/webhook-payload-extraction:
    overrides: {}
  feature/rag-knowledge-base:
    overrides: {}


================================================
File: f/development/3_3_log_usage.py
================================================
import wmill
import psycopg2
from psycopg2.extras import RealDictCursor
from typing import Dict, Any


def main(
    context_payload: dict,  # From Step 1
    llm_result: dict,  # From Step 2
    send_result: dict,  # From Step 3 (Meta API response)
    webhook_event_id: int = None,  # From flow input (if tracked)
    db_resource: str = "f/development/business_layer_db_postgreSQL",
) -> Dict[str, Any]:
    """
    Step 5: Usage Logging

    ONLY logs usage if Meta API successfully delivered the message.
    Tracks token usage and message counts for billing/analytics.
    Updates the usage_summary table for quick limit checks.
    """

    # Check if previous steps succeeded
    if not context_payload.get("proceed", False):
        print(f"Step 1 failed: {context_payload.get('reason', 'Unknown error')}")
        print("Skipping usage logging")
        return {"success": False, "error": "Cannot log usage - Step 1 failed"}

    if "error" in llm_result:
        print(f"Step 2 failed: {llm_result.get('error', 'Unknown error')}")
        print("Skipping usage logging")
        return {"success": False, "error": "Cannot log usage - Step 2 failed"}

    # CRITICAL: Don't charge users for messages that Meta failed to deliver
    if not send_result.get("success", False):
        print(f"Step 3 failed: {send_result.get('error', 'Meta API delivery failed')}")
        print("Skipping usage logging - message was not delivered to user")
        return {"success": False, "error": "Cannot log usage - Step 3 failed (message not delivered)"}

    # Extract data from previous steps
    org_id = context_payload["chatbot"]["organization_id"]
    chatbot_id = context_payload["chatbot"]["id"]
    contact_id = context_payload["user"]["id"]
    
    # Get usage info from LLM result
    usage_info = llm_result.get("usage_info", {})
    model_name = usage_info.get("model", context_payload["chatbot"].get("model_name", "unknown"))
    provider = usage_info.get("provider", "unknown")
    
    # Token counts - try to get from LLM response metadata
    # If not available, estimate based on content length
    tokens_input = usage_info.get("tokens_input", 0)
    tokens_output = usage_info.get("tokens_output", 0)
    
    # Fallback: rough estimation if not provided
    if tokens_input == 0 and tokens_output == 0:
        # Rough estimation: ~4 chars per token
        user_message = llm_result.get("user_message", "")
        reply_text = llm_result.get("reply_text", "")
        tokens_input = max(len(user_message) // 4, 10)
        tokens_output = max(len(reply_text) // 4, 10)
    
    tokens_total = tokens_input + tokens_output
    
    # Cost estimation (simplified, should be updated with actual pricing TBD)
    cost_per_1k_tokens = _get_cost_per_1k_tokens(provider, model_name)
    estimated_cost = (tokens_total / 1000.0) * cost_per_1k_tokens

    # Setup DB connection
    raw_config = wmill.get_resource(db_resource)
    db_params = {
        "host": raw_config.get("host"),
        "port": raw_config.get("port"),
        "user": raw_config.get("user"),
        "password": raw_config.get("password"),
        "dbname": raw_config.get("dbname"),
        "sslmode": "disable",
    }

    try:
        conn = psycopg2.connect(**db_params)
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # 1. Insert usage log
        insert_usage = """
            INSERT INTO usage_logs (
                organization_id,
                chatbot_id,
                contact_id,
                webhook_event_id,
                message_count,
                tokens_input,
                tokens_output,
                tokens_total,
                model_name,
                provider,
                estimated_cost_usd,
                date_bucket
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, CURRENT_DATE
            )
            RETURNING id
        """
        
        cur.execute(
            insert_usage,
            (
                org_id,
                chatbot_id,
                contact_id,
                webhook_event_id,
                1,  # message_count
                tokens_input,
                tokens_output,
                tokens_total,
                model_name,
                provider,
                estimated_cost,
            ),
        )
        
        usage_log_id = cur.fetchone()["id"]

        # 2. Update usage summary (for quick limit checks)
        update_summary = """
            INSERT INTO usage_summary (
                organization_id,
                current_period_messages,
                current_period_tokens,
                period_start,
                period_end,
                last_updated_at
            )
            SELECT 
                %s,
                1,
                %s,
                billing_period_start,
                billing_period_end,
                NOW()
            FROM organizations
            WHERE id = %s
            ON CONFLICT (organization_id) 
            DO UPDATE SET
                current_period_messages = usage_summary.current_period_messages + 1,
                current_period_tokens = usage_summary.current_period_tokens + %s,
                last_updated_at = NOW()
        """
        
        cur.execute(update_summary, (org_id, tokens_total, org_id, tokens_total))

        conn.commit()

        return {
            "success": True,
            "usage_log_id": usage_log_id,
            "tokens_used": tokens_total,
            "estimated_cost": float(estimated_cost),
            "message_count": 1,
        }

    except Exception as e:
        print(f"Usage Logging Error: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "error": str(e),
            "tokens_used": tokens_total,  # Still report even if logging failed
        }
    finally:
        if "cur" in locals():
            cur.close()
        if "conn" in locals():
            conn.close()


def _get_cost_per_1k_tokens(provider: str, model: str) -> float:
    """
    Returns estimated cost per 1000 tokens.
    
    These are simplified rates - in production, you'd want:
    - Separate input/output pricing
    - Cached pricing from a config table
    - Regular updates as pricing changes
    """
    
    # Pricing as of Dec 2024 (approximate)
    # IMPORTANT: Order from most specific to least specific to avoid substring matching issues
    # e.g., "gpt-4o-mini" must come before "gpt-4o" to avoid incorrect matches
    pricing = {
        "openai": {
            "gpt-4o-mini": 0.0002,
            "gpt-4o": 0.005,
            "gpt-4-turbo": 0.01,
            "gpt-3.5-turbo": 0.0015,
        },
        "google": {
            "gemini-3-flash-preview": 0.00025,
        },
        "anthropic": {
            "claude-3-opus": 0.015,
            "claude-3-sonnet": 0.003,
            "claude-3-haiku": 0.00025,
        },
    }

    provider_lower = provider.lower()
    model_lower = model.lower()

    # Match using substring - order matters! More specific models first
    if provider_lower in pricing:
        for model_key, cost in pricing[provider_lower].items():
            if model_key in model_lower:
                return cost
    
    # Fallback: conservative estimate
    return 0.001  # $1 per million tokens


def estimate_tokens_from_text(text: str) -> int:
    """
    Rough token estimation: ~4 characters per token.
    This is a simplified heuristic - in production, use tiktoken or similar.
    """
    return max(len(text) // 4, 1)

================================================
File: f/development/whatsapp_webhook_processor__flow/flow.yaml
================================================
summary: WhatsApp webhook processor
description: ''
value:
  modules:
    - id: step1
      value:
        type: script
        input_transforms:
          db_resource:
            type: static
            value: f/development/business_layer_db_postgreSQL
          message_id:
            type: javascript
            expr: flow_input.message_id
          user_name:
            type: javascript
            expr: flow_input.user_name
          user_phone:
            type: javascript
            expr: flow_input.user_phone
          whatsapp_phone_id:
            type: javascript
            expr: flow_input.phone_number_id
        is_trigger: false
        path: f/development/1_whatsapp_context_loading
    - id: step2
      value:
        type: script
        input_transforms:
          context_payload:
            type: javascript
            expr: results.step1
          default_provider:
            type: static
            value: google
          google_api_key:
            type: static
            value: <function call>
          openai_api_key:
            type: static
            value: ''
          user_message:
            type: javascript
            expr: flow_input.message_body
        is_trigger: false
        path: f/development/2_whatsapp_llm_processing
    - id: step3
      summary: Send reply to WhatsApp via Meta API
      value:
        type: script
        input_transforms:
          context_payload:
            type: javascript
            expr: results.step1
          llm_result:
            type: javascript
            expr: results.step2
          phone_number_id:
            type: javascript
            expr: flow_input.phone_number_id
        is_trigger: false
        path: f/development/3_1_send_reply_to_whatsapp
    - id: step4
      summary: Save chat history to database
      skip_if:
        expr: '!results.step3.success'
      value:
        type: script
        input_transforms:
          context_payload:
            type: javascript
            expr: results.step1
          db_resource:
            type: static
            value: f/development/business_layer_db_postgreSQL
          llm_result:
            type: javascript
            expr: results.step2
          user_message:
            type: javascript
            expr: flow_input.message_body
          send_result:
            type: javascript
            expr: results.step3
        is_trigger: false
        path: f/development/3_2_save_chat_history
    - id: step5
      summary: Log usage for billing and analytics
      skip_if:
        expr: '!results.step3.success'
      value:
        type: script
        input_transforms:
          context_payload:
            type: javascript
            expr: results.step1
          llm_result:
            type: javascript
            expr: results.step2
          db_resource:
            type: static
            value: f/development/business_layer_db_postgreSQL
          send_result:
            type: javascript
            expr: results.step3
        is_trigger: false
        path: f/development/3_3_log_usage
    - id: step6
      summary: Alert admin on LLM errors (rate limits, quota exceeded, etc.)
      skip_if:
        expr: '!results.step2.usage_info || !results.step2.usage_info.error'
      value:
        type: script
        input_transforms:
          error_message:
            type: javascript
            expr: results.step2.usage_info.error
          step_id:
            type: static
            value: step2
          error_name:
            type: javascript
            expr: "results.step2.usage_info.is_limit_error ? 'RATE_LIMIT_ERROR' : 'LLM_ERROR'"
          chatbot_id:
            type: javascript
            expr: results.step1.chatbot.id
          user_phone:
            type: javascript
            expr: flow_input.user_phone
          db_resource:
            type: static
            value: f/development/business_layer_db_postgreSQL
        is_trigger: false
        path: f/development/utils/alert_on_failure
  failure_module:
    id: failure
    value:
      type: script
      path: f/development/utils/alert_on_failure
      input_transforms:
        error_message:
          type: javascript
          expr: error.message
        step_id:
          type: javascript
          expr: error.step_id
        error_name:
          type: javascript
          expr: error.name
        chatbot_id:
          type: javascript
          expr: flow_input.phone_number_id || 'unknown'
        user_phone:
          type: javascript
          expr: flow_input.user_phone || 'unknown'
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  order:
    - phone_number_id
    - user_phone
    - message_body
    - user_name
  properties:
    message_body:
      type: string
      description: ''
      default: ''
    phone_number_id:
      type: string
      description: ''
      default: ''
    user_name:
      type: string
      description: ''
      default: ''
    user_phone:
      type: string
      description: ''
      default: ''
  required: []


================================================
File: tests/test_harness/windmill_mock.py
================================================
"""
Mock Windmill functions for testing.

This module mocks:
- wmill.get_resource() - Returns test database config
- wmill.get_variable() - Returns test API keys
"""

from typing import Dict, Any
import os


class WindmillMock:
    """Mock for Windmill SDK functions."""
    
    def __init__(self):
        self.resources = {
            "f/development/business_layer_db_postgreSQL": {
                "host": os.getenv("TEST_DB_HOST", "localhost"),
                "port": int(os.getenv("TEST_DB_PORT", "5434")),
                "user": os.getenv("TEST_DB_USER", "test_user"),
                "password": os.getenv("TEST_DB_PASSWORD", "test_password"),
                "dbname": os.getenv("TEST_DB_NAME", "test_business_logic"),
                "sslmode": "disable",
                "root_certificate_pem": "",  # Windmill adds this
            }
        }
        
        self.variables = {
            "u/admin/GoogleAPI_JD": os.getenv("GOOGLE_API_KEY", "test_google_key"),
            "u/admin/OpenAI_API_Key": os.getenv("OPENAI_API_KEY", "test_openai_key"),
        }
    
    def get_resource(self, path: str) -> Dict[str, Any]:
        """
        Mock wmill.get_resource().
        
        Args:
            path: Resource path (e.g., "f/development/db_resource")
        
        Returns:
            Resource configuration dict
        
        Raises:
            KeyError: If resource not found
        """
        if path not in self.resources:
            raise KeyError(f"Resource not found: {path}")
        
        return self.resources[path].copy()
    
    def get_variable(self, path: str) -> str:
        """
        Mock wmill.get_variable().
        
        Args:
            path: Variable path (e.g., "u/admin/api_key")
        
        Returns:
            Variable value
        
        Raises:
            KeyError: If variable not found
        """
        if path not in self.variables:
            raise KeyError(f"Variable not found: {path}")
        
        return self.variables[path]
    
    def set_resource(self, path: str, config: Dict[str, Any]):
        """Add or update a resource (for test setup)."""
        self.resources[path] = config.copy()
    
    def set_variable(self, path: str, value: str):
        """Add or update a variable (for test setup)."""
        self.variables[path] = value
    
    def clear(self):
        """Clear all mocked resources and variables."""
        self.resources.clear()
        self.variables.clear()
    
    def reset(self):
        """Reset to default test configuration."""
        self.__init__()

================================================
File: f/development/3_1_send_reply_to_whatsapp.script.lock
================================================
# py: 3.12
certifi==2025.11.12
charset-normalizer==3.4.4
idna==3.11
requests==2.32.5
urllib3==2.6.2

================================================
File: wmill-lock.yaml
================================================
version: v2
locks:
  f/development/1_whatsapp_context_loading: c231f6430873e6f4ef66a7c99c3a3a069a7406ccd121a4617b5b19b0266c8b34
  f/development/2_whatsapp_llm_processing: d5dec80764b5cc91757d1a4443c4f464ec94b973d6a8c98f6689ec7469120166
  f/development/3_1_send_reply_to_whatsapp: b66b4b85462963607cd91c5d42c9f0e55f97b3981300c1cb320cd47672492821
  f/development/3_2_save_chat_history: 5288ed02d9c23bdb4e9a4ad4211038c2e02fcc24f37615b9ed73c6151a83b05d
  f/development/3_3_log_usage: 3a5c4db27711da5f0c45be6a2486e58e99d99cafed58f9bbc0ecd3fb23fa65e5
  f/development/ingest_multiple_urls: 4ef4dea3dd8be1fdf2f27c402e6e93d7c0f2cbd006729fb6a2124d28ad68c32c
  f/development/RAG_process_documents: 518f4b97a815392da44afced949c7ae848409fc28d6b7073fa0eaf6c1743544f
  f/development/upload_document: a8d4d4a96d52bd7acb4420b2248e39ab6381b4359fb42ec7d8a0688273afbd30
  f/development/utils/alert_on_failure: 2c7fd9cbda89a9706d4550f8108adb206d59c55d2a2791ecd11da3e04a7cf9a5
  f/development/utils/check_knowledge_quota: 0f62820893f8c5720b41dd0c67794d4da0235a5df392a3675d423d596f9360ba
  f/development/utils/web_crawler: faf678a9e214155032f72d364c92710c27ac7468f95ba75170272aa2f3b73b73
  f/development/whatsapp-webhook: f201f7144a9df42f8d98c3b8501a7038e7c7437f6d4e53bfb520af551469cae8
  f/development/whatsapp-webhook-preprocessor: 8c6c845cb934ec09d72b6ffb60123bb5a0e8670ff88d989dfa83101aa6e66723


================================================
File: f/development/3_2_save_chat_history.script.yaml
================================================
summary: 3_2_save chat history
description: ''
lock: '!inline f/development/3_2_save_chat_history.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    context_payload:
      type: object
      description: ''
      default: null
      properties: {}
    db_resource:
      type: string
      description: ''
      default: f/development/business_layer_db_postgreSQL
      originalType: string
    llm_result:
      type: object
      description: ''
      default: null
      properties: {}
    send_result:
      type: object
      description: ''
      default: null
      properties: {}
    user_message:
      type: string
      description: ''
      default: null
      originalType: string
  required:
    - context_payload
    - user_message
    - llm_result
    - send_result


================================================
File: api-server/README.md
================================================
# Chatbot Knowledge Base API

REST API for managing chatbot knowledge bases with PDF/DOCX uploads, URL ingestion, web crawling, and RAG search.

## Base URL

```
http://localhost:4000
```

## Endpoints

### 1. Upload File (PDF/DOCX)

Upload a PDF or DOCX document to the knowledge base.

**Request:**
```http
POST /api/chatbots/:id/knowledge/upload
Content-Type: multipart/form-data

file: <binary file data>
```

**Response:**
```json
{
  "success": true,
  "knowledge_source_id": "550e8400-e29b-41d4-a716-446655440000",
  "job_id": "abc-123-def",
  "status": "pending",
  "quota": {
    "allowed": true,
    "current": 5,
    "max": 50,
    "remaining": 45
  }
}
```

**Example:**
```bash
curl -X POST http://localhost:4000/api/chatbots/YOUR_CHATBOT_ID/knowledge/upload \
  -F "file=@document.pdf"
```

**Status Codes:**
- `200 OK` - File uploaded successfully
- `400 Bad Request` - No file provided or invalid file type
- `403 Forbidden` - Quota exceeded
- `413 Payload Too Large` - File exceeds 10MB limit

---

### 2. Add Single URL

Add a single URL to the knowledge base.

**Request:**
```http
POST /api/chatbots/:id/knowledge/url
Content-Type: application/json

{
  "url": "https://example.com/page"
}
```

**Response:**
```json
{
  "success": true,
  "knowledge_source_id": "550e8400-e29b-41d4-a716-446655440001",
  "job_id": "xyz-789-ghi",
  "quota": {
    "allowed": true,
    "current": 2,
    "max": 20,
    "remaining": 18
  }
}
```

**Example:**
```bash
curl -X POST http://localhost:4000/api/chatbots/YOUR_CHATBOT_ID/knowledge/url \
  -H "Content-Type: application/json" \
  -d '{"url": "https://docs.example.com/faq"}'
```

---

### 3. Crawl Website

Discover relevant links from a base URL.

**Request:**
```http
POST /api/chatbots/:id/knowledge/crawl
Content-Type: application/json

{
  "baseUrl": "https://example.com",
  "maxDepth": 2,
  "maxPages": 50,
  "filterKeywords": ["faq", "docs", "support"]
}
```

**Response:**
```json
{
  "success": true,
  "discovered_urls": [
    {
      "url": "https://example.com/faq",
      "title": "Frequently Asked Questions",
      "relevance_score": 0.7,
      "depth": 1,
      "content_preview": "Q: How do I get started? A: First, create an account...",
      "suggested": true
    },
    {
      "url": "https://example.com/docs",
      "title": "Documentation",
      "relevance_score": 0.65,
      "depth": 1,
      "content_preview": "Welcome to our documentation. Here you will find...",
      "suggested": true
    }
  ],
  "total_discovered": 15,
  "crawl_time_seconds": 18.5,
  "base_domain": "example.com",
  "robots_txt_respected": true
}
```

**Example:**
```bash
curl -X POST http://localhost:4000/api/chatbots/YOUR_CHATBOT_ID/knowledge/crawl \
  -H "Content-Type: application/json" \
  -d '{
    "baseUrl": "https://docs.example.com",
    "maxDepth": 2,
    "maxPages": 30
  }'
```

**Parameters:**
- `baseUrl` (required) - Starting URL to crawl
- `maxDepth` (optional, default: 2) - Maximum crawl depth
- `maxPages` (optional, default: 50) - Maximum pages to discover
- `filterKeywords` (optional) - Keywords to boost relevance scoring

---

### 4. Ingest Batch URLs

Ingest multiple URLs discovered from crawling.

**Request:**
```http
POST /api/chatbots/:id/knowledge/ingest-batch
Content-Type: application/json

{
  "urls": [
    "https://example.com/faq",
    "https://example.com/docs",
    "https://example.com/support"
  ]
}
```

**Response:**
```json
{
  "success": true,
  "total_urls": 3,
  "successful": 3,
  "failed": 0,
  "results": [
    {
      "url": "https://example.com/faq",
      "success": true,
      "knowledge_source_id": "550e8400-e29b-41d4-a716-446655440002",
      "job_id": "job-123",
      "quota_info": {
        "allowed": true,
        "current": 3,
        "max": 20,
        "remaining": 17
      }
    },
    {
      "url": "https://example.com/docs",
      "success": true,
      "knowledge_source_id": "550e8400-e29b-41d4-a716-446655440003",
      "job_id": "job-124",
      "quota_info": {
        "allowed": true,
        "current": 4,
        "max": 20,
        "remaining": 16
      }
    }
  ],
  "timestamp": "2024-01-15T10:30:00.000Z"
}
```

**Example:**
```bash
curl -X POST http://localhost:4000/api/chatbots/YOUR_CHATBOT_ID/knowledge/ingest-batch \
  -H "Content-Type: application/json" \
  -d '{
    "urls": [
      "https://example.com/faq",
      "https://example.com/docs"
    ]
  }'
```

---

### 5. List Knowledge Sources

Get all knowledge sources for a chatbot with pagination and filtering.

**Request:**
```http
GET /api/chatbots/:id/knowledge/sources?page=1&limit=20&status=synced&sourceType=url
```

**Query Parameters:**
- `page` (optional, default: 1) - Page number
- `limit` (optional, default: 20) - Results per page
- `status` (optional) - Filter by status: `pending`, `processing`, `synced`, `failed`
- `sourceType` (optional) - Filter by type: `pdf`, `doc`, `url`

**Response:**
```json
{
  "success": true,
  "sources": [
    {
      "id": "550e8400-e29b-41d4-a716-446655440002",
      "source_type": "url",
      "source_url": "https://example.com/faq",
      "source_file_name": null,
      "status": "synced",
      "error_message": null,
      "file_size_bytes": 15360,
      "chunk_count": 12,
      "created_at": "2024-01-15T10:00:00.000Z",
      "updated_at": "2024-01-15T10:02:30.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 20,
    "total": 25,
    "totalPages": 2
  }
}
```

**Example:**
```bash
curl http://localhost:4000/api/chatbots/YOUR_CHATBOT_ID/knowledge/sources?page=1&limit=10
```

---

### 6. Get Source Status

Get detailed processing status for a specific knowledge source.

**Request:**
```http
GET /api/chatbots/:id/knowledge/sources/:sourceId/status
```

**Response:**
```json
{
  "success": true,
  "source_id": "550e8400-e29b-41d4-a716-446655440002",
  "status": "synced",
  "error_message": null,
  "chunk_count": 12,
  "processing_time_seconds": 8.5,
  "created_at": "2024-01-15T10:00:00.000Z",
  "updated_at": "2024-01-15T10:02:30.000Z"
}
```

**Example:**
```bash
curl http://localhost:4000/api/chatbots/YOUR_CHATBOT_ID/knowledge/sources/550e8400-e29b-41d4-a716-446655440002/status
```

**Status Values:**
- `pending` - Queued for processing
- `processing` - Currently being processed
- `synced` - Successfully processed and ready
- `failed` - Processing failed (see error_message)

---

### 7. Delete Knowledge Source

Delete a knowledge source and all its associated chunks.

**Request:**
```http
DELETE /api/chatbots/:id/knowledge/sources/:sourceId
```

**Response:**
```json
{
  "success": true,
  "message": "Knowledge source deleted successfully"
}
```

**Example:**
```bash
curl -X DELETE http://localhost:4000/api/chatbots/YOUR_CHATBOT_ID/knowledge/sources/550e8400-e29b-41d4-a716-446655440002
```

**Status Codes:**
- `200 OK` - Source deleted successfully
- `404 Not Found` - Source not found or doesn't belong to this chatbot

---

### 8. Test RAG Search

Test RAG search functionality to see what context would be retrieved.

**Request:**
```http
POST /api/chatbots/:id/knowledge/search
Content-Type: application/json

{
  "query": "How do I get started?",
  "limit": 5
}
```

**Response:**
```json
{
  "success": true,
  "query": "How do I get started?",
  "results": [],
  "message": "RAG search endpoint - implementation pending"
}
```

**Example:**
```bash
curl -X POST http://localhost:4000/api/chatbots/YOUR_CHATBOT_ID/knowledge/search \
  -H "Content-Type: application/json" \
  -d '{"query": "How do I reset my password?", "limit": 5}'
```

**Note:** This endpoint is currently a placeholder. RAG search will be fully implemented in a future update.

---

### 9. Get Quota Usage

Get current quota usage for the chatbot's organization.

**Request:**
```http
GET /api/chatbots/:id/knowledge/quota
```

**Response:**
```json
{
  "success": true,
  "quota": {
    "pdfs": {
      "current": 5,
      "max": 50,
      "remaining": 45
    },
    "urls": {
      "current": 12,
      "max": 20,
      "remaining": 8
    },
    "storage": {
      "current_mb": 45.3,
      "max_mb": 500,
      "remaining_mb": 454.7
    },
    "daily_ingestions": {
      "today": 8,
      "max": 100,
      "remaining": 92
    }
  }
}
```

**Example:**
```bash
curl http://localhost:4000/api/chatbots/YOUR_CHATBOT_ID/knowledge/quota
```

---

## Error Handling

All endpoints return errors in the following format:

```json
{
  "error": "Error message here",
  "stack": "Stack trace (only in development mode)"
}
```

### Common Error Codes

- `400 Bad Request` - Invalid input parameters
- `403 Forbidden` - Quota exceeded
- `404 Not Found` - Resource not found
- `413 Payload Too Large` - File size exceeds 10MB
- `500 Internal Server Error` - Server error

---

## Health Check

**Request:**
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:00:00.000Z",
  "service": "whatsapp-chatbot-server",
  "version": "1.0.0"
}
```

---

## Development

### Running Locally

```bash
cd api-server
npm install
npm run dev
```

### Environment Variables

Set these in your `.env` file:

```env
PORT=4000
NODE_ENV=development
DB_HOST=localhost
DB_PORT=5432
DB_USER=business_logic_user
DB_PASSWORD=business_logic_password
DB_NAME=business_logic_app
WINDMILL_URL=http://localhost:8000
WINDMILL_TOKEN=your_windmill_token
WINDMILL_WORKSPACE=development
```

### Running with Docker

```bash
docker-compose up whatsapp_chatbot_api
```

---

## Architecture

The API server acts as a gateway between the frontend and Windmill:

1. **File Upload**: API receives file → Calls Windmill `upload_document` → Returns job ID
2. **URL Ingestion**: API receives URLs → Calls Windmill `ingest_multiple_urls` → Returns job IDs
3. **Web Crawling**: API receives base URL → Calls Windmill `web_crawler` → Returns discovered URLs
4. **Status Checking**: API queries PostgreSQL directly for real-time status
5. **Quota Enforcement**: Middleware calls Windmill `check_knowledge_quota` before operations

All processing happens asynchronously in Windmill workers. The API provides immediate responses with job IDs for tracking.


================================================
File: f/development/RAG_process_documents.script.lock
================================================
# py: 3.12
annotated-types==0.7.0
anyio==4.12.0
certifi==2025.11.12
distro==1.9.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.11
jiter==0.12.0
openai==2.14.0
psycopg2-binary==2.9.11
pydantic==2.12.5
pydantic-core==2.41.5
sniffio==1.3.1
tqdm==4.67.1
typing-extensions==4.15.0
typing-inspection==0.4.2
wmill==1.601.1

================================================
File: f/development/business_layer_db_postgreSQL.resource.yaml
================================================
description: Business logic database
value:
  dbname: business_logic_app
  host: business_logic_db
  password: business_logic_password
  port: 5432
  root_certificate_pem: ''
  sslmode: disable
  user: business_logic_user
resource_type: postgresql


================================================
File: f/development/utils/alert_on_failure.script.yaml
================================================
summary: ''
description: ''
lock: '!inline f/development/utils/alert_on_failure.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    chatbot_id:
      type: string
      description: ''
      default: null
      originalType: string
    db_resource:
      type: string
      description: ''
      default: f/development/business_layer_db_postgreSQL
      originalType: string
    error_message:
      type: string
      description: ''
      default: null
      originalType: string
    error_name:
      type: string
      description: ''
      default: null
      originalType: string
    slack_webhook_url:
      type: string
      description: ''
      default: <function call>
      originalType: string
    step_id:
      type: string
      description: ''
      default: null
      originalType: string
    user_phone:
      type: string
      description: ''
      default: null
      originalType: string
  required:
    - error_message
    - step_id


================================================
File: mcp-servers/lead-capture/package.json
================================================
{
  "name": "lead-capture-mcp",
  "version": "1.0.0",
  "description": "MCP server for capturing and managing sales leads",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "nodemon server.js"
  },
  "keywords": ["mcp", "leads", "crm", "sales"],
  "author": "JD Labs",
  "license": "MIT",
  "dependencies": {
    "express": "^4.18.2",
    "pg": "^8.11.3"
  },
  "devDependencies": {
    "nodemon": "^3.0.1"
  }
}


================================================
File: mcp-servers/pricing-calculator/server.js
================================================
const express = require('express');
const app = express();
const PORT = process.env.PORT || 3001;

app.use(express.json());

// Health check endpoint
app.get('/health', (req, res) => {
  res.json({ status: 'healthy', service: 'pricing-calculator' });
});

// Pricing tiers for WhatsApp chatbot product
const PRICING_TIERS = {
  basic: {
    name: 'Básico',
    base_price: 299, // MXN per month
    included_messages: 1000,
    price_per_extra_message: 0.30,
    features: ['Respuestas automáticas 24/7', 'Hasta 1,000 mensajes/mes', 'Soporte por email']
  },
  professional: {
    name: 'Profesional',
    base_price: 799,
    included_messages: 5000,
    price_per_extra_message: 0.20,
    features: ['Todo lo del plan Básico', 'Hasta 5,000 mensajes/mes', 'Integraciones con CRM', 'Soporte prioritario']
  },
  enterprise: {
    name: 'Empresarial',
    base_price: 1999,
    included_messages: 20000,
    price_per_extra_message: 0.15,
    features: ['Todo lo del plan Profesional', 'Hasta 20,000 mensajes/mes', 'API personalizada', 'Gerente de cuenta dedicado', 'SLA garantizado']
  }
};

// MCP tool endpoint - calculate pricing
app.post('/tools/calculate_pricing', (req, res) => {
  try {
    const { message_volume, tier } = req.body;

    // Validate inputs
    if (!message_volume || typeof message_volume !== 'number') {
      return res.status(400).json({
        error: 'message_volume es requerido y debe ser un número'
      });
    }

    const selectedTier = tier || 'basic';
    const pricingTier = PRICING_TIERS[selectedTier];

    if (!pricingTier) {
      return res.status(400).json({
        error: `Plan no válido. Opciones: ${Object.keys(PRICING_TIERS).join(', ')}`
      });
    }

    // Calculate pricing
    const included = pricingTier.included_messages;
    const extra_messages = Math.max(0, message_volume - included);
    const extra_cost = extra_messages * pricingTier.price_per_extra_message;
    const total_cost = pricingTier.base_price + extra_cost;

    // Format response in Spanish
    const response = {
      plan: pricingTier.name,
      precio_base: `$${pricingTier.base_price.toFixed(2)} MXN/mes`,
      mensajes_incluidos: included.toLocaleString('es-MX'),
      mensajes_solicitados: message_volume.toLocaleString('es-MX'),
      mensajes_extra: extra_messages.toLocaleString('es-MX'),
      costo_extra: `$${extra_cost.toFixed(2)} MXN`,
      costo_total: `$${total_cost.toFixed(2)} MXN/mes`,
      caracteristicas: pricingTier.features,
      detalles: extra_messages > 0
        ? `Incluye ${included.toLocaleString('es-MX')} mensajes. ${extra_messages.toLocaleString('es-MX')} mensajes adicionales a $${pricingTier.price_per_extra_message} MXN cada uno.`
        : `Incluye hasta ${included.toLocaleString('es-MX')} mensajes por mes.`
    };

    res.json(response);
  } catch (error) {
    console.error('Error calculating pricing:', error);
    res.status(500).json({
      error: 'Error al calcular el precio',
      details: error.message
    });
  }
});

// MCP tool endpoint - list available plans
app.post('/tools/list_plans', (req, res) => {
  try {
    const plans = Object.keys(PRICING_TIERS).map(key => {
      const tier = PRICING_TIERS[key];
      return {
        id: key,
        nombre: tier.name,
        precio: `$${tier.base_price} MXN/mes`,
        mensajes_incluidos: tier.included_messages.toLocaleString('es-MX'),
        caracteristicas: tier.features
      };
    });

    res.json({
      planes_disponibles: plans,
      moneda: 'MXN',
      nota: 'Todos los precios en pesos mexicanos. Mensajes adicionales disponibles según plan.'
    });
  } catch (error) {
    console.error('Error listing plans:', error);
    res.status(500).json({
      error: 'Error al listar planes',
      details: error.message
    });
  }
});

// MCP tool discovery endpoint
app.get('/tools', (req, res) => {
  res.json({
    tools: [
      {
        name: 'calculate_pricing',
        description: 'Calcula el costo del chatbot de WhatsApp según el volumen de mensajes y plan seleccionado',
        input_schema: {
          type: 'object',
          properties: {
            message_volume: {
              type: 'number',
              description: 'Número estimado de mensajes por mes'
            },
            tier: {
              type: 'string',
              enum: ['basic', 'professional', 'enterprise'],
              description: 'Plan deseado (básico, profesional o empresarial)',
              default: 'basic'
            }
          },
          required: ['message_volume']
        }
      },
      {
        name: 'list_plans',
        description: 'Lista todos los planes disponibles con sus precios y características',
        input_schema: {
          type: 'object',
          properties: {}
        }
      }
    ]
  });
});

app.listen(PORT, () => {
  console.log(`Pricing Calculator MCP server running on port ${PORT}`);
  console.log(`Health check: http://localhost:${PORT}/health`);
  console.log(`Tool discovery: http://localhost:${PORT}/tools`);
});


================================================
File: tests/unit/test_step2_gemini_tool_calling.py
================================================
"""
Unit tests for Step 2: Gemini tool calling

Tests Step 2's ability to:
- Call Gemini with tools
- Parse tool call arguments from protobuf
- Execute MCP tools
- Return responses
"""

import pytest
import sys
import os
from unittest.mock import Mock, patch, MagicMock, PropertyMock
from google.protobuf.struct_pb2 import Struct
from collections import namedtuple

# Add parent directories to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../f/development'))

# Mock required modules before importing step2
mock_wmill = Mock()
mock_wmill.get_variable.return_value = "fake_google_api_key"
sys.modules['wmill'] = mock_wmill

# Mock Google GenAI SDK
mock_genai = Mock()
mock_genai_types = Mock()
sys.modules['google.genai'] = mock_genai
sys.modules['google.genai.types'] = mock_genai_types

# Import the module under test
import importlib.util
spec = importlib.util.spec_from_file_location(
    "step2",
    os.path.join(os.path.dirname(__file__), '../../f/development/2_whatsapp_llm_processing.py')
)
step2_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(step2_module)
step2_main = step2_module.main


# Simple class to hold usage metadata
UsageMetadata = namedtuple('UsageMetadata', ['prompt_token_count', 'candidates_token_count'])


class TestGeminiToolCalling:
    """Test Gemini's tool calling functionality"""

    def test_gemini_tool_call_with_pricing_calculator(self):
        """Test that Gemini can call the pricing calculator tool"""

        # Create mock for function call arguments (protobuf Struct)
        mock_args = Struct()
        mock_args.update({
            "message_volume": 3000,
            "tier": "basic"
        })

        # Create mock function call
        mock_function_call = Mock()
        mock_function_call.name = "calculate_pricing"
        mock_function_call.args = mock_args

        # Create mock part with function call
        mock_part = Mock()
        mock_part.function_call = mock_function_call

        # Mock first response: model wants to call tool
        mock_response_1 = Mock()
        mock_candidate_1 = Mock()
        mock_candidate_1.content = Mock()
        mock_candidate_1.content.parts = [mock_part]
        mock_response_1.candidates = [mock_candidate_1]
        # Create usage metadata with real integers
        mock_response_1.usage_metadata = UsageMetadata(
            prompt_token_count=100,
            candidates_token_count=50
        )

        # Mock second response: final answer after tool execution
        mock_text_part = Mock(spec=['text'])
        mock_text_part.text = "Para 3,000 mensajes al mes con el plan Básico, el costo sería $899 MXN/mes."
        # Make sure hasattr(part, 'function_call') returns False
        delattr(mock_text_part, 'function_call') if hasattr(mock_text_part, 'function_call') else None

        mock_candidate_2 = Mock()
        mock_candidate_2.content = Mock()
        mock_candidate_2.content.parts = [mock_text_part]

        mock_response_2 = Mock()
        mock_response_2.text = "Para 3,000 mensajes al mes con el plan Básico, el costo sería $899 MXN/mes."
        mock_response_2.candidates = [mock_candidate_2]
        # Create usage metadata with real integers
        mock_response_2.usage_metadata = UsageMetadata(
            prompt_token_count=150,
            candidates_token_count=80
        )

        # Setup mock GenAI Client
        mock_client = Mock()
        mock_models = Mock()
        mock_models.generate_content = Mock(side_effect=[mock_response_1, mock_response_2])
        mock_client.models = mock_models

        # Patch genai.Client to return our mock client
        with patch.object(mock_genai, 'Client', return_value=mock_client), \
             patch('requests.post') as mock_post:
            mock_mcp_response = Mock()
            mock_mcp_response.ok = True
            mock_mcp_response.json.return_value = {
                "plan": "Básico",
                "precio_base": "$299.00 MXN/mes",
                "mensajes_incluidos": "1,000",
                "mensajes_solicitados": "3,000",
                "mensajes_extra": "2,000",
                "costo_extra": "$600.00 MXN",
                "costo_total": "$899.00 MXN/mes"
            }
            mock_post.return_value = mock_mcp_response

            # Call function with Gemini + tools (matching database format from Step 1)
            result = step2_main(
                context_payload={
                    "proceed": True,
                    "chatbot": {
                        "id": "test-chatbot-id",
                        "organization_id": "test-org-id",
                        "model_name": "gemini-3-flash-preview",
                        "system_prompt": "Eres un representante de ventas para JD Labs.",
                        "persona": "Hablas en español, eres conciso.",
                        "temperature": 0.7,
                        "rag_enabled": False
                    },
                    "user": {"id": "test-user-id"},
                    "history": [],
                    "tools": [{
                        "integration_id": "test-integration-id",
                        "provider": "mcp",  # Changed from mcp_tool to mcp to match prepare_tool_definitions
                        "name": "calculate_pricing",
                        "config": {
                            "type": "mcp_server",
                            "server_url": "http://mcp_pricing_calculator:3001",
                            "description": "Calcula precios del chatbot de WhatsApp según volumen de mensajes",
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    "message_volume": {"type": "number", "description": "Número de mensajes al mes"},
                                    "tier": {"type": "string", "description": "Tier del plan (basic, professional, enterprise)"}
                                },
                                "required": ["message_volume"]
                            }
                        },
                        "credentials": None
                    }]
                },
                user_message="Que tal, cuánto me costarian 3000 mensajes al mes?",
                google_api_key="fake_key",
                default_provider="google"
            )

            # Assertions
            assert "error" not in result, f"Expected no error, got: {result.get('error')}"
            assert result["reply_text"] == "Para 3,000 mensajes al mes con el plan Básico, el costo sería $899 MXN/mes."
            assert len(result["tool_executions"]) == 1
            assert result["tool_executions"][0]["tool_name"] == "calculate_pricing"
            assert result["tool_executions"][0]["arguments"]["message_volume"] == 3000
            assert result["usage_info"]["provider"] == "google"
            assert result["usage_info"]["tool_calls"] == 1

    def test_protobuf_struct_conversion(self):
        """Test that we can properly convert protobuf Struct to dict"""
        from google.protobuf.struct_pb2 import Struct
        from google.protobuf.json_format import MessageToDict

        # Create a protobuf Struct with various types
        mock_args = Struct()
        mock_args.update({
            "message_volume": 3000,
            "tier": "basic",
            "enabled": True,
            "tags": ["sales", "premium"]
        })

        # Convert to dict
        result = MessageToDict(mock_args)

        # Verify conversion
        assert result["message_volume"] == 3000
        assert result["tier"] == "basic"
        assert result["enabled"] is True
        assert result["tags"] == ["sales", "premium"]


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
File: README.md
================================================
# Multi-Tenant WhatsApp Chatbot Platform

A production-ready, multi-tenant SaaS platform for WhatsApp chatbots with RAG (Retrieval-Augmented Generation), agentic tool calling, and comprehensive monitoring.

## 🎯 Project Overview

This platform enables organizations to create and manage AI-powered WhatsApp chatbots with:
- **RAG Knowledge Base**: Upload PDFs, URLs, documents for context-aware responses
- **Agentic Capabilities**: Tool calling (MCP, Windmill scripts, built-in tools)
- **Multi-Tenancy**: Organizations with multiple admins and chatbots
- **Usage Tracking**: Token and message limits with automatic quota enforcement
- **Production Monitoring**: Slack alerts, database logging, failure handling

## 🏗️ Architecture

### Technology Stack
- **Orchestration**: Windmill (self-hosted, port 8081)
- **Database**: PostgreSQL with pgvector extension
- **Webhook Server**: Node.js Express (port 3000)
- **LLM Providers**: Google Gemini, OpenAI
- **Vector Search**: pgvector with HNSW indexing
- **Storage**: Windmill S3 integration

### Flow Architecture

```
WhatsApp → Node.js Webhook → Windmill Flow → PostgreSQL → LLM → Response
                                     ↓
                          [Step 1: Context Loading]
                                     ↓
                          [Step 2: Agent Loop + RAG]
                                     ↓
                       [Step 3: Parallel Actions]
                        ├─ 3a: Send WhatsApp Reply
                        ├─ 3b: Save Chat History
                        └─ 3c: Log Usage for Billing
```

## 📂 Project Structure

```
src/
├── docker-compose.yml              # Main services (Windmill, Postgres, Nginx)
├── docker-compose.test.yml         # Test database
├── webhook-server/                 # Node.js WhatsApp webhook handler
│   ├── app.js                      # Express server (Meta verification)
│   └── Dockerfile
├── db/
│   ├── create.sql                  # Complete database schema
│   ├── seed.sql                    # Sample data
│   └── migrations/                 # Database migrations
│       ├── 001_add_tool_tables.sql      # Tool execution tracking
│       └── 002_add_usage_triggers.sql   # Quota enforcement
├── f/development/                  # Windmill scripts
│   ├── 1_whatsapp_context_loading.py    # Step 1: Load context
│   ├── 2_whatsapp_llm_processing.py     # Step 2: Agent loop + RAG
│   ├── 3_1_send_reply_to_whatsapp.py    # Step 3a: Send reply
│   ├── 3_2_save_chat_history.py         # Step 3b: Save history
│   ├── 3_3_log_usage.py                 # Step 3c: Log usage
│   ├── RAG_process_documents.py         # Document processing
│   ├── upload_document.py               # Document upload flow
│   ├── whatsapp_webhook_processor__flow/
│   │   └── flow.yaml                    # Main flow definition
│   └── utils/
│       └── alert_on_failure.py          # Failure monitoring
├── tests/                          # Comprehensive test suite
│   ├── conftest.py                 # Pytest fixtures
│   ├── test_harness/               # Mock services
│   │   ├── windmill_mock.py
│   │   ├── llm_mock.py
│   │   └── whatsapp_mock.py
│   ├── unit/                       # Unit tests
│   └── integration/                # Integration tests
└── docs/                           # Documentation
    ├── ARCHITECTURE.md             # System design
    ├── QUICK_REFERENCE.md          # Developer guide
    └── RAG_IMPLEMENTATION_GUIDE.md # RAG setup guide
```

## 🚀 Quick Start

### Prerequisites
- Docker & Docker Compose
- Node.js 18+ (for webhook server)
- PostgreSQL 16+ with pgvector extension
- OpenAI API key (for embeddings)
- Google Gemini API key (for LLM)
- WhatsApp Business Account & Meta App

### 1. Environment Setup

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your credentials
nano .env
```

Required environment variables: refer to .env.example
```

### 2. Start Services

```bash
# Start all services
docker-compose up -d

# Check logs
docker-compose logs -f

# Windmill will be available at http://localhost:8081
# Webhook server at http://localhost:3000
```

### 3. Initialize Database (FOR DEVELOPMENT ONLY)

For any database operations in development, use /db/manage_db.sh, refer to this extract from the script:
```bash
    echo "Usage: db/manage_db.sh [OPTIONS] <COMMAND>"
    echo ""
    echo "Options:"
    echo "  --test  - Target the test database (test_business_logic_db)"
    echo "  --dev   - Target the dev database (business_logic_db) [default]"
    echo ""
    echo "Commands:"
    echo "  create  - Create database schema"
    echo "  seed    - Insert seed data (requires WHATSAPP_PHONE_NUMBER_ID and WHATSAPP_ACCESS_TOKEN in .env for dev DB)"
    echo "  drop    - Drop all tables (with confirmation)"
    echo "  reset   - Drop, create, and seed database (full reset)"
    echo "  verify  - Show all tables, their structure, and sample data"
    echo ""
    echo "Examples:"
    echo "  ./manage_db.sh reset             # Reset dev database"
    echo "  ./manage_db.sh --test reset      # Reset test database"
```

### 4. Configure WhatsApp Webhook

1. In Meta Developer Console, set webhook URL: `https://your-domain.com/webhook`
2. Set verify token (must match `WHATSAPP_VERIFY_TOKEN`)
3. Subscribe to messages webhook

### 5. Test the Flow

```bash
# Send test webhook
curl -X POST http://localhost:3000/webhook \
  -H "Content-Type: application/json" \
  -d '{
    "entry": [{
      "changes": [{
        "value": {
          "messages": [{
            "from": "+1234567890",
            "id": "test123",
            "text": {"body": "Hello!"},
            "type": "text"
          }],
          "metadata": {
            "phone_number_id": "your-phone-number-id"
          }
        }
      }]
    }]
  }'
```

## 🔧 Key Features Implementation

### 1. Agent Loop with Tool Calling

Located in `f/development/2_whatsapp_llm_processing.py`

**Features:**
- Max 5 iterations per message
- Support for 3 tool types:
  - **Built-in**: RAG search (search_knowledge_base)
  - **MCP**: External HTTP servers
  - **Windmill**: Internal scripts
- Automatic token tracking across iterations
- Graceful degradation on tool failures

**Usage:**
```python
# Tools are loaded from chatbot_integrations table
# Agent automatically decides when to use tools
# Example: Agent searches knowledge base, then calls custom tool
```

### 2. RAG Document Processing

**Upload Flow** (`f/development/upload_document.py`):
```python
# Upload PDF
result = upload_document(
    chatbot_id="uuid",
    source_type="pdf",
    name="Product Manual",
    file_content="base64_encoded_pdf"
)
# Returns: {"success": True, "source_id": "...", "processing_job_id": "..."}

# Upload URL
result = upload_document(
    chatbot_id="uuid",
    source_type="url",
    name="FAQ Page",
    url="https://example.com/faq"
)
```

**Processing Pipeline** (`RAG_process_documents.py`):
1. Extract content (PDF/URL/DOC)
2. Smart chunking (1000 chars, 200 overlap)
3. Generate embeddings (OpenAI ada-002)
4. Store in pgvector
5. Update status to 'synced'

**Search:**
```python
# Automatic via agent loop
# Or manual via retrieve_knowledge()
chunks = retrieve_knowledge(
    chatbot_id="uuid",
    query="What is your return policy?",
    openai_api_key="sk-...",
    top_k=5,
    similarity_threshold=0.7
)
```

### 3. Usage Quota Enforcement

**Automatic Enforcement:**
- Triggers on `usage_summary` updates
- Checks message and token limits
- Auto-disables chatbots when exceeded
- Sends Slack alerts
- Warning at 80% threshold

**Manual Reset:**
```sql
-- Reset billing period for all organizations
SELECT * FROM reset_billing_period();

-- Check usage for specific org
SELECT
    o.name,
    us.current_period_messages,
    o.message_limit_monthly,
    (us.current_period_messages::float / o.message_limit_monthly) * 100 as usage_percent
FROM organizations o
JOIN usage_summary us ON o.id = us.organization_id
WHERE o.id = 'your-org-id';
```

### 4. Failure Monitoring

**Slack Alerts** (`utils/alert_on_failure.py`):
- Color-coded by severity (critical/error/warning/info)
- Includes: flow ID, step ID, chatbot ID, error message
- Automatic severity detection
- Database logging in `system_alerts`

**Configure in flow.yaml:**
```yaml
failure_module:
  id: failure
  value:
    type: script
    path: f/development/utils/alert_on_failure
    input_transforms:
      error_message:
        type: javascript
        expr: error.message
      step_id:
        type: javascript
        expr: error.step_id
```

## 📊 Database Schema Highlights

### Core Tables
- **organizations**: Tenants with billing info and quotas
- **users**: Dashboard access (owner/admin/member roles)
- **chatbots**: AI agents with WhatsApp phone numbers
- **contacts**: End-users with conversation history
- **messages**: Chat history with tool calls
- **knowledge_sources**: Uploaded documents
- **document_chunks**: pgvector storage (1536 dims)
- **tool_executions**: Agent tool call tracking
- **usage_logs**: Billing data
- **system_alerts**: Monitoring events

### Key Functions
- `search_knowledge_base(chatbot_id, query_embedding, top_k, threshold)`: Vector search
- `get_current_usage(org_id)`: Current billing period usage
- `check_usage_limits()`: Quota enforcement trigger
- `reset_billing_period()`: Monthly billing reset

## 🧪 Testing

### Run Tests
```bash
# Install test dependencies
pip install -r tests/requirements.txt

# Start test database
docker-compose -f docker-compose.test.yml up -d

# Run all tests
pytest tests/

# Run specific test suite
pytest tests/unit/test_step1_context_loading.py -v

# With coverage
pytest --cov=f/development --cov-report=html
```

### Test Structure
- **Unit Tests**: Individual script testing with mocks
- **Integration Tests**: End-to-end flow testing
- **Test Harness**: Mock Windmill, LLM, WhatsApp APIs

## 🛠️ Development

### Adding a New Tool

1. **Create tool in database:**
```sql
INSERT INTO org_integrations (
    organization_id,
    provider,
    name,
    description,
    credentials,
    settings
) VALUES (
    'org-uuid',
    'mcp',
    'hubspot_lookup',
    'Look up contact in HubSpot',
    '{"api_key": "encrypted_key"}',
    '{"mcp_server_url": "https://mcp-hubspot.example.com"}'
);

INSERT INTO chatbot_integrations (
    chatbot_id,
    integration_id,
    enabled,
    tool_config
) VALUES (
    'chatbot-uuid',
    integration_id,
    true,
    '{"parameters": {"type": "object", "properties": {"email": {"type": "string"}}}}'
);
```

2. **Tool is automatically available to agent** - no code changes needed!

### Adding a Document Type

1. **Add extraction function** in `RAG_process_documents.py`:
```python
def extract_new_type(file_path: str) -> str:
    # Your extraction logic
    return extracted_text
```

2. **Update `extract_content()` switch**:
```python
elif source_type == "new_type":
    return extract_new_type(source["file_path"])
```

3. **Update upload flow** to accept new type

## 📝 API Reference

### Windmill Scripts

**1_whatsapp_context_loading.py**
```python
main(
    whatsapp_phone_id: str,    # Meta phone number ID
    user_phone: str,           # User's WhatsApp number
    user_name: str,            # User's display name
    db_resource: str
) -> Dict
```

**2_whatsapp_llm_processing.py**
```python
main(
    context_payload: dict,     # From Step 1
    user_message: str,         # User's message
    openai_api_key: str,
    google_api_key: str,
    default_provider: str,
    db_resource: str
) -> Dict
```

**upload_document.py**
```python
main(
    chatbot_id: str,
    source_type: str,          # "pdf" | "url" | "text" | "doc"
    name: str,                 # Display name
    file_content: str = None,  # Base64 encoded
    url: str = None,           # For URL type
    openai_api_key: str,
    db_resource: str
) -> Dict
```

## 🔐 Security Considerations

### Implemented
- ✅ Webhook signature verification (Node.js server)
- ✅ Database row-level security via foreign keys
- ✅ API key storage in Windmill variables (encrypted)
- ✅ HTTPS for all external API calls
- ✅ SQL injection prevention (parameterized queries)
- ✅ File size limits (10MB)
- ✅ Tool execution timeouts (30s)
- ✅ Quota enforcement to prevent abuse

### Recommended for Production
- [ ] Encrypt credentials in org_integrations table
- [ ] Add rate limiting to webhook endpoint
- [ ] Implement audit logging for sensitive operations
- [ ] Add data retention/deletion policies
- [ ] Enable SSL/TLS for database connections
- [ ] Implement backup and disaster recovery
- [ ] Add WAF for webhook endpoint
- [ ] Implement secret rotation

## 📈 Monitoring & Maintenance

### Daily Tasks
- Check `system_alerts` table for critical alerts
- Monitor Slack channel for failure notifications
- Review `recent_tool_failures` view

### Weekly Tasks
- Analyze `tool_usage_stats` view
- Check database disk usage
- Review slow query logs

### Monthly Tasks
- Run `reset_billing_period()` (or schedule via cron)
- Backup database
- Review and archive old webhook_events (7+ days)
- Update dependencies

### Monitoring Queries

```sql
-- Today's failures
SELECT * FROM system_alerts
WHERE severity IN ('critical', 'error')
  AND created_at > NOW() - INTERVAL '24 hours'
ORDER BY created_at DESC;

-- Top chatbots by usage
SELECT
    c.name,
    COUNT(*) as message_count,
    SUM(tokens_total) as total_tokens
FROM usage_logs ul
JOIN chatbots c ON ul.chatbot_id = c.id
WHERE ul.created_at > NOW() - INTERVAL '7 days'
GROUP BY c.id, c.name
ORDER BY message_count DESC
LIMIT 10;

-- Tool failure rate
SELECT
    tool_name,
    tool_type,
    COUNT(*) as total,
    COUNT(*) FILTER (WHERE status = 'failed') as failures,
    (COUNT(*) FILTER (WHERE status = 'failed')::float / COUNT(*)) * 100 as failure_rate
FROM tool_executions
WHERE created_at > NOW() - INTERVAL '7 days'
GROUP BY tool_name, tool_type
ORDER BY failure_rate DESC;
```

## 🚧 Roadmap

### Completed ✅
- [x] Multi-tenant database schema
- [x] WhatsApp webhook integration
- [x] 3-step processing flow
- [x] Agent loop with tool calling
- [x] RAG document processing (PDF, URL, DOC)
- [x] pgvector search
- [x] Usage tracking and quota enforcement
- [x] Slack alerting
- [x] Tool execution tracking
- [x] Comprehensive test harness

### In Progress 🚧
- [ ] Google Docs integration
- [ ] Advanced RAG (hybrid search, re-ranking)
- [ ] Dashboard UI for management
- [ ] Real-time WebSocket for live monitoring
- [ ] Multi-language support

### Future 🔮
- [ ] WhatsApp media handling (images, voice notes)
- [ ] Visual flow builder
- [ ] Custom embedding models
- [ ] A/B testing for prompts
- [ ] Analytics dashboard
- [ ] Multi-model support (Claude, Llama)
- [ ] Conversation flow templates

## 🤝 Contributing

This is a proprietary project. For internal development:

1. Create feature branch from `master`
2. Follow naming: `feature/short-description` or `fix/issue-description`
3. Write tests for new features
4. Update documentation
5. Create PR with detailed description

## 📄 License

Proprietary - All Rights Reserved

## 🆘 Support

For issues and questions:
1. Check documentation in `/docs`
2. Review `system_alerts` table
3. Check Slack alerts channel
4. Contact development team

---

**Built with ❤️ for scalable WhatsApp automation**


================================================
File: f/development/1_whatsapp_context_loading.script.yaml
================================================
summary: 1 Whatsapp context loading
description: Step 1 in whatsapp webhook flow - extract context from business database
lock: '!inline f/development/1_whatsapp_context_loading.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    db_resource:
      type: string
      description: ''
      default: f/development/business_layer_db_postgreSQL
      originalType: string
    message_id:
      type: string
      description: ''
      default: null
      originalType: string
    user_name:
      type: string
      description: ''
      default: Unknown
      originalType: string
    user_phone:
      type: string
      description: ''
      default: null
      originalType: string
    whatsapp_phone_id:
      type: string
      description: ''
      default: null
      originalType: string
  required:
    - whatsapp_phone_id
    - user_phone
    - message_id


================================================
File: docs/DEVELOPMENT_WORKFLOW.md
================================================
# Development Workflow

## Database Schema Changes

**CRITICAL:** Whenever you modify the database schema (create.sql, seed.sql), you MUST:

1. **Reset the database:**
   ```bash
   ./db/manage_db.sh reset
   ```

2. **Verify your changes:**
   - Check that all new tables/columns exist
   - Verify triggers are created
   - Confirm seed data loaded correctly
   - Test any new database functions

3. **Common verification queries:**
   ```sql
   -- List all tables
   \dt

   -- Describe a table
   \d table_name

   -- List all triggers
   SELECT trigger_name, event_manipulation, event_object_table
   FROM information_schema.triggers;

   -- List all extensions
   SELECT * FROM pg_extension;
   ```

## Windmill Script Changes

After creating or modifying scripts:

1. **Generate metadata:**
   ```bash
   wmill script generate-metadata
   ```

2. **For flow inline scripts:**
   ```bash
   wmill flow generate-locks --yes
   ```

3. **Push to Windmill (if needed):**
   ```bash
   wmill sync push
   ```

## Docker Container Changes

When modifying Dockerfiles or docker-compose.yml:

1. **Rebuild specific service:**
   ```bash
   docker-compose up --build service_name
   ```

2. **Rebuild all:**
   ```bash
   docker-compose up --build
   ```

3. **Clean rebuild (remove volumes):**
   ```bash
   docker-compose down -v
   docker-compose up --build
   ```

## Git Workflow

### Commit Frequency
- Commit after completing each logical unit of work
- Commit after database schema changes
- Commit after creating new files/components
- Commit after successful test runs
- Commit after completing each task within a phase

### Commit Message Format
```
[Phase X.Y] Brief description

Detailed changes:
- What was added/modified
- Why the change was made
- Any related files

Technical details:
- Implementation notes
- Breaking changes
- Migration notes

Issue: #N/A (or issue number)
```

### Branch Strategy
```
master (production-ready code)
  └── feature/rag-knowledge-base (main development branch)
       ├── feature/rag-plan-limits (Phase 1)
       ├── feature/rag-web-crawler (Phase 2)
       └── ... (other phase branches)
```

## Testing Workflow

### After Database Changes
1. Reset database: `./db/manage_db.sh reset`
2. Verify schema changes with SQL queries
3. Test seed data loaded correctly
4. Run integration tests

### After API Changes
1. Rebuild API container
2. Test health endpoint: `curl http://localhost:4000/health`
3. Test each modified endpoint with curl
4. Verify error handling

### After Windmill Script Changes
1. Generate metadata
2. Test script execution via Windmill UI or CLI
3. Verify error handling
4. Check logs for issues

## Pre-Commit Checklist

- [ ] Database changes verified with `./db/manage_db.sh reset`
- [ ] Windmill metadata generated
- [ ] Docker containers build successfully
- [ ] No syntax errors in code
- [ ] No secrets in committed files
- [ ] Tests passing
- [ ] Documentation updated

## Common Issues

### Database Connection Failed
**Cause:** Database container not running or wrong credentials
**Solution:**
```bash
docker-compose ps
docker-compose logs business_logic_db
```

### Windmill Script Not Found
**Cause:** Metadata not generated or script not synced
**Solution:**
```bash
wmill script generate-metadata
wmill sync push
```

### Docker Build Fails
**Cause:** Missing dependencies or syntax errors in Dockerfile
**Solution:**
- Check Dockerfile syntax
- Review build logs
- Try clean rebuild: `docker-compose down && docker-compose up --build`

### npm ci Fails
**Cause:** Missing package-lock.json
**Solution:** Use `npm install --production` instead of `npm ci`


================================================
File: f/development/RAG_process_documents.script.yaml
================================================
summary: ''
description: ''
lock: '!inline f/development/RAG_process_documents.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    chatbot_id:
      type: string
      description: ''
      default: null
      originalType: string
    chunk_overlap:
      type: integer
      description: ''
      default: 200
    chunk_size:
      type: integer
      description: ''
      default: 1000
    db_resource:
      type: string
      description: ''
      default: f/development/business_layer_db_postgreSQL
      originalType: string
    knowledge_source_id:
      type: string
      description: ''
      default: null
      originalType: string
    openai_api_key:
      type: string
      description: ''
      default: ''
      originalType: string
  required:
    - knowledge_source_id
    - chatbot_id


================================================
File: webhook-server/rateLimiter.js
================================================
/**
 * Rate Limiting Middleware for WhatsApp Messages
 *
 * Prevents spam and abuse by limiting messages per chatbot based on plan tier:
 * - Free tier: 20 messages/hour
 * - Pro tier: 100 messages/hour
 * - Enterprise tier: 500 messages/hour
 *
 * Uses Redis for distributed rate limiting across multiple server instances.
 */

import { createClient } from 'redis';

// Rate limits per plan tier (messages per hour)
const RATE_LIMITS = {
  free: 20,
  pro: 100,
  enterprise: 500
};

// Redis client
let redisClient = null;

/**
 * Initialize Redis client
 */
export async function initializeRedis() {
  const redisHost = process.env.REDIS_HOST || 'localhost';
  const redisPort = process.env.REDIS_PORT || 6379;

  redisClient = createClient({
    socket: {
      host: redisHost,
      port: redisPort
    }
  });

  redisClient.on('error', (err) => {
    console.error('Redis Client Error:', err);
  });

  redisClient.on('connect', () => {
    console.log(`Redis connected: ${redisHost}:${redisPort}`);
  });

  await redisClient.connect();
}

/**
 * Check rate limit for a chatbot
 *
 * @param {string} phoneNumberId - WhatsApp phone number ID (chatbot identifier)
 * @param {string} planTier - Organization plan tier (free, pro, enterprise)
 * @returns {Promise<{allowed: boolean, current: number, max: number, resetIn: number}>}
 */
export async function checkRateLimit(phoneNumberId, planTier = 'free') {
  if (!redisClient || !redisClient.isOpen) {
    console.warn('Redis not connected, allowing request (fail-open)');
    return {
      allowed: true,
      current: 0,
      max: RATE_LIMITS[planTier] || RATE_LIMITS.free,
      resetIn: 3600
    };
  }

  const maxRequests = RATE_LIMITS[planTier] || RATE_LIMITS.free;
  const windowSeconds = 3600; // 1 hour
  const key = `ratelimit:${phoneNumberId}`;

  try {
    // Use Redis for sliding window rate limit
    const now = Date.now();
    const windowStart = now - (windowSeconds * 1000);

    // Multi-command transaction for atomic rate limit check
    const multi = redisClient.multi();

    // Remove old entries outside the time window
    multi.zRemRangeByScore(key, 0, windowStart);

    // Count current requests in window
    multi.zCard(key);

    // Add current request
    multi.zAdd(key, { score: now, value: now.toString() });

    // Set expiry on the key (cleanup)
    multi.expire(key, windowSeconds);

    const results = await multi.exec();

    // results[1] is the count before adding current request
    const currentCount = results[1];

    if (currentCount >= maxRequests) {
      // Rate limit exceeded
      // Get the oldest request to calculate reset time
      const oldestRequest = await redisClient.zRange(key, 0, 0, { REV: false });
      const resetIn = oldestRequest.length > 0
        ? Math.ceil((parseInt(oldestRequest[0]) + (windowSeconds * 1000) - now) / 1000)
        : windowSeconds;

      return {
        allowed: false,
        current: currentCount,
        max: maxRequests,
        resetIn
      };
    }

    return {
      allowed: true,
      current: currentCount + 1,
      max: maxRequests,
      resetIn: windowSeconds
    };

  } catch (error) {
    console.error('Rate limit check error:', error);
    // Fail open - allow request if Redis has issues
    return {
      allowed: true,
      current: 0,
      max: maxRequests,
      resetIn: windowSeconds
    };
  }
}

/**
 * Close Redis connection
 */
export async function closeRedis() {
  if (redisClient) {
    await redisClient.quit();
  }
}


================================================
File: db/create.sql
================================================
/* 
====================================================================
  REFINED MVP SCHEMA v2.0
  Database: PostgreSQL
  
  Key Features:
  - Idempotency via webhook_events table
  - Usage tracking and limits
  - Proper indexes for performance
  - Multi-tenant SaaS structure
====================================================================
*/

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "pgcrypto";
CREATE EXTENSION IF NOT EXISTS "vector";  -- Required for RAG/embeddings

-- 1. ORGANIZATIONS (The Tenant)
CREATE TABLE organizations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    slug VARCHAR(50) UNIQUE, 
    
    -- Billing & Status
    stripe_customer_id VARCHAR(100),
    plan_tier VARCHAR(20) DEFAULT 'free', -- free, starter, pro, enterprise
    is_active BOOLEAN DEFAULT TRUE,
    
    -- Usage Limits (per billing period)
    message_limit_monthly INT DEFAULT 100, -- Messages per month
    token_limit_monthly BIGINT DEFAULT 100000, -- Tokens per month
    billing_period_start DATE DEFAULT CURRENT_DATE, -- When current period started
    billing_period_end DATE DEFAULT (CURRENT_DATE + INTERVAL '1 month'), -- When it ends

    -- Notification Settings (for contact_owner MCP)
    notification_method VARCHAR(20) DEFAULT 'disabled', -- 'disabled', 'slack', 'email', 'whatsapp'
    slack_webhook_url TEXT, -- Slack webhook URL for notifications
    notification_email VARCHAR(255), -- Email address for notifications

    -- Knowledge Base Quotas (Configurable per plan tier)
    max_knowledge_pdfs INT DEFAULT 10,
    max_knowledge_urls INT DEFAULT 5,
    max_knowledge_ingestions_per_day INT DEFAULT 20,
    max_knowledge_storage_mb INT DEFAULT 100,

    -- Knowledge Base Usage Tracking
    current_knowledge_pdfs INT DEFAULT 0,
    current_knowledge_urls INT DEFAULT 0,
    current_storage_mb DECIMAL(10,2) DEFAULT 0,

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 2. USERS (Dashboard Access)
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255), 
    full_name VARCHAR(100),
    role VARCHAR(20) DEFAULT 'member', -- owner, admin, member
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    last_login_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_users_org ON users(organization_id);
CREATE INDEX idx_users_email ON users(email);

-- 3. ORG INTEGRATIONS (The "Vault" for Credentials)
CREATE TABLE org_integrations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    
    provider VARCHAR(50) NOT NULL, -- 'mcp', 'hubspot', 'slack', 'custom'
    name VARCHAR(100),
    
    -- Encrypted credentials go here
    credentials JSONB DEFAULT '{}',
    config JSONB DEFAULT '{}',
    
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_org_integrations_org ON org_integrations(organization_id);
CREATE INDEX idx_org_integrations_provider ON org_integrations(organization_id, provider);

-- 4. CHATBOTS (The Agents)
CREATE TABLE chatbots (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    name VARCHAR(100) NOT NULL,
    
    -- Meta / WhatsApp Connection (The Routing Key)
    whatsapp_phone_number_id VARCHAR(50) UNIQUE NOT NULL, 
    whatsapp_business_account_id VARCHAR(50),
    whatsapp_access_token TEXT,
    
    -- AI Config
    model_name VARCHAR(50) DEFAULT 'gemini-3-flash-preview',
    system_prompt TEXT DEFAULT 'You are a helpful assistant.',
    persona TEXT DEFAULT '', -- Tone/Style instructions
    temperature DECIMAL(3,2) DEFAULT 0.7,
    
    -- RAG Config
    rag_enabled BOOLEAN DEFAULT FALSE,

    -- Fallback Messages (Customizable per chatbot)
    fallback_message_error TEXT DEFAULT 'Lo siento, estoy teniendo problemas técnicos. Por favor intenta de nuevo más tarde.',
    fallback_message_limit TEXT DEFAULT 'Lo siento, he alcanzado mi límite de uso. El administrador ha sido notificado.',

    -- Toggles & UI Settings
    is_active BOOLEAN DEFAULT TRUE,
    settings JSONB DEFAULT '{}',

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_chatbots_org ON chatbots(organization_id);
CREATE INDEX idx_chatbots_wa_phone ON chatbots(whatsapp_phone_number_id);
CREATE INDEX idx_chatbots_active ON chatbots(organization_id, is_active);

-- 5. CHATBOT INTEGRATIONS (The "Switch")
CREATE TABLE chatbot_integrations (
    chatbot_id UUID REFERENCES chatbots(id) ON DELETE CASCADE,
    integration_id UUID REFERENCES org_integrations(id) ON DELETE CASCADE,
    
    is_enabled BOOLEAN DEFAULT TRUE,
    settings_override JSONB DEFAULT '{}',
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    PRIMARY KEY (chatbot_id, integration_id)
);

CREATE INDEX idx_chatbot_integrations_enabled ON chatbot_integrations(chatbot_id, is_enabled);

-- 6. KNOWLEDGE SOURCES (Uploaded Files)
CREATE TABLE knowledge_sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    chatbot_id UUID REFERENCES chatbots(id) ON DELETE CASCADE,
    
    source_type VARCHAR(20) NOT NULL, -- pdf, url, text, doc
    name VARCHAR(255),
    
    file_path TEXT, 
    content_hash VARCHAR(64),
    file_size_bytes BIGINT,
    
    sync_status VARCHAR(20) DEFAULT 'pending', -- pending, processing, synced, failed
    last_synced_at TIMESTAMP WITH TIME ZONE,
    error_message TEXT,
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_knowledge_sources_chatbot ON knowledge_sources(chatbot_id);
CREATE INDEX idx_knowledge_sources_status ON knowledge_sources(chatbot_id, sync_status);

-- 6b. DAILY INGESTION COUNTS (Knowledge Base Usage Tracking)
CREATE TABLE daily_ingestion_counts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
    date DATE NOT NULL DEFAULT CURRENT_DATE,
    ingestion_count INT DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(organization_id, date)
);

CREATE INDEX idx_ingestion_counts_org_date ON daily_ingestion_counts(organization_id, date);

-- 7. CONTACTS (End Users)
CREATE TABLE contacts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    chatbot_id UUID REFERENCES chatbots(id) ON DELETE CASCADE,
    
    phone_number VARCHAR(20) NOT NULL,
    name VARCHAR(255),
    
    -- "Human Takeover" State
    conversation_mode VARCHAR(20) DEFAULT 'auto', -- 'auto' or 'manual'
    unread_count INT DEFAULT 0,
    
    -- CRM Data (Email, Tags, Extracted Info)
    variables JSONB DEFAULT '{}',
    tags TEXT[],
    
    last_message_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    UNIQUE(chatbot_id, phone_number)
);

CREATE INDEX idx_contacts_chatbot ON contacts(chatbot_id);
CREATE INDEX idx_contacts_phone ON contacts(chatbot_id, phone_number);
CREATE INDEX idx_contacts_last_message ON contacts(chatbot_id, last_message_at DESC);

-- 8. MESSAGES (Chat History)
CREATE TABLE messages (
    id BIGSERIAL PRIMARY KEY,
    contact_id UUID REFERENCES contacts(id) ON DELETE CASCADE,
    
    role VARCHAR(20) NOT NULL, -- user, assistant, system, tool
    content TEXT,
    
    -- Idempotency & Tracking
    whatsapp_message_id VARCHAR(255), -- From Meta, nullable for assistant messages
    
    -- Tool Usage
    tool_calls JSONB, -- Stores raw tool call data if used
    tool_results JSONB, -- Stores tool execution results
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_messages_contact_date ON messages(contact_id, created_at DESC);
CREATE INDEX idx_messages_wa_id ON messages(whatsapp_message_id) WHERE whatsapp_message_id IS NOT NULL;
CREATE INDEX idx_messages_role ON messages(contact_id, role);

-- 9. WEBHOOK EVENTS (Idempotency & Deduplication)
CREATE TABLE webhook_events (
    id BIGSERIAL PRIMARY KEY,
    
    -- Identification
    whatsapp_message_id VARCHAR(255) UNIQUE NOT NULL,
    phone_number_id VARCHAR(50) NOT NULL,
    chatbot_id UUID REFERENCES chatbots(id) ON DELETE SET NULL,
    
    -- Processing Status
    status VARCHAR(20) DEFAULT 'received', -- received, processing, completed, failed, duplicate
    
    -- Payload & Response
    raw_payload JSONB, -- Store original webhook payload for debugging
    error_message TEXT,
    
    -- Timestamps
    received_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    processed_at TIMESTAMP WITH TIME ZONE,
    expires_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() + INTERVAL '7 days',
    
    -- Metadata
    processing_time_ms INT, -- How long it took to process
    retry_count INT DEFAULT 0
);

CREATE INDEX idx_webhook_events_lookup ON webhook_events(whatsapp_message_id, status);
CREATE INDEX idx_webhook_events_chatbot ON webhook_events(chatbot_id, status);
CREATE INDEX idx_webhook_events_expiry ON webhook_events(expires_at) WHERE status IN ('completed', 'failed');

-- 10. USAGE LOGS (Token & Message Tracking)
CREATE TABLE usage_logs (
    id BIGSERIAL PRIMARY KEY,
    
    -- Relationships
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    chatbot_id UUID REFERENCES chatbots(id) ON DELETE CASCADE,
    contact_id UUID REFERENCES contacts(id) ON DELETE SET NULL,
    message_id BIGINT REFERENCES messages(id) ON DELETE SET NULL,
    webhook_event_id BIGINT REFERENCES webhook_events(id) ON DELETE SET NULL,
    
    -- Usage Metrics
    message_count INT DEFAULT 1,
    tokens_input INT DEFAULT 0,
    tokens_output INT DEFAULT 0,
    tokens_total INT DEFAULT 0,
    
    -- Provider Info
    model_name VARCHAR(50),
    provider VARCHAR(50), -- 'openai', 'google', 'anthropic'
    
    -- Cost Tracking
    estimated_cost_usd DECIMAL(10, 6),
    
    -- Timestamps & Bucketing
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    date_bucket DATE DEFAULT CURRENT_DATE -- For easy daily aggregation
);


-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Document chunks table (for RAG)
CREATE TABLE document_chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    
    -- Relationships
    knowledge_source_id UUID REFERENCES knowledge_sources(id) ON DELETE CASCADE,
    chatbot_id UUID REFERENCES chatbots(id) ON DELETE CASCADE,
    
    -- Content
    content TEXT NOT NULL,
    chunk_index INT NOT NULL, -- Which chunk # in the document (0, 1, 2...)
    
    -- Embeddings (using OpenAI's ada-002: 1536 dimensions, or your chosen model)
    embedding vector(1536), -- Change dimension based on your embedding model
    
    -- Metadata (for filtering and display)
    metadata JSONB DEFAULT '{}', -- Store page numbers, headers, etc.
    
    -- Timestamps
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    -- Composite key to ensure unique chunks per source
    UNIQUE(knowledge_source_id, chunk_index)
);

-- Critical indexes for performance
CREATE INDEX idx_document_chunks_chatbot ON document_chunks(chatbot_id);
CREATE INDEX idx_document_chunks_source ON document_chunks(knowledge_source_id);

-- Vector similarity search index (HNSW is faster than IVFFlat for most cases)
-- HNSW: Better for high-dimensional vectors, faster queries, more memory
CREATE INDEX idx_document_chunks_embedding_hnsw 
ON document_chunks 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Alternative: IVFFlat (use if HNSW is too memory-intensive)
-- CREATE INDEX idx_document_chunks_embedding_ivfflat 
-- ON document_chunks 
-- USING ivfflat (embedding vector_cosine_ops)
-- WITH (lists = 100);

-- Partial index for fast tenant-specific searches
CREATE INDEX idx_document_chunks_chatbot_embedding 
ON document_chunks(chatbot_id) 
WHERE embedding IS NOT NULL;

-- Update knowledge_sources table to track embedding status
ALTER TABLE knowledge_sources 
ADD COLUMN IF NOT EXISTS chunks_count INT DEFAULT 0,
ADD COLUMN IF NOT EXISTS embedding_model VARCHAR(50) DEFAULT 'text-embedding-ada-002',
ADD COLUMN IF NOT EXISTS embedding_dimensions INT DEFAULT 1536;

-- Function to search similar chunks for a chatbot
CREATE OR REPLACE FUNCTION search_knowledge_base(
    p_chatbot_id UUID,
    p_query_embedding vector(1536),
    p_limit INT DEFAULT 5,
    p_similarity_threshold FLOAT DEFAULT 0.7
)
RETURNS TABLE (
    chunk_id UUID,
    content TEXT,
    similarity FLOAT,
    source_name VARCHAR(255),
    source_type VARCHAR(20),
    metadata JSONB
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        dc.id as chunk_id,
        dc.content,
        1 - (dc.embedding <=> p_query_embedding) as similarity,
        ks.name as source_name,
        ks.source_type,
        dc.metadata
    FROM document_chunks dc
    JOIN knowledge_sources ks ON dc.knowledge_source_id = ks.id
    WHERE dc.chatbot_id = p_chatbot_id
      AND dc.embedding IS NOT NULL
      AND (1 - (dc.embedding <=> p_query_embedding)) >= p_similarity_threshold
    ORDER BY dc.embedding <=> p_query_embedding
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;

-- Function to get embedding statistics for a chatbot
CREATE OR REPLACE FUNCTION get_embedding_stats(p_chatbot_id UUID)
RETURNS TABLE (
    total_chunks BIGINT,
    total_sources INT,
    avg_chunk_size FLOAT,
    embedding_coverage FLOAT -- % of chunks with embeddings
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        COUNT(dc.id) as total_chunks,
        COUNT(DISTINCT dc.knowledge_source_id) as total_sources,
        AVG(LENGTH(dc.content)) as avg_chunk_size,
        (COUNT(*) FILTER (WHERE dc.embedding IS NOT NULL)::FLOAT / 
         NULLIF(COUNT(*)::FLOAT, 0) * 100) as embedding_coverage
    FROM document_chunks dc
    WHERE dc.chatbot_id = p_chatbot_id;
END;
$$ LANGUAGE plpgsql;

-- Trigger to update chunks_count when chunks are added/removed
CREATE OR REPLACE FUNCTION update_knowledge_source_chunks_count()
RETURNS TRIGGER AS $$
BEGIN
    IF (TG_OP = 'DELETE') THEN
        UPDATE knowledge_sources 
        SET chunks_count = chunks_count - 1
        WHERE id = OLD.knowledge_source_id;
        RETURN OLD;
    ELSIF (TG_OP = 'INSERT') THEN
        UPDATE knowledge_sources 
        SET chunks_count = chunks_count + 1
        WHERE id = NEW.knowledge_source_id;
        RETURN NEW;
    END IF;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_update_chunks_count
AFTER INSERT OR DELETE ON document_chunks
FOR EACH ROW
EXECUTE FUNCTION update_knowledge_source_chunks_count();

-- Trigger function to auto-increment knowledge base counters
CREATE OR REPLACE FUNCTION increment_knowledge_counters()
RETURNS TRIGGER AS $$
DECLARE
    org_id UUID;
    file_size_mb DECIMAL(10,2);
BEGIN
    -- Get organization_id from chatbot
    SELECT c.organization_id INTO org_id
    FROM chatbots c
    WHERE c.id = NEW.chatbot_id;

    -- Calculate file size in MB
    file_size_mb := COALESCE(NEW.file_size_bytes / 1048576.0, 0);

    -- Increment counters based on source type
    IF NEW.source_type IN ('pdf', 'doc') THEN
        UPDATE organizations
        SET current_knowledge_pdfs = current_knowledge_pdfs + 1,
            current_storage_mb = current_storage_mb + file_size_mb
        WHERE id = org_id;
    ELSIF NEW.source_type = 'url' THEN
        UPDATE organizations
        SET current_knowledge_urls = current_knowledge_urls + 1,
            current_storage_mb = current_storage_mb + file_size_mb
        WHERE id = org_id;
    END IF;

    -- Increment daily ingestion count
    INSERT INTO daily_ingestion_counts (organization_id, date, ingestion_count)
    VALUES (org_id, CURRENT_DATE, 1)
    ON CONFLICT (organization_id, date)
    DO UPDATE SET ingestion_count = daily_ingestion_counts.ingestion_count + 1;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER knowledge_source_counter_trigger
AFTER INSERT ON knowledge_sources
FOR EACH ROW
EXECUTE FUNCTION increment_knowledge_counters();

CREATE INDEX idx_usage_logs_org_date ON usage_logs(organization_id, date_bucket);
CREATE INDEX idx_usage_logs_chatbot_date ON usage_logs(chatbot_id, date_bucket);
-- Index for recent usage queries (removed WHERE clause due to IMMUTABLE requirement)
CREATE INDEX idx_usage_logs_org_period ON usage_logs(organization_id, created_at);

-- 11. USAGE SUMMARY (Cached Aggregates for Performance)
-- This is a materialized view alternative - stores current billing period usage
CREATE TABLE usage_summary (
    organization_id UUID PRIMARY KEY REFERENCES organizations(id) ON DELETE CASCADE,
    
    -- Current Period Usage
    current_period_messages INT DEFAULT 0,
    current_period_tokens BIGINT DEFAULT 0,
    
    -- Billing Period Reference
    period_start DATE,
    period_end DATE,
    
    -- Metadata
    last_updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_usage_summary_period ON usage_summary(period_start, period_end);

-- Helper function to get current usage for an org
CREATE OR REPLACE FUNCTION get_current_usage(org_id UUID)
RETURNS TABLE(messages_used INT, tokens_used BIGINT) AS $$
DECLARE
    org_record RECORD;
BEGIN
    -- Get org billing period
    SELECT billing_period_start, billing_period_end 
    INTO org_record
    FROM organizations 
    WHERE id = org_id;
    
    -- Return aggregated usage for current period
    RETURN QUERY
    SELECT 
        COALESCE(SUM(message_count), 0)::INT as messages_used,
        COALESCE(SUM(tokens_total), 0)::BIGINT as tokens_used
    FROM usage_logs
    WHERE organization_id = org_id
      AND created_at >= org_record.billing_period_start
      AND created_at < org_record.billing_period_end;
END;
$$ LANGUAGE plpgsql;

-- Trigger to update updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Apply the trigger to relevant tables
CREATE TRIGGER update_organizations_updated_at BEFORE UPDATE ON organizations
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_users_updated_at BEFORE UPDATE ON users
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_chatbots_updated_at BEFORE UPDATE ON chatbots
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_contacts_updated_at BEFORE UPDATE ON contacts
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- Create a cleanup job for old webhook events (you'd run this periodically)
-- This is just the function - you'd need to schedule it with pg_cron or external scheduler
CREATE OR REPLACE FUNCTION cleanup_old_webhook_events()
RETURNS INT AS $$
DECLARE
    deleted_count INT;
BEGIN
    DELETE FROM webhook_events
    WHERE expires_at < NOW()
      AND status IN ('completed', 'failed');
    
    GET DIAGNOSTICS deleted_count = ROW_COUNT;
    RETURN deleted_count;
END;
$$ LANGUAGE plpgsql;

================================================
File: db/manage_db.sh
================================================
#!/bin/bash

# Get the directory where this script is located (resolve symlinks)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Parse command-line arguments for target database
TARGET="dev"  # Default to dev database
COMMAND=""

for arg in "$@"; do
  case "$arg" in
    --test)
      TARGET="test"
      ;;
    --dev)
      TARGET="dev"
      ;;
    *)
      if [ -z "$COMMAND" ]; then
        COMMAND="$arg"
      fi
      ;;
  esac
done

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Helper functions for colored output
error() {
  echo -e "${RED}✗ Error: $1${NC}" >&2
}

success() {
  echo -e "${GREEN}✓ $1${NC}"
}

info() {
  echo -e "${YELLOW}ℹ $1${NC}"
}

# 1. Load environment variables from the .env file in parent directory
load_env() {
  # Get the directory where this script is located
  local script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
  # .env should be in the parent of the script's directory (src/.env)
  local env_file="${script_dir}/../.env"
  
  if [ ! -f "$env_file" ]; then
    error ".env file not found at: $env_file"
    echo "  Expected location: $(dirname "$script_dir")/.env"
    exit 1
  fi
  
  # Use a more robust method: set -a enables automatic export
  # Filter out comments and empty lines, then source
  set -a
  # Create a temporary file with cleaned .env content
  local temp_env=$(mktemp)
  grep -v '^[[:space:]]*#' "$env_file" | grep -v '^[[:space:]]*$' > "$temp_env"
  
  # Source the cleaned .env file
  # This properly handles quoted values, spaces, and special characters
  . "$temp_env" 2>/dev/null || {
    # Fallback: read line by line if sourcing fails
    while IFS= read -r line || [ -n "$line" ]; do
      [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue
      # Remove inline comments (simple approach)
      line=$(echo "$line" | sed 's/[[:space:]]*#.*$//')
      [[ -z "$line" ]] && continue
      # Export the variable
      export "$line" 2>/dev/null || true
    done < "$env_file"
  }
  set +a
  rm -f "$temp_env"
  
  success ".env file loaded"
}

load_env

# Configuration based on target database
if [ "$TARGET" = "test" ]; then
  # Test database configuration (from docker-compose.test.yml)
  info "🧪 Targeting TEST database"
  CONTAINER_NAME="test_business_logic_db"
  DB_USER="test_user"
  DB_NAME="test_business_logic"
  # Test DB doesn't require .env validation since credentials are hardcoded in docker-compose.test.yml
else
  # Development database configuration (from .env and docker-compose.yml)
  info "🔧 Targeting DEV database"
  CONTAINER_NAME="business_logic_db"
  DB_USER="$BUSINESS_LOGIC_DB_USER"
  DB_NAME="$BUSINESS_LOGIC_DB_NAME"

  # Validate configuration for dev database
  if [ -z "$DB_USER" ] || [ -z "$DB_NAME" ]; then
    error "Database configuration missing in .env file:"
    [ -z "$DB_USER" ] && echo "  - BUSINESS_LOGIC_DB_USER"
    [ -z "$DB_NAME" ] && echo "  - BUSINESS_LOGIC_DB_NAME"
    echo ""
    info "Debug: Checking loaded environment variables..."
    echo "  BUSINESS_LOGIC_DB_USER=${BUSINESS_LOGIC_DB_USER:-<not set>}"
    echo "  BUSINESS_LOGIC_DB_NAME=${BUSINESS_LOGIC_DB_NAME:-<not set>}"
    echo ""
    echo "  All variables containing 'DB' or 'BUSINESS':"
    env | grep -iE "(DB|BUSINESS)" | sed 's/^/    /' || echo "    (none found)"
    exit 1
  fi
fi

# Check if container exists and is running
check_container() {
  if ! docker ps --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
    error "Container '${CONTAINER_NAME}' is not running."
    echo "  Please start it with: docker-compose up -d ${CONTAINER_NAME}"
    exit 1
  fi
  success "Container '${CONTAINER_NAME}' is running"
}

# Check if SQL file exists
check_file() {
  local file=$1
  if [ ! -f "$file" ]; then
    error "SQL file not found: $file"
    exit 1
  fi
}

# Helper function to run SQL inside the Docker container
run_sql() {
  local file=$1
  local full_path
  local temp_output
  local exit_code=0

  # If file is absolute path, use as-is; otherwise prepend SCRIPT_DIR
  if [[ "$file" = /* ]]; then
    full_path="$file"
  else
    full_path="$SCRIPT_DIR/$file"
  fi

  check_file "$full_path"

  # Get just the filename for display
  local display_name=$(basename "$file")
  info "Running $display_name..."

  # Capture output and exit code
  # For seed.sql, use envsubst to replace environment variables
  # For other files, just cat the file
  if [[ "$display_name" = "seed.sql" ]]; then
    temp_output=$(envsubst < "$full_path" | docker exec -i "$CONTAINER_NAME" psql -U "$DB_USER" -d "$DB_NAME" 2>&1)
    exit_code=$?
  else
    temp_output=$(cat "$full_path" | docker exec -i "$CONTAINER_NAME" psql -U "$DB_USER" -d "$DB_NAME" 2>&1)
    exit_code=$?
  fi
  
  if [ $exit_code -eq 0 ]; then
    success "$file executed successfully"
    return 0
  else
    error "$file failed with exit code $exit_code"
    # Show relevant error messages (filter out noise)
    local error_lines=$(echo "$temp_output" | grep -iE "error|fatal|failed|syntax|does not exist" | head -3)
    if [ -n "$error_lines" ]; then
      echo ""
      echo "Error details:"
      echo "$error_lines" | sed 's/^/  /'
    fi
    echo ""
    echo "  Troubleshooting:"
    echo "  - Check SQL syntax in $file"
    echo "  - Verify database connection and credentials"
    echo "  - Ensure container '$CONTAINER_NAME' is running"
    return $exit_code
  fi
}

# Apply database migrations
run_migrations() {
  local migrations_dir="$SCRIPT_DIR/migrations"

  info "Applying database migrations..."

  # Check if migrations directory exists
  if [ ! -d "$migrations_dir" ]; then
    error "Migrations directory not found: $migrations_dir"
    return 1
  fi

  # Get list of migration files in sorted order
  local migration_files=$(ls "$migrations_dir"/*.sql 2>/dev/null | sort)

  if [ -z "$migration_files" ]; then
    info "No migration files found in $migrations_dir/"
    return 0
  fi

  # Apply each migration file
  local migration_count=0
  for migration_file in $migration_files; do
    local migration_name=$(basename "$migration_file")
    info "  Applying migration: $migration_name"

    if ! run_sql "$migration_file"; then
      error "Migration failed: $migration_name"
      return 1
    fi

    migration_count=$((migration_count + 1))
  done

  success "Applied $migration_count migration(s)"
  return 0
}

# Validate required environment variables for seed.sql
validate_seed_vars() {
  if [ -z "$WHATSAPP_PHONE_NUMBER_ID" ] || [ -z "$WHATSAPP_ACCESS_TOKEN" ]; then
    error "Required environment variables not set in .env file:"
    [ -z "$WHATSAPP_PHONE_NUMBER_ID" ] && echo "  - WHATSAPP_PHONE_NUMBER_ID"
    [ -z "$WHATSAPP_ACCESS_TOKEN" ] && echo "  - WHATSAPP_ACCESS_TOKEN"
    echo ""
    echo "Please add these to your .env file:"
    echo "  WHATSAPP_PHONE_NUMBER_ID=your_phone_id"
    echo "  WHATSAPP_ACCESS_TOKEN=your_access_token"
    exit 1
  fi
  success "Required environment variables validated"
}

# Verify database tables and their contents
verify_db() {
  info "Verifying database '$DB_NAME'..."
  echo ""
  
  # Get list of tables
  local tables=$(docker exec "$CONTAINER_NAME" psql -U "$DB_USER" -d "$DB_NAME" -t -A -c "
    SELECT tablename 
    FROM pg_tables 
    WHERE schemaname = 'public' 
    ORDER BY tablename;
  " 2>/dev/null | grep -v '^$')
  
  if [ -z "$tables" ]; then
    error "No tables found in database '$DB_NAME'"
    echo "  Run './manage_db.sh create' to create the schema"
    return 1
  fi
  
  local table_count=$(echo "$tables" | wc -l)
  success "Found $table_count table(s)"
  echo ""
  
  # For each table, show structure and contents
  for table in $tables; do
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    info "Table: $table"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    
    # Get row count
    local count=$(docker exec "$CONTAINER_NAME" psql -U "$DB_USER" -d "$DB_NAME" -t -A -c "
      SELECT COUNT(*) FROM \"$table\";
    " 2>/dev/null | tr -d ' ')
    
    echo "Row count: $count"
    echo ""
    
    # Show table structure (column names and types)
    echo "Schema:"
    docker exec "$CONTAINER_NAME" psql -U "$DB_USER" -d "$DB_NAME" -c "
      SELECT 
        column_name AS \"Column\",
        data_type AS \"Type\",
        CASE 
          WHEN character_maximum_length IS NOT NULL 
          THEN data_type || '(' || character_maximum_length || ')'
          ELSE data_type
        END AS \"Full Type\",
        is_nullable AS \"Nullable\"
      FROM information_schema.columns
      WHERE table_name = '$table'
      ORDER BY ordinal_position;
    " 2>/dev/null
    
    echo ""
    
    # Show sample data (limit to 3 rows for readability)
    if [ "$count" -gt 0 ]; then
      local display_rows=3
      if [ "$count" -lt "$display_rows" ]; then
        display_rows=$count
      fi
      echo "Sample data (showing $display_rows of $count rows):"
      docker exec "$CONTAINER_NAME" psql -U "$DB_USER" -d "$DB_NAME" -c "
        SELECT * FROM \"$table\" LIMIT $display_rows;
      " 2>/dev/null
    else
      echo "No data in this table"
    fi
    
    echo ""
    echo ""
  done
  
  success "Verification complete"
  return 0
}

# 2. Command Router
case "$COMMAND" in
  "drop")
    check_container
    info "WARNING: This will delete all data in database '$DB_NAME'."
    read -p "Are you sure? (y/n) " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
      if run_sql "drop.sql"; then
        success "Database '$DB_NAME' dropped successfully"
      else
        error "Failed to drop database"
        exit 1
      fi
    else
      info "Operation cancelled"
    fi
    ;;
  
  "create")
    check_container
    if ! run_sql "create.sql"; then
      error "Failed to create schema"
      exit 1
    fi

    if ! run_migrations; then
      error "Failed to apply migrations"
      exit 1
    fi

    success "Schema and migrations applied successfully in database '$DB_NAME'"
    ;;
  
  "seed")
    check_container
    validate_seed_vars
    if run_sql "seed.sql"; then
      success "Seed data inserted successfully into database '$DB_NAME'"
    else
      error "Failed to insert seed data"
      exit 1
    fi
    ;;
  
  "reset")
    check_container
    validate_seed_vars
    info "Resetting database '$DB_NAME' (drop -> create -> migrations -> seed)..."

    if ! run_sql "drop.sql"; then
      error "Failed to drop database. Aborting reset."
      exit 1
    fi

    if ! run_sql "create.sql"; then
      error "Failed to create schema. Aborting reset."
      exit 1
    fi

    if ! run_migrations; then
      error "Failed to apply migrations. Aborting reset."
      exit 1
    fi

    if ! run_sql "seed.sql"; then
      error "Failed to insert seed data. Aborting reset."
      exit 1
    fi

    success "Database reset complete!"
    ;;
  
  "verify")
    check_container
    if verify_db; then
      success "Database verification completed successfully"
    else
      error "Database verification failed"
      exit 1
    fi
    ;;
  
  *)
    error "Invalid command: $COMMAND"
    echo ""
    echo "Usage: ./manage_db.sh [OPTIONS] <COMMAND>"
    echo ""
    echo "Options:"
    echo "  --test  - Target the test database (test_business_logic_db)"
    echo "  --dev   - Target the dev database (business_logic_db) [default]"
    echo ""
    echo "Commands:"
    echo "  create  - Create database schema"
    echo "  seed    - Insert seed data (requires WHATSAPP_PHONE_NUMBER_ID and WHATSAPP_ACCESS_TOKEN in .env for dev DB)"
    echo "  drop    - Drop all tables (with confirmation)"
    echo "  reset   - Drop, create, and seed database (full reset)"
    echo "  verify  - Show all tables, their structure, and sample data"
    echo ""
    echo "Examples:"
    echo "  ./manage_db.sh reset             # Reset dev database"
    echo "  ./manage_db.sh --test reset      # Reset test database"
    echo "  ./manage_db.sh --test create     # Create test database schema"
    exit 1
    ;;
esac

================================================
File: f/development/3_1_send_reply_to_whatsapp.script.yaml
================================================
summary: 3.1 send reply to whatsapp
description: ''
lock: '!inline f/development/3_1_send_reply_to_whatsapp.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    context_payload:
      type: object
      description: ''
      default: null
      properties: {}
    llm_result:
      type: object
      description: ''
      default: null
      properties: {}
    phone_number_id:
      type: string
      description: ''
      default: null
      originalType: string
  required:
    - phone_number_id
    - context_payload
    - llm_result


================================================
File: f/development/upload_document.py
================================================
"""
Document Upload Flow for RAG Knowledge Base

This script handles document uploads to the knowledge base:
1. Validates chatbot ownership
2. Saves file to Windmill S3 storage
3. Creates knowledge_source record
4. Triggers async RAG processing

Supported document types:
- PDF files
- URLs (web pages)
- Plain text
- Word documents (.docx)
"""

import wmill
import psycopg2
from psycopg2.extras import RealDictCursor
from typing import Dict, Any, Optional
import base64
import mimetypes


def main(
    chatbot_id: str,
    source_type: str,  # "pdf", "url", "text", "doc"
    name: str,  # Display name for the document
    file_content: Optional[str] = None,  # Base64 encoded file content (for pdf/doc/text)
    url: Optional[str] = None,  # URL for web pages
    openai_api_key: str = wmill.get_variable("u/admin/OpenAI_API_Key"),
    db_resource: str = "f/development/business_layer_db_postgreSQL",
) -> Dict[str, Any]:
    """
    Upload a document to the knowledge base.

    Args:
        chatbot_id: UUID of the chatbot
        source_type: Type of document (pdf, url, text, doc)
        name: Display name for the document
        file_content: Base64 encoded file content (for pdf/doc/text)
        url: URL for web pages
        openai_api_key: OpenAI API key for embedding generation
        db_resource: Database resource path

    Returns:
        Upload result with source_id and processing job_id
    """

    # Validate inputs
    if source_type not in ["pdf", "url", "text", "doc"]:
        return {"success": False, "error": f"Invalid source_type: {source_type}"}

    if source_type == "url" and not url:
        return {"success": False, "error": "URL required for url source_type"}

    if source_type in ["pdf", "doc", "text"] and not file_content:
        return {"success": False, "error": f"file_content required for {source_type} source_type"}

    # Setup database
    raw_config = wmill.get_resource(db_resource)
    db_params = {
        "host": raw_config.get("host"),
        "port": raw_config.get("port"),
        "user": raw_config.get("user"),
        "password": raw_config.get("password"),
        "dbname": raw_config.get("dbname"),
        "sslmode": "disable",
    }

    conn = psycopg2.connect(**db_params)
    cur = conn.cursor(cursor_factory=RealDictCursor)

    try:
        # 1. Validate chatbot exists and is active
        cur.execute(
            """
            SELECT c.id, c.organization_id, c.is_active, c.rag_enabled
            FROM chatbots c
            WHERE c.id = %s
            """,
            (chatbot_id,)
        )
        chatbot = cur.fetchone()

        if not chatbot:
            return {"success": False, "error": "Chatbot not found"}

        if not chatbot["is_active"]:
            return {"success": False, "error": "Chatbot is disabled"}

        if not chatbot.get("rag_enabled"):
            return {"success": False, "error": "RAG is not enabled for this chatbot"}

        # 2. Handle file storage
        file_path = None
        file_size = 0

        if source_type == "url":
            # For URLs, store the URL directly
            file_path = url

        else:
            # For files, save to S3
            try:
                # Decode base64 content
                file_bytes = base64.b64decode(file_content)
                file_size = len(file_bytes)

                # Validate file size (max 10MB for now)
                max_size = 10 * 1024 * 1024  # 10MB
                if file_size > max_size:
                    return {
                        "success": False,
                        "error": f"File too large. Max size: {max_size / 1024 / 1024:.1f}MB"
                    }

                # Determine file extension
                ext_map = {
                    "pdf": ".pdf",
                    "doc": ".docx",
                    "text": ".txt"
                }
                file_ext = ext_map.get(source_type, ".bin")

                # Generate S3 path
                # Format: knowledge/{chatbot_id}/{timestamp}_{name}{ext}
                import time
                timestamp = int(time.time())
                safe_name = "".join(c for c in name if c.isalnum() or c in (' ', '-', '_')).strip()
                safe_name = safe_name.replace(' ', '_')
                s3_filename = f"{timestamp}_{safe_name}{file_ext}"

                # Write to S3
                s3_result = wmill.write_s3_file(
                    s3object=None,  # Auto-generate path
                    file_content=file_bytes,
                    s3_resource_path=None,  # Use default workspace S3
                )

                file_path = s3_result.get("s3")
                print(f"File saved to S3: {file_path}")

            except Exception as e:
                return {"success": False, "error": f"File storage failed: {str(e)}"}

        # 3. Create knowledge_source record
        cur.execute(
            """
            INSERT INTO knowledge_sources (
                chatbot_id,
                source_type,
                name,
                file_path,
                file_size,
                sync_status,
                created_at
            ) VALUES (%s, %s, %s, %s, %s, 'pending', NOW())
            RETURNING id
            """,
            (chatbot_id, source_type, name, file_path, file_size)
        )

        source_id = cur.fetchone()["id"]
        conn.commit()

        print(f"Created knowledge_source: {source_id}")

        # 4. Trigger async RAG processing
        try:
            job_id = wmill.run_script_by_path_async(
                path="f/development/RAG_process_documents",
                args={
                    "knowledge_source_id": str(source_id),
                    "chatbot_id": chatbot_id,
                    "openai_api_key": openai_api_key,
                }
            )

            print(f"RAG processing job started: {job_id}")

            return {
                "success": True,
                "source_id": str(source_id),
                "processing_job_id": job_id,
                "message": "Document uploaded successfully. Processing started.",
                "file_path": file_path if source_type != "url" else None,
                "url": url if source_type == "url" else None,
                "file_size": file_size,
                "status": "processing"
            }

        except Exception as e:
            print(f"Failed to trigger RAG processing: {e}")
            # Update status to failed
            cur.execute(
                """
                UPDATE knowledge_sources
                SET sync_status = 'failed',
                    error_message = %s
                WHERE id = %s
                """,
                (f"Failed to start processing: {str(e)}", source_id)
            )
            conn.commit()

            return {
                "success": False,
                "error": f"Document uploaded but processing failed to start: {str(e)}",
                "source_id": str(source_id)
            }

    except Exception as e:
        print(f"Upload error: {e}")
        conn.rollback()
        return {"success": False, "error": str(e)}

    finally:
        cur.close()
        conn.close()


def validate_url(url: str) -> bool:
    """
    Validate URL format.

    Args:
        url: URL to validate

    Returns:
        True if valid, False otherwise
    """
    import re

    # Simple URL validation
    url_pattern = re.compile(
        r'^https?://'  # http:// or https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)

    return url_pattern.match(url) is not None


================================================
File: tests/README.md
================================================
# Testing Guide

This directory contains comprehensive tests for the WhatsApp chatbot flow.

## Directory Structure

```
tests/
├── README.md                           # This file
├── conftest.py                         # Pytest configuration & fixtures
├── pytest.ini                          # Pytest settings
├── requirements.txt                    # Test dependencies
├── test_harness/                       # Mock implementations
│   ├── __init__.py
│   ├── windmill_mock.py               # Mock wmill functions
│   ├── llm_mock.py                    # Mock OpenAI/Google APIs
│   └── whatsapp_mock.py               # Mock WhatsApp API
├── unit/                              # Unit tests for individual steps
│   ├── test_step1_context_loading.py
│   ├── test_step2_llm_processing.py
│   ├── test_step3_1_send_reply.py
│   ├── test_step3_2_save_history.py
│   └── test_step3_3_usage_logging.py
├── integration/                        # Integration tests
│   ├── test_full_flow.py
│   └── test_database_operations.py
└── fixtures/                          # Test data
    ├── sample_webhook_payloads.json
    └── sample_llm_responses.json
```

## Setup

### 1. Install Dependencies

```bash
pip install -r tests/requirements.txt
```

### 2. Start Test Database

```bash
# Start the test database container
docker-compose -f docker-compose.test.yml up -d

# Wait for it to be healthy
docker-compose -f docker-compose.test.yml ps

# The database will auto-initialize with the schema from db/*.sql
```

### 3. Configure Environment

Create a `.env.test` file (or use your existing `.env`):

```bash
# Test Database
TEST_DB_HOST=localhost
TEST_DB_PORT=5434
TEST_DB_USER=test_user
TEST_DB_PASSWORD=test_password
TEST_DB_NAME=test_business_logic

# Test API Keys (can be fake for testing)
GOOGLE_API_KEY=test_google_key
OPENAI_API_KEY=test_openai_key
WHATSAPP_PHONE_NUMBER_ID=test_phone_123
WHATSAPP_ACCESS_TOKEN=test_token_xyz
OWNER_EMAIL=test@example.com
```

## Running Tests

### Run All Tests

```bash
pytest
```

### Run Specific Test Categories

```bash
# Unit tests only
pytest tests/unit/

# Integration tests only
pytest tests/integration/

# Tests for a specific step
pytest tests/unit/test_step1_context_loading.py

# Tests with database
pytest -m db

# Tests without database
pytest -m "not db"
```

### Run with Coverage

```bash
pytest --cov=f/development --cov-report=html
```

### Run in Verbose Mode

```bash
pytest -v
pytest -vv  # Extra verbose
```

### Run Specific Test

```bash
# By test name
pytest tests/unit/test_step1_context_loading.py::TestIdempotency::test_duplicate_message_blocked

# By keyword match
pytest -k "idempotency"
pytest -k "usage_limits"
```

## Test Fixtures

### Database Fixtures

- **`clean_db`** - Resets database to seed state before test
- **`db_with_data`** - Provides DB cursor with seed data
- **`db_cursor`** - Raw database cursor (auto-rollback)
- **`query_helper`** - Helper methods for common queries

Example usage:

```python
def test_something(db_with_data, query_helper):
    # Database is clean and seeded
    org = query_helper.get_organization("11111111-1111-1111-1111-111111111111")
    assert org["name"] == "Dev Corp"
```

### Mock Fixtures

- **`mock_wmill`** - Mock Windmill functions
- **`mock_llm`** - Mock LLM providers
- **`mock_whatsapp`** - Mock WhatsApp API
- **`mock_all_external`** - All mocks together

Example usage:

```python
def test_something(mock_wmill, mock_llm):
    # Configure mock responses
    mock_llm.add_response("This is a test response", tokens_input=100, tokens_output=50)
    
    # Patch and test
    with patch('wmill.get_resource', mock_wmill.get_resource):
        result = my_function()
    
    # Verify
    assert mock_llm.get_call_count() == 1
```

### Data Fixtures

- **`sample_webhook_payload`** - WhatsApp webhook payload
- **`sample_context_payload`** - Step 1 output
- **`sample_llm_result`** - Step 2 output

## Writing Tests

### Unit Test Template

```python
import pytest
from unittest.mock import patch

def test_feature_name(db_with_data, mock_wmill):
    """Test that feature works as expected."""
    # Arrange
    with patch('wmill.get_resource', mock_wmill.get_resource):
        # Import the module AFTER patching
        from f.development import module_under_test
        
        # Act
        result = module_under_test.main(
            param1="value1",
            param2="value2"
        )
    
    # Assert
    assert result["expected_field"] == "expected_value"
```

### Integration Test Template

```python
import pytest
from unittest.mock import patch

def test_full_flow(clean_db, mock_all_external):
    """Test complete flow from webhook to response."""
    # Configure mocks
    mocks = mock_all_external
    mocks["llm"].add_response("Test response")
    
    # Run flow
    # ... simulate full workflow
    
    # Verify end-to-end behavior
    assert mocks["whatsapp"].get_call_count() == 1
```

## Test Database

### Resetting the Database

The database automatically resets before each test using the `clean_db` fixture. If you need to manually reset:

```bash
# Stop and remove test database
docker-compose -f docker-compose.test.yml down -v

# Restart
docker-compose -f docker-compose.test.yml up -d
```

### Inspecting Test Data

```bash
# Connect to test database
docker exec -it test_business_logic_db psql -U test_user -d test_business_logic

# View tables
\dt

# Query data
SELECT * FROM organizations;
SELECT * FROM chatbots;
```

### Custom Test Data

To add custom test data, modify `db/seed.sql` or insert data in your test:

```python
def test_with_custom_data(db_with_data):
    # Insert test data
    db_with_data.execute(
        "INSERT INTO contacts (id, chatbot_id, phone_number, name) VALUES (%s, %s, %s, %s)",
        ("test-id", "chatbot-id", "15551234567", "Test User")
    )
    db_with_data.connection.commit()
    
    # Run test
    # ...
```

## Best Practices

### 1. Use Appropriate Test Level

- **Unit Tests**: Test individual functions with mocked dependencies
- **Integration Tests**: Test multiple components together with real database
- **E2E Tests**: Use Windmill's built-in testing for full workflow

### 2. Keep Tests Fast

- Use mocks for external APIs (LLM, WhatsApp)
- Use real database for data integrity tests
- Run slow tests separately: `pytest -m "not slow"`

### 3. Test Isolation

- Each test should be independent
- Use `clean_db` fixture for tests that modify data
- Don't rely on test execution order

### 4. Descriptive Names

```python
# Good
def test_duplicate_message_blocked_when_already_completed()

# Bad
def test_dup_msg()
```

### 5. AAA Pattern

```python
def test_something():
    # Arrange - Set up test data
    data = {"key": "value"}
    
    # Act - Execute the code
    result = function_under_test(data)
    
    # Assert - Verify results
    assert result == expected_value
```

## Continuous Integration

### GitHub Actions Example

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: test_business_logic
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5434:5432
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: |
          pip install -r tests/requirements.txt
      
      - name: Run tests
        run: |
          pytest --cov --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

## Debugging Tests

### Print Database State

```python
def test_something(db_with_data):
    # Print all contacts
    db_with_data.execute("SELECT * FROM contacts")
    for row in db_with_data.fetchall():
        print(row)
```

### Use pytest's built-in debugging

```bash
# Drop into debugger on failure
pytest --pdb

# Drop into debugger at start of test
pytest --trace
```

### Inspect Mock Calls

```python
def test_something(mock_llm, mock_whatsapp):
    # ... run test
    
    # Inspect calls
    print(f"LLM called {mock_llm.get_call_count()} times")
    print(f"Last call: {mock_llm.get_last_call()}")
    print(f"WhatsApp messages: {mock_whatsapp.get_sent_messages()}")
```

## Common Issues

### Issue: Database Connection Failed

**Solution**: Ensure test database is running
```bash
docker-compose -f docker-compose.test.yml ps
docker-compose -f docker-compose.test.yml up -d
```

### Issue: Import Errors

**Solution**: Ensure project root is in Python path
```python
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
```

### Issue: Tests Interfering With Each Other

**Solution**: Use `clean_db` fixture to reset between tests

### Issue: Mocks Not Working

**Solution**: Patch AFTER imports
```python
# Wrong
from module import function
with patch('module.function'):
    ...

# Right
with patch('module.function'):
    from module import function
    ...
```

## Performance

### Test Execution Time

Track slow tests:
```bash
pytest --durations=10
```

### Parallel Execution

```bash
# Install pytest-xdist
pip install pytest-xdist

# Run tests in parallel
pytest -n auto
```

## Maintenance

### Adding New Tests

1. Create test file in appropriate directory
2. Import fixtures from `conftest.py`
3. Follow naming convention: `test_*.py`
4. Add docstrings explaining what's tested

### Updating Test Data

1. Modify `db/seed.sql` for permanent changes
2. Use fixtures for test-specific data

### Deprecating Tests

Mark tests as expected to fail:
```python
@pytest.mark.xfail(reason="Known issue #123")
def test_something():
    ...
```

## Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [Testing Best Practices](https://docs.pytest.org/en/latest/goodpractices.html)
- [Mocking Guide](https://docs.python.org/3/library/unittest.mock.html)

================================================
File: f/development/3_2_save_chat_history.script.lock
================================================
# py: 3.12
anyio==4.12.0
certifi==2025.11.12
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.11
psycopg2-binary==2.9.11
typing-extensions==4.15.0
wmill==1.601.1

================================================
File: htmlcov/.gitignore
================================================
# Created by coverage.py
*


================================================
File: docker-compose.test.yml
================================================
version: "3.7"

services:
  test_db:
    build: ./docker/postgres
    container_name: test_business_logic_db
    environment:
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
      POSTGRES_DB: test_business_logic
    ports:
      - "5434:5432"  # Different port to avoid conflict with dev DB
    volumes:
      - test_db_data:/var/lib/postgresql/data
      # Mount SQL scripts for easy initialization
      - ./db:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U test_user -d test_business_logic"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - test_network

volumes:
  test_db_data:
    driver: local

networks:
  test_network:
    driver: bridge

================================================
File: tests/unit/test_step3_2_save_history.py
================================================
"""
Unit tests for Step 3.2: Save Chat History

Tests Step 3.2's ability to:
- Save user and assistant messages to database
- Update user variables from LLM extraction
- Skip saving when previous steps failed
- Handle database errors gracefully
- Maintain conversation threading
"""

import pytest
import sys
import os
from unittest.mock import Mock, patch, MagicMock

# Add parent directories to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../f/development'))

# Mock wmill module before importing step3_2
mock_wmill = Mock()
mock_wmill.get_resource.return_value = {
    "host": "localhost",
    "port": 5432,
    "user": "test_user",
    "password": "test_password",
    "dbname": "test_db"
}
sys.modules['wmill'] = mock_wmill

# Import the module under test
import importlib.util
spec = importlib.util.spec_from_file_location(
    "step3_2",
    os.path.join(os.path.dirname(__file__), '../../f/development/3_2_save_chat_history.py')
)
step3_2_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(step3_2_module)
step3_2_main = step3_2_module.main


class TestStep3_2SaveHistory:
    """Test Step 3.2's chat history persistence functionality"""

    @patch('psycopg2.connect')
    def test_successful_message_persistence(self, mock_connect):
        """Test successful saving of user and assistant messages"""
        # Setup mock database
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": "contact-123"}
            },
            user_message="Hello, how are you?",
            llm_result={
                "reply_text": "I'm doing great, thanks for asking!",
                "updated_variables": {}
            },
            send_result={"success": True}
        )

        # Assertions
        assert result["success"] is True
        
        # Verify two INSERT statements were executed (user + assistant)
        assert mock_cursor.execute.call_count == 2
        
        # Verify first call was user message
        first_call = mock_cursor.execute.call_args_list[0]
        assert "INSERT INTO messages" in first_call[0][0]
        assert "'user'" in first_call[0][0]
        assert first_call[0][1] == ("contact-123", "Hello, how are you?")
        
        # Verify second call was assistant message
        second_call = mock_cursor.execute.call_args_list[1]
        assert "INSERT INTO messages" in second_call[0][0]
        assert "'assistant'" in second_call[0][0]
        assert second_call[0][1] == ("contact-123", "I'm doing great, thanks for asking!")
        
        # Verify commit was called
        assert mock_conn.commit.called

    @patch('psycopg2.connect')
    def test_variable_update_persistence(self, mock_connect):
        """Test that LLM-extracted variables are persisted"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": "contact-123"}
            },
            user_message="My email is john@example.com",
            llm_result={
                "reply_text": "Got it, I've saved your email address.",
                "updated_variables": {"email": "john@example.com", "email_verified": False}
            },
            send_result={"success": True}
        )

        assert result["success"] is True
        
        # Should have 3 executes: user message, assistant message, variable update
        assert mock_cursor.execute.call_count == 3
        
        # Verify variable update call
        third_call = mock_cursor.execute.call_args_list[2]
        assert "UPDATE contacts" in third_call[0][0]
        assert "variables = variables ||" in third_call[0][0]
        # Variables are JSON dumped
        import json
        assert json.loads(third_call[0][1][0]) == {"email": "john@example.com", "email_verified": False}
        assert third_call[0][1][1] == "contact-123"

    @patch('psycopg2.connect')
    def test_skip_when_step1_failed(self, mock_connect):
        """Test that history is not saved when Step 1 failed"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_2_main(
            context_payload={
                "proceed": False,  # Step 1 failed
                "reason": "Chatbot not found"
            },
            user_message="Hello",
            llm_result={"reply_text": "Hi"},
            send_result={"success": True}
        )

        # Should not save
        assert result["success"] is False
        assert "Step 1 failed" in result["error"]
        
        # Database should not be touched
        assert not mock_cursor.execute.called

    @patch('psycopg2.connect')
    def test_skip_when_step2_failed(self, mock_connect):
        """Test that history is not saved when Step 2 (LLM) failed"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": "contact-123"}
            },
            user_message="Hello",
            llm_result={
                "error": "LLM quota exceeded"  # Step 2 failed
            },
            send_result={"success": True}
        )

        assert result["success"] is False
        assert "Step 2 failed" in result["error"]
        assert not mock_cursor.execute.called

    @patch('psycopg2.connect')
    def test_skip_when_step3_failed(self, mock_connect):
        """Test that history is not saved when Step 3 (send to WhatsApp) failed"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": "contact-123"}
            },
            user_message="Hello",
            llm_result={"reply_text": "Hi there!"},
            send_result={
                "success": False,  # Message not delivered!
                "error": "WhatsApp API error"
            }
        )

        assert result["success"] is False
        assert "Step 3 failed" in result["error"]
        assert "message not delivered" in result["error"]
        assert not mock_cursor.execute.called

    @patch('psycopg2.connect')
    def test_database_connection_error(self, mock_connect):
        """Test handling of database connection failures"""
        # Simulate connection failure
        mock_connect.side_effect = Exception("Connection refused")

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": "contact-123"}
            },
            user_message="Hello",
            llm_result={"reply_text": "Hi"},
            send_result={"success": True}
        )

        assert result["success"] is False
        assert "Connection refused" in result["error"]

    @patch('psycopg2.connect')
    def test_database_insert_error(self, mock_connect):
        """Test handling of database insert failures"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Simulate INSERT failure
        mock_cursor.execute.side_effect = Exception("Foreign key constraint violation")

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": "contact-999"}  # Non-existent contact
            },
            user_message="Hello",
            llm_result={"reply_text": "Hi"},
            send_result={"success": True}
        )

        assert result["success"] is False
        assert "Foreign key constraint violation" in result["error"]

    @patch('psycopg2.connect')
    def test_empty_reply_text_handling(self, mock_connect):
        """Test handling when LLM returns no reply text"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": "contact-123"}
            },
            user_message="Hello",
            llm_result={
                "reply_text": None,  # No reply text
                "updated_variables": {}
            },
            send_result={"success": True}
        )

        assert result["success"] is True
        
        # Should only insert user message (not assistant message)
        assert mock_cursor.execute.call_count == 1
        
        # Verify only user message was inserted
        first_call = mock_cursor.execute.call_args_list[0]
        assert "'user'" in first_call[0][0]

    @patch('psycopg2.connect')
    def test_no_variable_updates(self, mock_connect):
        """Test that no UPDATE is executed when there are no variable updates"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": "contact-123"}
            },
            user_message="Hello",
            llm_result={
                "reply_text": "Hi there!",
                "updated_variables": {}  # No new variables
            },
            send_result={"success": True}
        )

        assert result["success"] is True
        
        # Should only have 2 executes (user + assistant messages, no variable update)
        assert mock_cursor.execute.call_count == 2

    @patch('psycopg2.connect')
    def test_conversation_threading(self, mock_connect):
        """Test that messages maintain conversation threading via contact_id"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        contact_id = "contact-abc-123"

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": contact_id}
            },
            user_message="What's the weather?",
            llm_result={
                "reply_text": "It's sunny today!",
                "updated_variables": {}
            },
            send_result={"success": True}
        )

        assert result["success"] is True
        
        # Both messages should have the same contact_id
        user_call = mock_cursor.execute.call_args_list[0]
        assistant_call = mock_cursor.execute.call_args_list[1]
        
        assert user_call[0][1][0] == contact_id
        assert assistant_call[0][1][0] == contact_id

    @patch('psycopg2.connect')
    def test_cleanup_on_error(self, mock_connect):
        """Test that database connections are properly closed on error"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Simulate error during execution
        mock_cursor.execute.side_effect = Exception("Test error")

        result = step3_2_main(
            context_payload={
                "proceed": True,
                "user": {"id": "contact-123"}
            },
            user_message="Hello",
            llm_result={"reply_text": "Hi"},
            send_result={"success": True}
        )

        assert result["success"] is False
        
        # Verify cleanup was called
        assert mock_cursor.close.called
        assert mock_conn.close.called


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
File: f/development/utils/web_crawler.py
================================================
"""
Web Crawler with Auto-Discovery and Relevance Scoring

This utility crawls a website starting from a base URL, discovers linked pages,
and scores them by relevance for knowledge base ingestion. It implements the
hybrid approach: auto-discover + manual selection.

Usage:
Called by the API server or frontend when user wants to crawl a website
for knowledge base ingestion.

Features:
- Respects robots.txt
- Rate limiting (1 request/second)
- Relevance scoring algorithm
- Depth-based crawling
- Same-domain restriction
"""

import time
import urllib.parse
import urllib.robotparser
from typing import Dict, List, Any, Set
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup


def main(
    base_url: str,
    max_depth: int = 2,
    max_pages: int = 50,
    same_domain_only: bool = True,
    filter_keywords: List[str] = None
) -> Dict[str, Any]:
    """
    Discover links from a base URL and score them by relevance.

    Args:
        base_url: Starting URL (e.g., https://example.com)
        max_depth: How many levels deep to crawl (default: 2)
        max_pages: Maximum pages to discover (default: 50)
        same_domain_only: Only crawl pages on same domain (default: True)
        filter_keywords: Keywords to boost relevance (e.g., ['faq', 'docs'])

    Returns:
        {
            "discovered_urls": [
                {
                    "url": str,
                    "title": str,
                    "relevance_score": float,  # 0-1
                    "depth": int,
                    "content_preview": str,  # first 200 chars
                    "suggested": bool  # auto-selected if score > 0.5
                },
                ...
            ],
            "total_discovered": int,
            "crawl_time_seconds": float,
            "base_domain": str,
            "robots_txt_respected": bool
        }
    """

    start_time = time.time()

    # Parse base URL
    parsed_base = urlparse(base_url)
    base_domain = parsed_base.netloc

    # Default keywords that indicate valuable content
    if filter_keywords is None:
        filter_keywords = [
            'faq', 'docs', 'documentation', 'support', 'help',
            'about', 'guide', 'tutorial', 'api', 'reference'
        ]

    # Check robots.txt
    robots_txt_respected = True
    robot_parser = urllib.robotparser.RobotFileParser()
    robots_url = f"{parsed_base.scheme}://{parsed_base.netloc}/robots.txt"

    try:
        robot_parser.set_url(robots_url)
        robot_parser.read()
    except Exception as e:
        print(f"Warning: Could not read robots.txt from {robots_url}: {e}")
        robots_txt_respected = False

    # Data structures for crawling
    discovered_urls: List[Dict[str, Any]] = []
    visited: Set[str] = set()
    to_visit: List[tuple] = [(base_url, 0)]  # (url, depth)

    print(f"Starting crawl of {base_url} (max_depth={max_depth}, max_pages={max_pages})")

    while to_visit and len(discovered_urls) < max_pages:
        current_url, current_depth = to_visit.pop(0)

        # Skip if already visited
        if current_url in visited:
            continue

        # Skip if too deep
        if current_depth > max_depth:
            continue

        # Check robots.txt
        if robots_txt_respected and not robot_parser.can_fetch("*", current_url):
            print(f"Skipping {current_url} (blocked by robots.txt)")
            continue

        visited.add(current_url)

        # Fetch page with rate limiting
        try:
            time.sleep(1)  # Rate limit: 1 request/second

            response = requests.get(
                current_url,
                headers={'User-Agent': 'Knowledge Crawler/1.0'},
                timeout=10,
                allow_redirects=True
            )
            response.raise_for_status()

            # Only process HTML pages
            content_type = response.headers.get('Content-Type', '')
            if 'text/html' not in content_type:
                print(f"Skipping {current_url} (not HTML: {content_type})")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract page info
            title = soup.find('title')
            title_text = title.string.strip() if title else urlparse(current_url).path

            # Get content preview (first 200 chars of visible text)
            content_preview = ""
            for text in soup.stripped_strings:
                content_preview += text + " "
                if len(content_preview) >= 200:
                    break
            content_preview = content_preview[:200].strip()

            # Calculate relevance score
            relevance_score = calculate_relevance_score(
                current_url,
                title_text,
                current_depth,
                base_domain,
                filter_keywords
            )

            # Add to discovered URLs
            discovered_urls.append({
                "url": current_url,
                "title": title_text,
                "relevance_score": round(relevance_score, 2),
                "depth": current_depth,
                "content_preview": content_preview,
                "suggested": relevance_score > 0.5
            })

            print(f"✓ Discovered: {current_url} (score: {relevance_score:.2f}, depth: {current_depth})")

            # Find links to crawl next (only if not at max depth)
            if current_depth < max_depth:
                for link in soup.find_all('a', href=True):
                    absolute_url = urljoin(current_url, link['href'])

                    # Normalize URL (remove fragments)
                    parsed_url = urlparse(absolute_url)
                    normalized_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
                    if parsed_url.query:
                        normalized_url += f"?{parsed_url.query}"

                    # Skip if already visited or queued
                    if normalized_url in visited:
                        continue

                    # Check same domain restriction
                    if same_domain_only and urlparse(normalized_url).netloc != base_domain:
                        continue

                    # Skip common non-content URLs
                    if should_skip_url(normalized_url):
                        continue

                    # Add to queue
                    to_visit.append((normalized_url, current_depth + 1))

        except requests.RequestException as e:
            print(f"Error fetching {current_url}: {e}")
            continue
        except Exception as e:
            print(f"Unexpected error processing {current_url}: {e}")
            continue

    # Sort by relevance score (highest first)
    discovered_urls.sort(key=lambda x: x['relevance_score'], reverse=True)

    crawl_time = time.time() - start_time

    return {
        "discovered_urls": discovered_urls,
        "total_discovered": len(discovered_urls),
        "crawl_time_seconds": round(crawl_time, 2),
        "base_domain": base_domain,
        "robots_txt_respected": robots_txt_respected
    }


def calculate_relevance_score(
    url: str,
    title: str,
    depth: int,
    base_domain: str,
    keywords: List[str]
) -> float:
    """
    Calculate relevance score for a URL.

    Scoring algorithm:
    - Same domain: +0.4
    - Keywords in path: +0.3
    - Depth penalty: -0.1 per level
    - Response time bonus: +0.1 (not implemented here, would need timing)

    Returns:
        Score between 0 and 1
    """
    score = 0.0

    # Same domain bonus
    parsed = urlparse(url)
    if parsed.netloc == base_domain:
        score += 0.4

    # Keywords in URL path or title
    url_lower = url.lower()
    title_lower = title.lower()

    for keyword in keywords:
        if keyword in url_lower or keyword in title_lower:
            score += 0.3
            break  # Only count once

    # Depth penalty
    score -= (depth * 0.1)

    # Ensure score is between 0 and 1
    return max(0.0, min(1.0, score))


def should_skip_url(url: str) -> bool:
    """
    Check if URL should be skipped (non-content pages).

    Returns:
        True if URL should be skipped
    """
    url_lower = url.lower()

    skip_patterns = [
        '/login', '/signin', '/signup', '/register',
        '/cart', '/checkout', '/account', '/profile',
        '/admin', '/wp-admin', '/dashboard',
        '.pdf', '.jpg', '.png', '.gif', '.zip', '.mp4',
        'javascript:', 'mailto:', 'tel:',
        '#', '/search?', '/tag/', '/category/',
        '/page/', '/wp-content/', '/wp-includes/'
    ]

    for pattern in skip_patterns:
        if pattern in url_lower:
            return True

    return False


================================================
File: f/development/utils/web_crawler.script.lock
================================================
# py: 3.12
beautifulsoup4==4.14.3
bs4==0.0.2
certifi==2025.11.12
charset-normalizer==3.4.4
idna==3.11
requests==2.32.5
soupsieve==2.8.1
typing-extensions==4.15.0
urllib3==2.6.2

================================================
File: rt.d.ts
================================================
declare namespace RT {
  type Ably = {
    apiKye: string
  }

  type Abstractapi = {
    apiKey: string
  }

  type Accelo = {
    clientId: string,
    deployment: string,
    clientSecret: string
  }

  type Actimo = {
    apiKey: string
  }

  type Acumbamail = {
    authToken: string
  }

  type Adhook = {
    token: string
  }

  type Adrapid = {
    apiToken: string
  }

  type AeroWorkflow = {
    apiKey: string
  }

  type Ai21 = {
    apiKey: string
  }

  type Airtable = {
    apiKey: string
  }

  type AirtableTable = {
    baseId: string,
    tableName: string
  }

  type AnsibleInventory = {
    content: string
  }

  type Anthropic = {
    apiKey: string
  }

  type Apify = {
    token: string
  }

  type ApifyApiKey = {
    api_key: string
  }

  type ApifyWebhookConfig = {
    url: string,
    token: string
  }

  type ApiKeyAuth = {
    api_key_header: string,
    api_key_secret: string
  }

  type Apollo = {
    apiKey: string
  }

  type Appwrite = {
    key: string,
    project: string,
    endpoint: string,
    self_signed: boolean
  }

  type ArcgisAccount = {
    password: string,
    username: string
  }

  type Asana = {
    token: string
  }

  type Assemblyai = {
    apiKey: string
  }

  type Attio = {
    token: string
  }

  type Aws = {
    region: string,
    awsAccessKeyId: string,
    awsSecretAccessKey: string
  }

  type AwsBedrock = {
    apiKey: string,
    region: string,
    awsAccessKeyId: string,
    awsSecretAccessKey: string
  }

  type AwsOidc = {
    region: string,
    roleArn: string
  }

  type Azure = {
    azureClientId: string,
    azureTenantId: string,
    azureClientSecret: string
  }

  type AzureBlob = {
    useSSL: boolean,
    endpoint: string,
    accessKey: string,
    accountName: string,
    containerName: string
  }

  type AzureOauth = {
    token: string
  }

  type AzureOpenai = {
    apiKey: string,
    baseUrl: string
  }

  type AzureWorkloadIdentity = {
    useSSL: boolean,
    accountName: string,
    containerName: string
  }

  type BambooHr = {
    apiKey: string,
    companyDomain: string
  }

  type Baremetrics = {
    apiKey: string
  }

  type Baserow = {
    token: string,
    base_url: string
  }

  type BaserowTable = {
    table_id: number,
    database_id: number
  }

  type BasicHttpAuth = {
    password: string,
    username: string
  }

  type BasisTheory = {
    apiKey: string
  }

  type Beamer = {
    apiKey: string
  }

  type Bigquery = {
    type: string,
    auth_uri: string,
    client_id: string,
    token_uri: string,
    project_id: string,
    private_key: string,
    client_email: string,
    private_key_id: string,
    client_x509_cert_url: string,
    auth_provider_x509_cert_url: string
  }

  type Bitbucket = {
    password: string,
    username: string
  }

  type Bitly = {
    token: string
  }

  type Bluesky = {
    password: string,
    username: string
  }

  type Botify = {
    token: string
  }

  type Box = {
    token: string
  }

  type Brevo = {
    apiKey: string
  }

  type Brex = {
    token: string
  }

  type Buttondown = {
    token: string
  }

  type Cacertificate = {
    certificate: string
  }

  type Calendly = {
    token: string
  }

  type Campayn = {
    apiKey: string
  }

  type Certopus = {
    apiKey: string
  }

  type Chromadb = {
    ssl: boolean,
    host: string,
    port: number,
    tenant: string,
    database: string
  }

  type Circleci = {
    token: string
  }

  type Clerk = {
    apiKey: string
  }

  type Clickhouse = {
    host: string,
    password: string,
    username: string
  }

  type Clickup = {
    token: string
  }

  type Cloudflare = {
    key: string,
    email: string,
    token: string
  }

  type Cockroachdb = {
    token: string
  }

  type Codat = {
    encodedKey: string
  }

  type Cohere = {
    apiKey: string
  }

  type ComapeoServer = {
    server_url: string,
    access_token: string
  }

  type Confluence = {
    email: string,
    baseUrl: string,
    apiToken: string
  }

  type Contentful = {
    spaceId: string,
    accessToken: string,
    environment: string
  }

  type Contiguity = {
    token: string
  }

  type Convertkit = {
    apiSecret: string
  }

  type Currencyapi = {
    apiKey: string
  }

  type Customai = {
    api_key: string,
    base_url: string
  }

  type Datadog = {
    apiKey: string,
    appKey: string,
    apiBase: string
  }

  type Datocms = {
    apiKey: string
  }

  type Deel = {
    apiKey: string
  }

  type DeepInfra = {
    token: string
  }

  type Deepl = {
    apiKey: string,
    baseUrl: string
  }

  type Deepseek = {
    api_key: string
  }

  type Digitalocean = {
    token: string
  }

  type DiscordBotConfiguration = {
    public_key: string,
    application_id: string
  }

  type DiscordWebhook = {
    webhook_url: string
  }

  type Discourse = {
    apiKey: string,
    apiUsername: string,
    defaultHost: string
  }

  type Docspring = {
    tokenId: string,
    tokenSecret: string
  }

  type Dust = {
    apiKey: string,
    workspaceId: string
  }

  type Dynatrace = {
    accessToken: string,
    environmentId: string,
    environmentUrl: string
  }

  type Edgedb = {
    dsn: string,
    host: string,
    port: number,
    user: string,
    database: string,
    password: string,
    secretKey: string,
    instanceName: string
  }

  type Enode = {
    token: string
  }

  type Exa = {
    apiKey: string
  }

  type Faunadb = {
    region: string,
    secret: string
  }

  type Figma = {
    token: string
  }

  type Firebase = {
    appId: string,
    apiKey: string,
    projectId: string,
    authDomain: string,
    measurementId: string,
    storageBucket: string,
    messagingSenderId: string
  }

  type Fly = {
    token: string
  }

  type Formstack = {
    token: string
  }

  type Foxentry = {
    apiKey: string
  }

  type Freshdesk = {
    apiKey: string,
    baseUrl: string
  }

  type Funkwhale = {
    token: string,
    baseUrl: string
  }

  type Gcal = {
    token: string
  }

  type Gcloud = {
    type: string,
    auth_uri: string,
    client_id: string,
    token_uri: string,
    project_id: string,
    private_key: string,
    client_email: string,
    private_key_id: string,
    client_x509_cert_url: string,
    auth_provider_x509_cert_url: string
  }

  type GcloudStorage = {
    bucket: string,
    serviceAccountKey: any
  }

  type GcpServiceAccount = {
    type: string,
    auth_uri: string,
    client_id: string,
    token_uri: string,
    project_id: string,
    private_key: string,
    client_email: string,
    private_key_id: string,
    client_x509_cert_url: string,
    auth_provider_x509_cert_url: string
  }

  type Gdocs = {
    token: string
  }

  type Gdrive = {
    token: string
  }

  type Gforms = {
    token: string
  }

  type Gfw = {
    api_key: string
  }

  type Ghostcms = {
    apiKey: string,
    apiUrl: string
  }

  type Gitbook = {
    token: string
  }

  type Github = {
    token: string
  }

  type Gitlab = {
    token: string,
    baseUrl: string
  }

  type GitRepository = {
    url: string,
    branch: string,
    folder: string,
    gpg_key: any,
    is_github_app: boolean
  }

  type Gmail = {
    token: string
  }

  type Googleai = {
    api_key: string
  }

  type Gorgias = {
    apiKey: string,
    domain: string,
    username: string
  }

  type GpgKey = {
    email: string,
    passphrase: string,
    private_key: string
  }

  type Graphql = {
    base_url: string,
    bearer_token: string,
    custom_headers: any
  }

  type Greip = {
    apiKey: string
  }

  type Grist = {
    host: string,
    apiKey: string
  }

  type Groq = {
    api_key: string
  }

  type Groqai = {
    api_key: string
  }

  type Gsheets = {
    token: string
  }

  type Gworkspace = {
    token: string
  }

  type Holded = {
    apiKey: string
  }

  type Hubspot = {
    token: string
  }

  type IfsCloudOidc = {
    server: string,
    clientId: string,
    oidcPath: string,
    clientSecret: string
  }

  type Intercom = {
    token: string,
    apiVersion: string
  }

  type Ipinfo = {
    token: string
  }

  type Jira = {
    domain: string,
    password: string,
    username: string
  }

  type Jotform = {
    apiKey: string,
    baseUrl: string
  }

  type JsonSchema = {
    schema: any
  }

  type Kafka = {
    brokers: string[],
    security: any
  }

  type Klaviyo = {
    apiKey: string
  }

  type Kobotoolbox = {
    api_key: string,
    server_url: string
  }

  type Kustomer = {
    apiKey: string
  }

  type Langfuse = {
    base_url: string,
    public_key: string,
    secret_key: string
  }

  type Ldap = {
    server: string,
    use_ssl: boolean,
    bind_user: string,
    ssl_validate: boolean,
    bind_password: string
  }

  type Leonardoai = {
    apiKey: string
  }

  type Linear = {
    apiKey: string
  }

  type Linkding = {
    token: string,
    baseUrl: string
  }

  type Linkedin = {
    token: string
  }

  type Linode = {
    token: string
  }

  type Lumaai = {
    apiKey: string
  }

  type Magento = {
    accessToken: string,
    consumerKey: string,
    consumerSecret: string,
    accessTokenSecret: string
  }

  type Mailchimp = {
    server: string,
    api_key: string
  }

  type Mailerlite = {
    apiToken: string
  }

  type Mailgun = {
    api_key: string
  }

  type Mastodon = {
    token: string,
    baseUrl: string
  }

  type Matrix = {
    token: string,
    baseUrl: string
  }

  type Mcp = {
    url: string,
    name: string,
    token: string,
    headers: any
  }

  type Meteosource = {
    tier: string,
    apiKey: string
  }

  type Mezmo = {
    apiKey: string
  }

  type Miro = {
    token: string
  }

  type Mistral = {
    apiKey: string
  }

  type Mollie = {
    token: string
  }

  type Mongodb = {
    db: string,
    tls: boolean,
    servers: any,
    credential: any
  }

  type MongodbRest = {
    api_key: string,
    endpoint: string
  }

  type Motimate = {
    token: string
  }

  type Mqtt = {
    tls: {
    enabled: boolean,
    ca_certificate: string,
    pkcs12_client_certificate: string,
    pkcs12_certificate_password: string
  },
    port: number,
    broker: string,
    credentials: {
    password: string,
    username: string
  }
  }

  type MsSqlServer = {
    host: string,
    port: number,
    user: string,
    dbname: string,
    ca_cert: string,
    encrypt: boolean,
    password: string,
    aad_token: any,
    trust_cert: boolean,
    instance_name: string
  }

  type MsTeamsWebhook = {
    webhook_url: string
  }

  type Mysql = {
    ssl: boolean,
    host: string,
    port: number,
    user: string,
    database: string,
    password: string,
    root_certificate_pem: string
  }

  type Nats = {
    auth: any,
    servers: string[],
    require_tls: boolean
  }

  type Neondb = {
    apiKey: string
  }

  type Netbox = {
    url: string,
    token: string
  }

  type Netlify = {
    token: string
  }

  type Newsapi = {
    apiKey: string
  }

  type Nextcloud = {
    baseUrl: string,
    password: string,
    username: string
  }

  type Nocodb = {
    table: string,
    apiUrl: string,
    xc_token: string,
    workspace: string
  }

  type Notion = {
    token: string
  }

  type Odk = {
    base_url: string,
    password: string,
    username: string,
    default_project_id: number
  }

  type Openai = {
    api_key: string,
    organization_id: string
  }

  type Openrouter = {
    api_key: string
  }

  type Oracledb = {
    user: string,
    database: string,
    password: string
  }

  type Pandadoc = {
    apiKey: string
  }

  type Paychex = {
    client_id: string,
    client_secret: string
  }

  type Paylocity = {
    token: string
  }

  type Paypal = {
    clientId: string,
    clientSecret: string
  }

  type Persona = {
    apiKey: string
  }

  type Personio = {
    clientId: string,
    clientSecret: string
  }

  type Phrase = {
    token: string,
    baseUrl: string
  }

  type Pinecone = {
    apiKey: string,
    environment: string
  }

  type Pinterest = {
    token: string
  }

  type Pipedrive = {
    apiToken: string
  }

  type Planetscale = {
    serviceToken: string,
    serviceTokenId: string
  }

  type Postgresql = {
    host: string,
    port: number,
    user: string,
    dbname: string,
    sslmode: string,
    password: string,
    root_certificate_pem: string
  }

  type Pushover = {
    user: string,
    token: string
  }

  type Qovery = {
    apiKey: string
  }

  type Quickbooks = {
    token: string,
    realmId: string,
    isSandBox: boolean
  }

  type Readme = {
    apiKey: string
  }

  type Recraft = {
    apiKey: string
  }

  type Reddit = {
    clientId: string,
    password: string,
    username: string,
    userAgent: string,
    clientSecret: string
  }

  type Render = {
    apiKey: string
  }

  type Replicate = {
    token: string
  }

  type Resend = {
    token: string
  }

  type Rss = {
    url: string
  }

  type S3 = {
    port: number,
    bucket: string,
    region: string,
    useSSL: boolean,
    endPoint: string,
    accessKey: string,
    pathStyle: boolean,
    secretKey: string
  }

  type S3AwsOidc = {
    bucket: string,
    region: string,
    roleArn: string
  }

  type SageIntacct = {
    token: string
  }

  type Salesflare = {
    apiKey: string
  }

  type Segment = {
    token: string,
    baseUrl: string
  }

  type Sendgrid = {
    token: string
  }

  type Sensortower = {
    base_url: string,
    auth_token: string
  }

  type Sentry = {
    token: string,
    region: string,
    organizationSlug: string
  }

  type Shopify = {
    token: string,
    store_name: string
  }

  type Shutterstock = {
    token: string
  }

  type SignatureAuth = {
    secret_key: string,
    signature_provider: string,
    authentication_config: {
    encoding: string,
    algorithm: string,
    signature_prefix: string,
    signature_header_name: string
  }
  }

  type Signoz = {
    apiKey: string,
    baseUrl: string
  }

  type Slack = {
    token: string
  }

  type Smartsheet = {
    token: string,
    baseUrl: string
  }

  type Smtp = {
    host: string,
    port: number,
    user: string,
    password: string
  }

  type Snowflake = {
    role: string,
    schema: string,
    database: string,
    username: string,
    warehouse: string,
    public_key: string,
    private_key: string,
    account_identifier: string
  }

  type Speechify = {
    token: string
  }

  type Spotify = {
    token: string
  }

  type Square = {
    token: string
  }

  type Stripe = {
    token: string
  }

  type Supabase = {
    key: string,
    url: string
  }

  type Surrealdb = {
    url: string,
    token: string
  }

  type Taskade = {
    token: string
  }

  type Telegram = {
    token: string
  }

  type Telnyx = {
    apiKey: string
  }

  type Terra = {
    devId: string,
    apiKey: string
  }

  type TheirStack = {
    apiKey: string
  }

  type Todoist = {
    token: string
  }

  type Togetherai = {
    api_key: string
  }

  type Toggl = {
    token: string
  }

  type Tomorrow = {
    apiKey: string
  }

  type Trello = {
    key: string,
    token: string
  }

  type Tripadvisor = {
    apiKey: string
  }

  type Turso = {
    apiToken: string
  }

  type Twilio = {
    token: string,
    accountSid: string
  }

  type TwilioMessageTemplate = {
    auth_token: string,
    recipients: string[],
    account_sid: string,
    content_sid: string,
    origin_number: string,
    message_service_sid: string
  }

  type Typeform = {
    token: string,
    baseUrl: string
  }

  type Ultravox = {
    apiKey: string
  }

  type Vectara = {
    apiKey: string
  }

  type Vercel = {
    token: string
  }

  type Visma = {
    token: string
  }

  type Weatherapi = {
    apiKey: string
  }

  type Webflow = {
    token: string
  }

  type Webscrapingai = {
    apiKey: string
  }

  type Woocommerce = {
    url: string,
    version: string,
    consumerKey: string,
    consumerSecret: string,
    queryStringAuth: boolean
  }

  type Xata = {
    apiKey: string
  }

  type Xero = {
    token: string
  }

  type Yelp = {
    apiKey: string
  }

  type Ynab = {
    token: string
  }

  type Zendesk = {
    password: string,
    username: string,
    subdomain: string
  }

  type Zixflow = {
    apiKey: string
  }

  type Zoho = {
    token: string
  }

  type Zoom = {
    accountId: string,
    oauthClientId: string,
    oauthClientSecret: string,
    webhookSecretToken: string
  }

  type Zuplo = {
    apiKey: string
  }
}

================================================
File: tests/conftest.py
================================================
"""
Pytest configuration and shared fixtures for testing the WhatsApp chatbot flow.

This module provides:
- Database fixtures (connection, transaction management, cleanup)
- Mock fixtures (Windmill, LLM, WhatsApp API)
- Test data factories
- Utilities for testing
"""

import os
import sys
import pytest
import psycopg2
from psycopg2.extras import RealDictCursor
from typing import Dict, Any, Generator
from pathlib import Path
from unittest.mock import Mock, patch

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Import test harness modules
from tests.test_harness.windmill_mock import WindmillMock
from tests.test_harness.llm_mock import LLMMock
from tests.test_harness.whatsapp_mock import WhatsAppMock


# ============================================================================
# CONFIGURATION
# ============================================================================

@pytest.fixture(scope="session")
def test_db_config() -> Dict[str, Any]:
    """Database configuration for tests."""
    return {
        "host": os.getenv("TEST_DB_HOST", "localhost"),
        "port": int(os.getenv("TEST_DB_PORT", "5434")),
        "user": os.getenv("TEST_DB_USER", "test_user"),
        "password": os.getenv("TEST_DB_PASSWORD", "test_password"),
        "dbname": os.getenv("TEST_DB_NAME", "test_business_logic"),
        "sslmode": "disable",
    }


@pytest.fixture(scope="session")
def test_env_vars() -> Dict[str, str]:
    """Environment variables for tests."""
    return {
        "WHATSAPP_PHONE_NUMBER_ID": "test_phone_123",
        "WHATSAPP_ACCESS_TOKEN": "test_token_xyz",
        "OWNER_EMAIL": "test@example.com",
        "GOOGLE_API_KEY": "test_google_key",
        "OPENAI_API_KEY": "test_openai_key",
        "WM_WORKSPACE": "test_workspace",
        "WM_TOKEN": "test_windmill_token",
        "WM_BASE_URL": "http://localhost:8000",
    }


@pytest.fixture(scope="session", autouse=True)
def set_test_env_vars(test_env_vars):
    """Automatically set environment variables for all tests."""
    import os

    # Store original values
    original_values = {}
    for key, value in test_env_vars.items():
        original_values[key] = os.environ.get(key)
        os.environ[key] = value

    yield

    # Restore original values
    for key, original_value in original_values.items():
        if original_value is None:
            os.environ.pop(key, None)
        else:
            os.environ[key] = original_value


# ============================================================================
# DATABASE FIXTURES
# ============================================================================

@pytest.fixture(scope="session")
def db_connection(test_db_config):
    """
    Create a database connection for the entire test session.
    This connection is used to reset the database between tests.
    """
    conn = psycopg2.connect(**test_db_config)
    conn.autocommit = True
    yield conn
    conn.close()


@pytest.fixture(scope="function")
def db_cursor(db_connection):
    """
    Provide a cursor for each test function.
    Uses a transaction that gets rolled back after each test.
    """
    # Start a transaction
    db_connection.autocommit = False
    cursor = db_connection.cursor(cursor_factory=RealDictCursor)
    
    yield cursor
    
    # Rollback transaction after test
    db_connection.rollback()
    cursor.close()
    db_connection.autocommit = True


@pytest.fixture(scope="function")
def clean_db(db_connection, test_env_vars):
    """
    Reset database to a clean state before each test.
    Runs the drop, create, and seed SQL scripts.
    """
    cursor = db_connection.cursor()
    
    # Read and execute SQL files
    sql_dir = PROJECT_ROOT / "db"
    
    # Drop tables
    with open(sql_dir / "drop.sql") as f:
        cursor.execute(f.read())
    
    # Create schema
    with open(sql_dir / "create.sql") as f:
        cursor.execute(f.read())
    
    # Seed data (with environment variable substitution)
    with open(sql_dir / "seed.sql") as f:
        seed_sql = f.read()
        # Replace environment variables
        for key, value in test_env_vars.items():
            seed_sql = seed_sql.replace(f"${{{key}}}", value)
        cursor.execute(seed_sql)
    
    cursor.close()
    
    yield  # Test runs here
    
    # No cleanup needed - next test will reset


@pytest.fixture
def db_with_data(clean_db, db_cursor):
    """
    Provides a database with seed data and a cursor for queries.
    This is the most commonly used database fixture.
    """
    return db_cursor


@pytest.fixture
def db_with_autocommit(clean_db, db_connection):
    """
    Provides a database cursor with autocommit enabled.

    Use this fixture for tests that call external scripts/processes
    that create their own database connections, as they need to see
    committed data (not just data in the current transaction).

    WARNING: Changes made with this fixture are NOT automatically
    rolled back, so clean_db is used to reset between tests.
    """
    cursor = db_connection.cursor(cursor_factory=RealDictCursor)
    yield cursor
    cursor.close()


# ============================================================================
# MOCK FIXTURES
# ============================================================================

@pytest.fixture
def mock_wmill():
    """Mock Windmill functions (get_resource, get_variable)."""
    return WindmillMock()


@pytest.fixture
def mock_llm():
    """Mock LLM providers (OpenAI, Google)."""
    return LLMMock()


@pytest.fixture
def mock_whatsapp():
    """Mock WhatsApp API calls."""
    return WhatsAppMock()


@pytest.fixture
def mock_all_external(mock_wmill, mock_llm, mock_whatsapp):
    """
    Mock all external dependencies at once.
    Useful for integration tests.
    """
    with patch('wmill.get_resource', mock_wmill.get_resource), \
         patch('wmill.get_variable', mock_wmill.get_variable), \
         patch('openai.OpenAI', mock_llm.get_openai_client), \
         patch('google.generativeai.GenerativeModel', mock_llm.get_google_client), \
         patch('requests.post', mock_whatsapp.post):
        
        yield {
            "wmill": mock_wmill,
            "llm": mock_llm,
            "whatsapp": mock_whatsapp,
        }


# ============================================================================
# TEST DATA FACTORIES
# ============================================================================

@pytest.fixture
def sample_webhook_payload():
    """Generate a sample WhatsApp webhook payload."""
    return {
        "object": "whatsapp_business_account",
        "entry": [
            {
                "id": "test_entry_123",
                "changes": [
                    {
                        "value": {
                            "messaging_product": "whatsapp",
                            "metadata": {
                                "display_phone_number": "15551234567",
                                "phone_number_id": "test_phone_123"
                            },
                            "contacts": [
                                {
                                    "profile": {
                                        "name": "Test User"
                                    },
                                    "wa_id": "15559876543"
                                }
                            ],
                            "messages": [
                                {
                                    "from": "15559876543",
                                    "id": "wamid.test.message.001",
                                    "timestamp": "1234567890",
                                    "text": {
                                        "body": "Hello, can you help me?"
                                    },
                                    "type": "text"
                                }
                            ]
                        },
                        "field": "messages"
                    }
                ]
            }
        ]
    }


@pytest.fixture
def sample_context_payload():
    """Generate a sample context payload from Step 1."""
    return {
        "proceed": True,
        "webhook_event_id": 1,
        "chatbot": {
            "id": "22222222-2222-2222-2222-222222222222",
            "organization_id": "11111111-1111-1111-1111-111111111111",
            "name": "Test Bot",
            "system_prompt": "You are a helpful assistant.",
            "persona": "Friendly and professional",
            "model_name": "gemini-pro",
            "temperature": 0.7,
            "wa_token": "test_token_xyz",
            "rag_config": {
                "enabled": False,
                "index": None,
                "namespace": None
            }
        },
        "user": {
            "id": "44444444-4444-4444-4444-444444444444",
            "phone": "15559876543",
            "name": "Test User",
            "variables": {},
            "tags": []
        },
        "history": [],
        "tools": [],
        "usage_info": {
            "has_quota": True,
            "messages_used": 0,
            "tokens_used": 0,
            "messages_remaining": 1000,
            "tokens_remaining": 1000000
        }
    }


@pytest.fixture
def sample_llm_result():
    """Generate a sample LLM response from Step 2."""
    return {
        "reply_text": "Hello! I'd be happy to help you. What do you need assistance with?",
        "updated_variables": {},
        "usage_info": {
            "provider": "google",
            "model": "gemini-pro",
            "tokens_input": 150,
            "tokens_output": 80,
        }
    }


@pytest.fixture
def gemini_simple_response():
    """Simple text response from Gemini (no tools)"""
    return {
        "reply_text": "Hello! How can I help?",
        "updated_variables": {},
        "tool_executions": [],
        "retrieved_sources": [],
        "usage_info": {
            "provider": "google",
            "model": "gemini-pro",
            "tokens_input": 50,
            "tokens_output": 20,
            "tool_calls": 0,
            "rag_used": False,
            "chunks_retrieved": 0,
            "iterations": 1
        }
    }


@pytest.fixture
def gemini_tool_call_response():
    """Gemini response with tool call"""
    return {
        "reply_text": "I've checked the weather for you.",
        "updated_variables": {},
        "tool_executions": [
            {
                "tool_name": "get_weather",
                "arguments": {"city": "NYC"},
                "result": {"temperature": 72, "condition": "sunny"}
            }
        ],
        "retrieved_sources": [],
        "usage_info": {
            "provider": "google",
            "model": "gemini-pro",
            "tokens_input": 100,
            "tokens_output": 50,
            "tool_calls": 1,
            "rag_used": False,
            "chunks_retrieved": 0,
            "iterations": 2
        }
    }


@pytest.fixture
def conversation_history(db_with_data, sample_context_payload):
    """Pre-populate conversation history"""
    contact_id = sample_context_payload["user"]["id"]
    db_with_data.execute("""
        INSERT INTO messages (contact_id, direction, content, created_at)
        VALUES
            (%s, 'incoming', 'Hello', NOW() - INTERVAL '2 minutes'),
            (%s, 'outgoing', 'Hi there!', NOW() - INTERVAL '1 minute')
    """, (contact_id, contact_id))
    return contact_id


@pytest.fixture
def openai_embedding_1536():
    """Valid 1536-dimensional embedding for testing"""
    return "[" + ", ".join(["0.1"] * 1536) + "]"


# ============================================================================
# UTILITY FIXTURES
# ============================================================================

@pytest.fixture
def assert_db_state():
    """Helper fixture for asserting database state."""
    def _assert(cursor, table: str, conditions: Dict[str, Any], expected_count: int = 1):
        """
        Assert that a table has the expected number of rows matching conditions.
        
        Args:
            cursor: Database cursor
            table: Table name
            conditions: Dict of column: value conditions
            expected_count: Expected number of matching rows
        """
        where_clauses = " AND ".join(f"{k} = %s" for k in conditions.keys())
        query = f"SELECT COUNT(*) as count FROM {table} WHERE {where_clauses}"
        
        cursor.execute(query, tuple(conditions.values()))
        result = cursor.fetchone()
        
        assert result["count"] == expected_count, \
            f"Expected {expected_count} rows in {table} matching {conditions}, found {result['count']}"
    
    return _assert


@pytest.fixture
def query_helper(db_cursor):
    """Helper fixture for common database queries."""
    class QueryHelper:
        def __init__(self, cursor):
            self.cursor = cursor
        
        def get_organization(self, org_id: str) -> Dict:
            self.cursor.execute("SELECT * FROM organizations WHERE id = %s", (org_id,))
            return self.cursor.fetchone()
        
        def get_chatbot(self, chatbot_id: str) -> Dict:
            self.cursor.execute("SELECT * FROM chatbots WHERE id = %s", (chatbot_id,))
            return self.cursor.fetchone()
        
        def get_contact(self, contact_id: str) -> Dict:
            self.cursor.execute("SELECT * FROM contacts WHERE id = %s", (contact_id,))
            return self.cursor.fetchone()
        
        def get_messages(self, contact_id: str) -> list:
            self.cursor.execute(
                "SELECT * FROM messages WHERE contact_id = %s ORDER BY created_at",
                (contact_id,)
            )
            return self.cursor.fetchall()
        
        def get_usage_logs(self, org_id: str) -> list:
            self.cursor.execute(
                "SELECT * FROM usage_logs WHERE organization_id = %s ORDER BY created_at",
                (org_id,)
            )
            return self.cursor.fetchall()
        
        def get_webhook_event(self, message_id: str) -> Dict:
            self.cursor.execute(
                "SELECT * FROM webhook_events WHERE whatsapp_message_id = %s",
                (message_id,)
            )
            return self.cursor.fetchone()
    
    return QueryHelper(db_cursor)


# ============================================================================
# SESSION HOOKS
# ============================================================================

def pytest_configure(config):
    """Pytest configuration hook."""
    # Add custom markers
    config.addinivalue_line("markers", "unit: Unit tests")
    config.addinivalue_line("markers", "integration: Integration tests")
    config.addinivalue_line("markers", "slow: Slow tests")
    config.addinivalue_line("markers", "db: Tests requiring database")


def pytest_collection_modifyitems(config, items):
    """Modify test collection to add markers automatically."""
    for item in items:
        # Auto-mark database tests
        if "db_cursor" in item.fixturenames or "clean_db" in item.fixturenames:
            item.add_marker(pytest.mark.db)
        
        # Auto-mark integration tests
        if "integration" in str(item.fspath):
            item.add_marker(pytest.mark.integration)


# ============================================================================
# EMBEDDING FIXTURES
# ============================================================================

@pytest.fixture(scope="session")
def real_embeddings() -> Dict[str, Any]:
    """
    Load pre-computed real embeddings from fixture file.

    These embeddings were generated using OpenAI's text-embedding-ada-002 model.
    Use this fixture for tests that need realistic embedding data without
    making API calls.

    Returns:
        Dict with 'metadata', 'documents', and 'queries' keys.
        If fixture file doesn't exist, returns mock embeddings.
    """
    import json

    fixture_path = Path(__file__).parent / "fixtures" / "embeddings.json"

    if fixture_path.exists():
        with open(fixture_path) as f:
            return json.load(f)

    # Return mock embeddings if fixture doesn't exist
    # This allows tests to run even without generating real embeddings
    mock_embedding = [0.1] * 1536

    return {
        "metadata": {
            "model": "mock",
            "dimensions": 1536,
            "description": "Mock embeddings - run generate_embeddings.py to create real ones"
        },
        "documents": [
            {
                "id": "doc_1",
                "title": "Return Policy",
                "content": "Our return policy allows returns within 30 days of purchase.",
                "embedding": mock_embedding
            }
        ],
        "queries": [
            {
                "id": "query_1",
                "text": "How do I return an item?",
                "expected_match": "doc_1",
                "embedding": mock_embedding
            }
        ]
    }


@pytest.fixture
def openai_embedding_1536() -> list:
    """
    Valid 1536-dimensional embedding for testing.

    Use this when you need a single embedding vector for tests.
    """
    return [0.1] * 1536


@pytest.fixture
def sample_document_with_embedding(real_embeddings) -> Dict[str, Any]:
    """
    Get a sample document with its real embedding.

    Returns the first document from the fixture.
    """
    return real_embeddings["documents"][0]


@pytest.fixture
def sample_query_with_embedding(real_embeddings) -> Dict[str, Any]:
    """
    Get a sample query with its real embedding.

    Returns the first query from the fixture.
    """
    return real_embeddings["queries"][0]

================================================
File: tests/unit/test_step1_context_loading.py
================================================
"""
Unit tests for Step 1: Context loading and validation

Tests Step 1's ability to:
- Validate chatbots and load configuration
- Check for duplicate messages (idempotency)
- Load user data and conversation history
- Handle various error conditions
"""

import pytest
import sys
import os
from unittest.mock import Mock, patch, MagicMock
from psycopg2.extras import RealDictCursor

# Add parent directories to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../f/development'))

# Mock wmill module before importing step1
mock_wmill = Mock()
mock_wmill.get_resource.return_value = {
    "host": "localhost",
    "port": 5432,
    "user": "test_user",
    "password": "test_password",
    "dbname": "test_db"
}
sys.modules['wmill'] = mock_wmill

# Import the module under test
import importlib.util
spec = importlib.util.spec_from_file_location(
    "step1",
    os.path.join(os.path.dirname(__file__), '../../f/development/1_whatsapp_context_loading.py')
)
step1_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(step1_module)
step1_main = step1_module.main


class TestStep1ContextLoading:
    """Test Step 1's context loading functionality"""

    @patch('psycopg2.connect')
    def test_duplicate_message_detection(self, mock_connect):
        """Test that duplicate messages are detected via whatsapp_message_id"""
        # Setup mock database connection
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Simulate duplicate message already exists (RealDictCursor returns dict-like rows)
        mock_cursor.fetchone.return_value = {
            "id": 1,
            "status": "completed",
            "processed_at": "2025-01-15 10:00:00"
        }

        result = step1_main(
            whatsapp_phone_id="123456123",
            user_phone="16315551181",
            message_id="ABGGFlA5Fpa",  # Same message ID
            user_name="Test User"
        )

        # Assertions
        assert result["proceed"] is False
        assert result["reason"] == "Duplicate - Already Processed"

    @patch('psycopg2.connect')
    def test_chatbot_not_found(self, mock_connect):
        """Test handling when chatbot doesn't exist for phone_number_id"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Setup responses: no duplicate, create webhook event, but no chatbot found
        mock_cursor.fetchone.side_effect = [
            None,  # No duplicate message
            {"id": 999},   # Webhook event ID created (RealDictCursor returns dict)
            None   # No chatbot found
        ]

        result = step1_main(
            whatsapp_phone_id="999999999",  # Non-existent phone ID
            user_phone="16315551181",
            message_id="ABGGFlA5Fpa_NEW",
            user_name="Test User"
        )

        # Assertions
        assert result["proceed"] is False
        assert result["reason"] == "Chatbot not found"
        assert result["notify_admin"] is True

    @patch('psycopg2.connect')
    def test_successful_context_loading(self, mock_connect):
        """Test successful loading of all context data"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Setup responses
        mock_cursor.fetchone.side_effect = [
            None,  # No duplicate
            {"id": 1},  # Webhook event created
            {  # Chatbot + organization data
                "id": "chatbot-123",
                "organization_id": "org-456",
                "name": "Test Bot",
                "system_prompt": "You are helpful",
                "persona": "Friendly",
                "model_name": "gemini-pro",
                "temperature": 0.7,
                "rag_enabled": False,
                "whatsapp_access_token": "token_xyz",
                "is_active": True,
                "fallback_message_error": "Error occurred",
                "fallback_message_limit": "Limit reached",
                "org_name": "Test Org",
                "plan_tier": "pro",
                "org_is_active": True,
                "message_limit_monthly": 1000,
                "token_limit_monthly": 100000,
                "billing_period_start": "2025-01-01",
                "billing_period_end": "2025-02-01"
            },
            {  # Usage from get_current_usage()
                "messages_used": 10,
                "tokens_used": 5000
            },
            {  # Contact upserted
                "id": "contact-789",
                "conversation_mode": "auto",
                "variables": {},
                "tags": []
            }
        ]

        # Mock fetchall queries (tools, history)
        mock_cursor.fetchall.side_effect = [
            [],  # No tools/integrations
            []   # No chat history
        ]

        result = step1_main(
            whatsapp_phone_id="123456",
            user_phone="16315551234",
            message_id="msg-new-123",
            user_name="John Doe"
        )

        # Assertions
        assert result["proceed"] is True
        assert result["chatbot"]["id"] == "chatbot-123"
        assert result["chatbot"]["name"] == "Test Bot"
        assert result["user"]["id"] == "contact-789"
        assert result["user"]["name"] == "John Doe"
        assert "history" in result
        assert "tools" in result
        assert "usage_info" in result

    @patch('psycopg2.connect')
    def test_inactive_organization(self, mock_connect):
        """Test handling when organization is inactive"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        mock_cursor.fetchone.side_effect = [
            None,  # No duplicate
            {"id": 1},  # Webhook event
            {  # Chatbot with inactive org
                "id": "bot-123",
                "organization_id": "org-456",
                "is_active": True,
                "org_is_active": False,  # Org is inactive!
                "message_limit_monthly": 1000,
                "token_limit_monthly": 100000
            }
        ]

        result = step1_main(
            whatsapp_phone_id="123",
            user_phone="16315551234",
            message_id="msg-123",
            user_name="Test"
        )

        assert result["proceed"] is False
        assert result["reason"] == "Service Inactive"
        assert result["notify_admin"] is True

    @patch('psycopg2.connect')
    def test_usage_limit_exceeded(self, mock_connect):
        """Test handling when usage limits are exceeded"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        mock_cursor.fetchone.side_effect = [
            None,  # No duplicate
            {"id": 1},  # Webhook event
            {  # Chatbot data
                "id": "bot-123",
                "organization_id": "org-456",
                "is_active": True,
                "org_is_active": True,
                "message_limit_monthly": 100,  # Low limit
                "token_limit_monthly": 10000,
                "billing_period_start": "2025-01-01",
                "billing_period_end": "2025-02-01",
                "whatsapp_access_token": "token_xyz"
            },
            {  # Usage from get_current_usage() - EXCEEDED!
                "messages_used": 150,  # Over the limit of 100
                "tokens_used": 5000
            }
        ]

        result = step1_main(
            whatsapp_phone_id="123",
            user_phone="16315551234",
            message_id="msg-123",
            user_name="Test"
        )

        assert result["proceed"] is False
        assert result["reason"] == "Usage Limit Exceeded"
        assert result["notify_admin"] is True
        assert "usage_info" in result

    @patch('psycopg2.connect')
    def test_manual_mode_human_takeover(self, mock_connect):
        """Test handling when contact is in manual mode"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        mock_cursor.fetchone.side_effect = [
            None,  # No duplicate
            {"id": 1},  # Webhook event
            {  # Chatbot data
                "id": "bot-123",
                "organization_id": "org-456",
                "is_active": True,
                "org_is_active": True,
                "message_limit_monthly": 1000,
                "token_limit_monthly": 100000,
                "billing_period_start": "2025-01-01",
                "billing_period_end": "2025-02-01"
            },
            {  # Usage from get_current_usage()
                "messages_used": 10,
                "tokens_used": 5000
            },
            {  # Contact in manual mode
                "id": "contact-789",
                "conversation_mode": "manual",  # Human takeover!
                "variables": {},
                "tags": []
            }
        ]

        result = step1_main(
            whatsapp_phone_id="123",
            user_phone="16315551234",
            message_id="msg-123",
            user_name="Test"
        )

        assert result["proceed"] is False
        assert result["reason"] == "Manual Mode - Human Agent Required"

    @patch('psycopg2.connect')
    def test_chat_history_loading(self, mock_connect):
        """Test that chat history is loaded and reversed to chronological order"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Setup basic responses
        mock_cursor.fetchone.side_effect = [
            None,  # No duplicate
            {"id": 1},  # Webhook event
            {  # Chatbot
                "id": "bot-123",
                "organization_id": "org-456",
                "name": "Test Bot",
                "system_prompt": "Helpful",
                "persona": "Friendly",
                "model_name": "gemini-pro",
                "temperature": 0.7,
                "rag_enabled": False,
                "whatsapp_access_token": "token",
                "is_active": True,
                "org_is_active": True,
                "message_limit_monthly": 1000,
                "token_limit_monthly": 100000,
                "billing_period_start": "2025-01-01",
                "billing_period_end": "2025-02-01",
                "fallback_message_error": "Error",
                "fallback_message_limit": "Limit"
            },
            {  # Usage from get_current_usage()
                "messages_used": 10,
                "tokens_used": 5000
            },
            {  # Contact
                "id": "contact-789",
                "conversation_mode": "auto",
                "variables": {},
                "tags": []
            }
        ]

        # Mock history (DESC order from DB)
        history_rows = [
            {"role": "assistant", "content": "Response 2", "tool_calls": None, "tool_results": None, "created_at": "2025-01-15 10:02"},
            {"role": "user", "content": "Question 2", "tool_calls": None, "tool_results": None, "created_at": "2025-01-15 10:01"},
            {"role": "assistant", "content": "Response 1", "tool_calls": None, "tool_results": None, "created_at": "2025-01-15 10:00"}
        ]

        mock_cursor.fetchall.side_effect = [
            [],  # Tools
            history_rows  # History
        ]

        result = step1_main(
            whatsapp_phone_id="123",
            user_phone="16315551234",
            message_id="msg-123",
            user_name="Test"
        )

        assert result["proceed"] is True
        # History should be reversed to chronological (oldest first)
        assert len(result["history"]) == 3
        assert result["history"][0]["content"] == "Response 1"  # Oldest
        assert result["history"][2]["content"] == "Response 2"  # Newest

    @patch('psycopg2.connect')
    def test_db_connection_error(self, mock_connect):
        """Test handling of database connection failures"""
        # Simulate connection failure
        mock_connect.side_effect = Exception("Connection refused")

        result = step1_main(
            whatsapp_phone_id="123",
            user_phone="16315551234",
            message_id="msg-123",
            user_name="Test"
        )

        assert result["proceed"] is False
        assert "DB Connection Failed" in result["reason"]
        assert result["notify_admin"] is True

    @patch('psycopg2.connect')
    def test_retry_failed_message(self, mock_connect):
        """Test that failed messages can be retried"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Setup: message exists with 'failed' status
        mock_cursor.fetchone.side_effect = [
            {"id": 1, "status": "failed", "processed_at": "2025-01-15 10:00"},  # Failed message
            {  # Chatbot (after retry allowed)
                "id": "bot-123",
                "organization_id": "org-456",
                "name": "Test Bot",
                "system_prompt": "Helpful",
                "persona": "Friendly",
                "model_name": "gemini-pro",
                "temperature": 0.7,
                "rag_enabled": False,
                "whatsapp_access_token": "token",
                "is_active": True,
                "org_is_active": True,
                "message_limit_monthly": 1000,
                "token_limit_monthly": 100000,
                "billing_period_start": "2025-01-01",
                "billing_period_end": "2025-02-01",
                "fallback_message_error": "Error",
                "fallback_message_limit": "Limit"
            },
            {  # Usage from get_current_usage()
                "messages_used": 10,
                "tokens_used": 5000
            },
            {  # Contact
                "id": "contact-789",
                "conversation_mode": "auto",
                "variables": {},
                "tags": []
            }
        ]

        mock_cursor.fetchall.side_effect = [
            [],  # No tools
            []   # No history
        ]

        result = step1_main(
            whatsapp_phone_id="123",
            user_phone="16315551234",
            message_id="msg-retry",
            user_name="Test"
        )

        # Should proceed with retry
        assert result["proceed"] is True
        assert result["webhook_event_id"] == 1  # Reuses existing webhook event


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
File: db/seed.sql
================================================
/* 
====================================================================
  SEED DATA v2.0
  Run this AFTER create.sql to populate test data.
  
  Note: ${VARIABLES} are replaced by manage_db.sh from .env file
====================================================================
*/

-- 1. Create Organization with Usage Limits and Notification Settings
INSERT INTO organizations (
    id,
    name,
    slug,
    plan_tier,
    message_limit_monthly,
    token_limit_monthly,
    billing_period_start,
    billing_period_end,
    notification_method,
    slack_webhook_url,
    notification_email,
    max_knowledge_pdfs,
    max_knowledge_urls,
    max_knowledge_ingestions_per_day,
    max_knowledge_storage_mb,
    is_active
) VALUES (
    '11111111-1111-1111-1111-111111111111',
    'JD Labs Corporation',
    'jd-labs-corp',
    'pro', -- Pro plan
    1000, -- 1000 messages per month
    1000000, -- 1M tokens per month
    CURRENT_DATE,
    CURRENT_DATE + INTERVAL '1 month',
    'slack', -- Enable Slack notifications
    '${SLACK_WEBHOOK_URL}', -- Slack webhook URL from .env
    '${OWNER_EMAIL}', -- Fallback to owner email
    50, -- 50 PDFs for pro plan
    20, -- 20 URLs for pro plan
    100, -- 100 ingestions per day
    500, -- 500 MB storage limit
    TRUE
);

-- 2. Create Users
INSERT INTO users (id, organization_id, email, full_name, role) VALUES 
(
    '99999999-9999-9999-9999-999999999999',
    '11111111-1111-1111-1111-111111111111',
    '${OWNER_EMAIL}',
    'Admin User',
    'owner'
),
(
    '88888888-8888-8888-8888-888888888888',
    '11111111-1111-1111-1111-111111111111',
    'member@devcorp.com',
    'Team Member',
    'member'
);

-- 3. Create Integrations (MCP Tools)
INSERT INTO org_integrations (id, organization_id, provider, name, config, is_active) VALUES
-- Pricing Calculator Tool
(
    '33333333-3333-3333-3333-333333333333',
    '11111111-1111-1111-1111-111111111111',
    'mcp',
    'calculate_pricing',
    '{
        "type": "mcp_server",
        "server_url": "http://mcp_pricing_calculator:3001",
        "description": "Calcula precios del chatbot de WhatsApp según volumen de mensajes y tier",
        "llm_instructions": "Usa esta herramienta cuando el cliente pregunte sobre precios o costos. Necesitas el volumen de mensajes estimado. Si no lo sabes, pregúntale al cliente primero.",
        "parameters": {
            "type": "object",
            "properties": {
                "message_volume": {
                    "type": "number",
                    "description": "Número de mensajes al mes"
                },
                "tier": {
                    "type": "string",
                    "description": "Tier del plan: basic, professional, o enterprise",
                    "enum": ["basic", "professional", "enterprise"]
                }
            },
            "required": ["message_volume"]
        }
    }',
    TRUE
),
-- Lead Capture Tool
(
    '33333333-3333-3333-3333-333333333334',
    '11111111-1111-1111-1111-111111111111',
    'mcp',
    'capture_lead',
    '{
        "type": "mcp_server",
        "server_url": "http://mcp_lead_capture:3002",
        "description": "Guarda información de un cliente potencial interesado en el servicio",
        "llm_instructions": "Usa esta herramienta cuando el cliente muestre interés genuino en contratar el servicio. Asegúrate de tener al menos su nombre y teléfono. Pregunta por la información faltante antes de llamar esta herramienta.",
        "parameters": {
            "type": "object",
            "properties": {
                "name": {
                    "type": "string",
                    "description": "Nombre del cliente"
                },
                "phone": {
                    "type": "string",
                    "description": "Teléfono del cliente"
                },
                "email": {
                    "type": "string",
                    "description": "Email del cliente (opcional)"
                },
                "company": {
                    "type": "string",
                    "description": "Nombre de la empresa (opcional)"
                },
                "estimated_messages": {
                    "type": "number",
                    "description": "Volumen estimado de mensajes al mes (opcional)"
                }
            },
            "required": ["name", "phone"]
        }
    }',
    TRUE
),
-- Contact Owner Tool (Purpose-Agnostic Notifications)
(
    '33333333-3333-3333-3333-333333333336',
    '11111111-1111-1111-1111-111111111111',
    'mcp',
    'contact_owner',
    '{
        "type": "mcp_server",
        "server_url": "http://mcp_contact_owner:3003",
        "description": "Envía una notificación importante al dueño del chatbot con información relevante del contexto",
        "llm_instructions": "Usa esta herramienta cuando necesites notificar al dueño sobre algo importante: leads de alto valor, problemas urgentes, quejas críticas, oportunidades de negocio, o cualquier situación que requiera atención humana. Incluye un mensaje claro explicando la situación, y en contact_info añade CUALQUIER información relevante del cliente o situación (nombre, contacto, detalles específicos del caso, etc). Este campo es completamente flexible - incluye lo que sea importante para el contexto.",
        "parameters": {
            "type": "object",
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Mensaje claro para el dueño explicando la situación y por qué requiere atención"
                },
                "contact_info": {
                    "type": "object",
                    "description": "Objeto genérico con CUALQUIER información relevante del cliente/situación. Puede incluir: nombre, teléfono, email, empresa, detalles del problema, presupuesto, timeline, o cualquier otro dato importante para el contexto. Completamente flexible.",
                    "additionalProperties": true
                },
                "urgency": {
                    "type": "string",
                    "description": "Nivel de urgencia: low (informativo), medium (importante), high (urgente/crítico)",
                    "enum": ["low", "medium", "high"]
                }
            },
            "required": ["message"]
        }
    }',
    TRUE
),
-- Custom Business Tool
(
    '33333333-3333-3333-3333-333333333335',
    '11111111-1111-1111-1111-111111111111',
    'custom',
    'Order Lookup',
    '{
        "type": "custom_api",
        "base_url": "https://api.devcorp.com/orders",
        "auth_type": "bearer",
        "tools": ["get_order", "track_shipment"]
    }',
    FALSE -- Disabled by default
);

-- 4. Create Chatbot
INSERT INTO chatbots (
    id, 
    organization_id, 
    name, 
    whatsapp_phone_number_id, 
    whatsapp_business_account_id,
    whatsapp_access_token, 
    model_name,
    system_prompt,
    persona,
    temperature,
    rag_enabled,
    is_active
) VALUES (
    '22222222-2222-2222-2222-222222222222',
    '11111111-1111-1111-1111-111111111111',
    'MVP Test Bot',
    '${WHATSAPP_PHONE_NUMBER_ID}',
    'JD-labs-WABA-ID',
    '${WHATSAPP_ACCESS_TOKEN}',
    'gemini-3-flash-preview',
    'Eres un representante de ventas y servicio al cliente para JD Labs, empresa en Guadalajara, México. Vendes "Chatbot de WhatsApp" - una solución SaaS para automatizar conversaciones por WhatsApp.

PLANES DISPONIBLES:
- Gratis: 100 mensajes/mes
- Básico: $499 MXN/mes (1,000 mensajes)
- Profesional: $999 MXN/mes (3,000 mensajes, acceso a bases de datos (PDFs, URLs))
- Empresarial: $2,999 MXN/mes (15,000 mensajes, API personalizada)
- Contáctanos para planes a medida y volúmenes mayores.

IMPORTANTE:
- Responde en español, sé breve y directo
- Mantén tono profesional pero amigable',
    'Hablas con tono cálido y profesional. Usas emojis ocasionalmente. Eres conciso - máximo 2-3 frases por respuesta a menos que lo amerite.',
    0.7,
    TRUE, -- RAG disabled for now
    TRUE
);

-- 5. Enable Integrations for this Bot
INSERT INTO chatbot_integrations (chatbot_id, integration_id, is_enabled, settings_override) VALUES
-- Enable Pricing Calculator
(
    '22222222-2222-2222-2222-222222222222',
    '33333333-3333-3333-3333-333333333333',
    TRUE,
    '{}'
),
-- Enable Lead Capture
(
    '22222222-2222-2222-2222-222222222222',
    '33333333-3333-3333-3333-333333333334',
    TRUE,
    '{}'
),
-- Enable Contact Owner
(
    '22222222-2222-2222-2222-222222222222',
    '33333333-3333-3333-3333-333333333336',
    TRUE,
    '{}'
);

-- 6. Create Sample Contacts
INSERT INTO contacts (id, chatbot_id, phone_number, name, conversation_mode, variables, tags) VALUES 
(
    '44444444-4444-4444-4444-444444444444',
    '22222222-2222-2222-2222-222222222222',
    '15550001234',
    'Alice Test',
    'auto',
    '{"email": "alice@example.com", "preferred_language": "en", "timezone": "America/New_York"}',
    ARRAY['vip', 'test_user']
),
(
    '44444444-4444-4444-4444-444444444445',
    '22222222-2222-2222-2222-222222222222',
    '15550005678',
    'Bob Demo',
    'auto',
    '{"company": "Demo Inc", "role": "CTO"}',
    ARRAY['demo']
);

-- 7. Create Sample Message History
INSERT INTO messages (contact_id, role, content, whatsapp_message_id, created_at) VALUES 
-- Alice's conversation
(
    '44444444-4444-4444-4444-444444444444',
    'user',
    'Hello! Can you help me with something?',
    'wamid.test.alice.001',
    NOW() - INTERVAL '2 days'
),
(
    '44444444-4444-4444-4444-444444444444',
    'assistant',
    'Hello Alice! Of course, I''d be happy to help you. What can I assist you with today? 😊',
    NULL,
    NOW() - INTERVAL '2 days' + INTERVAL '2 seconds'
),
(
    '44444444-4444-4444-4444-444444444444',
    'user',
    'What is 15% of 250?',
    'wamid.test.alice.002',
    NOW() - INTERVAL '1 day'
),
(
    '44444444-4444-4444-4444-444444444444',
    'assistant',
    '15% of 250 is 37.5. Is there anything else you''d like to calculate?',
    NULL,
    NOW() - INTERVAL '1 day' + INTERVAL '3 seconds'
),

-- Bob's conversation
(
    '44444444-4444-4444-4444-444444444445',
    'user',
    'Hey, what''s the weather like?',
    'wamid.test.bob.001',
    NOW() - INTERVAL '6 hours'
),
(
    '44444444-4444-4444-4444-444444444445',
    'assistant',
    'Let me check the weather for you! ⛅ In San Francisco, it''s currently 68°F and partly cloudy. Perfect weather for a walk!',
    NULL,
    NOW() - INTERVAL '6 hours' + INTERVAL '4 seconds'
);

-- 8. Create Webhook Events (for testing idempotency)
INSERT INTO webhook_events (
    whatsapp_message_id,
    phone_number_id,
    chatbot_id,
    status,
    raw_payload,
    received_at,
    processed_at,
    processing_time_ms
) VALUES 
(
    'wamid.test.alice.001',
    '${WHATSAPP_PHONE_NUMBER_ID}',
    '22222222-2222-2222-2222-222222222222',
    'completed',
    '{"object": "whatsapp_business_account", "entry": []}',
    NOW() - INTERVAL '2 days',
    NOW() - INTERVAL '2 days' + INTERVAL '1.5 seconds',
    1500
),
(
    'wamid.test.alice.002',
    '${WHATSAPP_PHONE_NUMBER_ID}',
    '22222222-2222-2222-2222-222222222222',
    'completed',
    '{"object": "whatsapp_business_account", "entry": []}',
    NOW() - INTERVAL '1 day',
    NOW() - INTERVAL '1 day' + INTERVAL '2 seconds',
    2000
),
(
    'wamid.test.bob.001',
    '${WHATSAPP_PHONE_NUMBER_ID}',
    '22222222-2222-2222-2222-222222222222',
    'completed',
    '{"object": "whatsapp_business_account", "entry": []}',
    NOW() - INTERVAL '6 hours',
    NOW() - INTERVAL '6 hours' + INTERVAL '2.5 seconds',
    2500
);

-- 9. Create Usage Logs
INSERT INTO usage_logs (
    organization_id,
    chatbot_id,
    contact_id,
    message_count,
    tokens_input,
    tokens_output,
    tokens_total,
    model_name,
    provider,
    estimated_cost_usd,
    created_at,
    date_bucket
) VALUES 
-- Alice's first conversation
(
    '11111111-1111-1111-1111-111111111111',
    '22222222-2222-2222-2222-222222222222',
    '44444444-4444-4444-4444-444444444444',
    1,
    150,
    80,
    230,
    'gemini-3-flash-preview',
    'google',
    0.000230,
    NOW() - INTERVAL '2 days',
    (NOW() - INTERVAL '2 days')::DATE
),
-- Alice's second conversation (with tool use)
(
    '11111111-1111-1111-1111-111111111111',
    '22222222-2222-2222-2222-222222222222',
    '44444444-4444-4444-4444-444444444444',
    1,
    200,
    120,
    320,
    'gemini-3-flash-preview',
    'google',
    0.000320,
    NOW() - INTERVAL '1 day',
    (NOW() - INTERVAL '1 day')::DATE
),
-- Bob's conversation
(
    '11111111-1111-1111-1111-111111111111',
    '22222222-2222-2222-2222-222222222222',
    '44444444-4444-4444-4444-444444444445',
    1,
    180,
    150,
    330,
    'gemini-3-flash-preview',
    'google',
    0.000330,
    NOW() - INTERVAL '6 hours',
    (NOW() - INTERVAL '6 hours')::DATE
);

-- 10. Initialize Usage Summary
INSERT INTO usage_summary (
    organization_id,
    current_period_messages,
    current_period_tokens,
    period_start,
    period_end,
    last_updated_at
) VALUES (
    '11111111-1111-1111-1111-111111111111',
    3, -- Total messages so far
    880, -- Total tokens so far (230 + 320 + 330)
    CURRENT_DATE,
    CURRENT_DATE + INTERVAL '1 month',
    NOW()
);

-- 11. Create a second org for testing multi-tenancy
INSERT INTO organizations (
    id, 
    name, 
    slug, 
    plan_tier,
    message_limit_monthly,
    token_limit_monthly,
    is_active
) VALUES (
    '11111111-1111-1111-1111-111111111112',
    'Test Corp (Free Plan)',
    'test-corp-free',
    'free',
    100, -- Limited messages
    50000, -- Limited tokens
    TRUE
);

-- Add a chatbot for the second org (without real WhatsApp credentials)
INSERT INTO chatbots (
    id, 
    organization_id, 
    name, 
    whatsapp_phone_number_id, 
    whatsapp_access_token, 
    model_name,
    system_prompt,
    is_active
) VALUES (
    '22222222-2222-2222-2222-222222222223',
    '11111111-1111-1111-1111-111111111112',
    'Free Plan Test Bot',
    'test_phone_id_free_plan',
    'test_token_free_plan',
    'gemini-3-flash-preview',
    'You are a basic assistant for Test Corp.',
    FALSE -- Inactive until configured
);

-- Verify seed data
DO $$
DECLARE
    org_count INT;
    chatbot_count INT;
    message_count INT;
BEGIN
    SELECT COUNT(*) INTO org_count FROM organizations;
    SELECT COUNT(*) INTO chatbot_count FROM chatbots;
    SELECT COUNT(*) INTO message_count FROM messages;
    
    RAISE NOTICE '=== SEED DATA SUMMARY ===';
    RAISE NOTICE 'Organizations: %', org_count;
    RAISE NOTICE 'Chatbots: %', chatbot_count;
    RAISE NOTICE 'Messages: %', message_count;
    RAISE NOTICE '========================';
END $$;

================================================
File: f/development/ingest_multiple_urls.script.yaml
================================================
summary: ''
description: ''
lock: '!inline f/development/ingest_multiple_urls.script.lock'
kind: script
schema:
  $schema: 'https://json-schema.org/draft/2020-12/schema'
  type: object
  properties:
    chatbot_id:
      type: string
      description: ''
      default: null
      originalType: string
    db_resource:
      type: string
      description: ''
      default: f/development/business_layer_db_postgreSQL
      originalType: string
    urls:
      type: array
      description: ''
      default: null
      items:
        type: string
      originalType: 'string[]'
  required:
    - chatbot_id
    - urls


================================================
File: output.txt
================================================


================================================
File: tests/live/test_live_llm.py
================================================
"""
Live LLM Integration Tests

These tests make REAL API calls to OpenAI and Gemini.
They verify that:
1. Response parsing works with actual API responses
2. Token counting is accurate
3. Error handling works for real error scenarios

Run with: pytest tests/live/ -m live_llm
Cost: ~$0.01-0.05 per full run

IMPORTANT: Set OPENAI_API_KEY and/or GOOGLE_API_KEY environment variables.
"""

import pytest
import os
from pathlib import Path
import sys

# Add project root
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


@pytest.mark.live
@pytest.mark.live_llm
class TestLiveOpenAI:
    """Live tests for OpenAI API integration."""

    def test_openai_simple_completion(self, openai_api_key, live_test_warning):
        """
        GOAL: Verify OpenAI API response parsing works correctly

        GIVEN: Valid OpenAI API key
        WHEN: Sending a simple completion request
        THEN:
        - Response contains expected fields
        - Token counts are non-zero
        - Content is parseable
        """
        from openai import OpenAI

        client = OpenAI(api_key=openai_api_key)

        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Cheapest model
            messages=[
                {"role": "user", "content": "Say 'test' and nothing else."}
            ],
            max_tokens=10
        )

        # Verify response structure
        assert response.choices is not None
        assert len(response.choices) > 0
        assert response.choices[0].message.content is not None

        # Verify token counts
        assert response.usage.prompt_tokens > 0
        assert response.usage.completion_tokens > 0
        assert response.usage.total_tokens == (
            response.usage.prompt_tokens + response.usage.completion_tokens
        )

        print(f"\nOpenAI Response: {response.choices[0].message.content}")
        print(f"Tokens: prompt={response.usage.prompt_tokens}, "
              f"completion={response.usage.completion_tokens}")

    def test_openai_tool_calling(self, openai_api_key, live_test_warning):
        """
        GOAL: Verify OpenAI tool calling works correctly

        GIVEN: Valid OpenAI API key and tool definition
        WHEN: Sending a request that should trigger tool use
        THEN:
        - Response contains tool call
        - Tool call has correct structure
        - Arguments are parseable JSON
        """
        from openai import OpenAI
        import json

        client = OpenAI(api_key=openai_api_key)

        tools = [{
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get the weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {"type": "string"}
                    },
                    "required": ["location"]
                }
            }
        }]

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": "What's the weather in Tokyo?"}
            ],
            tools=tools,
            tool_choice="auto",
            max_tokens=100
        )

        # Verify tool call
        message = response.choices[0].message
        assert message.tool_calls is not None, "Expected tool call but got none"
        assert len(message.tool_calls) > 0

        tool_call = message.tool_calls[0]
        assert tool_call.function.name == "get_weather"

        # Verify arguments are valid JSON
        args = json.loads(tool_call.function.arguments)
        assert "location" in args

        print(f"\nTool call: {tool_call.function.name}")
        print(f"Arguments: {args}")

    def test_openai_error_handling_invalid_key(self, live_test_warning):
        """
        GOAL: Verify error handling for invalid API key

        GIVEN: Invalid API key
        WHEN: Making an API request
        THEN: AuthenticationError is raised with proper message
        """
        from openai import OpenAI, AuthenticationError

        client = OpenAI(api_key="invalid-key-12345")

        with pytest.raises(AuthenticationError) as exc_info:
            client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )

        assert "Incorrect API key" in str(exc_info.value) or "invalid" in str(exc_info.value).lower()
        print(f"\nExpected error received: {exc_info.value}")


@pytest.mark.live
@pytest.mark.live_llm
class TestLiveGemini:
    """Live tests for Google Gemini API integration."""

    def test_gemini_simple_completion(self, google_api_key, live_test_warning):
        """
        GOAL: Verify Gemini API response parsing works correctly

        GIVEN: Valid Google API key
        WHEN: Sending a simple completion request
        THEN:
        - Response contains expected fields
        - Token counts are accessible
        - Content is parseable
        """
        from google import genai

        client = genai.Client(api_key=google_api_key)

        response = client.models.generate_content(
            model="gemini-2.0-flash-exp",
            contents="Say 'test' and nothing else."
        )

        # Verify response structure
        assert response.text is not None
        assert len(response.text) > 0

        # Verify token counts
        assert response.usage_metadata is not None
        assert response.usage_metadata.prompt_token_count > 0
        assert response.usage_metadata.candidates_token_count > 0

        print(f"\nGemini Response: {response.text}")
        print(f"Tokens: prompt={response.usage_metadata.prompt_token_count}, "
              f"completion={response.usage_metadata.candidates_token_count}")


@pytest.mark.live
@pytest.mark.live_embeddings
class TestLiveEmbeddings:
    """Live tests for OpenAI embeddings API."""

    def test_openai_embedding_generation(self, openai_api_key, live_test_warning):
        """
        GOAL: Verify embedding generation works correctly

        GIVEN: Valid OpenAI API key and sample text
        WHEN: Generating embeddings
        THEN:
        - Embedding is returned
        - Correct dimension (1536 for ada-002, 3072 for text-embedding-3-large)
        - Values are floats in expected range
        """
        from openai import OpenAI

        client = OpenAI(api_key=openai_api_key)

        response = client.embeddings.create(
            model="text-embedding-ada-002",
            input="This is a test document for embedding generation."
        )

        # Verify response structure
        assert response.data is not None
        assert len(response.data) > 0

        embedding = response.data[0].embedding

        # Verify dimensions
        assert len(embedding) == 1536, f"Expected 1536 dimensions, got {len(embedding)}"

        # Verify values are floats
        assert all(isinstance(v, float) for v in embedding)

        # Verify values are in reasonable range
        assert all(-1 <= v <= 1 for v in embedding), "Embedding values out of expected range"

        print(f"\nEmbedding generated: {len(embedding)} dimensions")
        print(f"Sample values: {embedding[:5]}")

        # Return embedding for fixture generation
        return embedding

    def test_openai_batch_embedding(self, openai_api_key, live_test_warning):
        """
        GOAL: Verify batch embedding generation works correctly

        GIVEN: Valid OpenAI API key and multiple texts
        WHEN: Generating embeddings in batch
        THEN:
        - All embeddings are returned
        - Each has correct dimensions
        - Order is preserved
        """
        from openai import OpenAI

        client = OpenAI(api_key=openai_api_key)

        texts = [
            "First test document.",
            "Second test document.",
            "Third test document."
        ]

        response = client.embeddings.create(
            model="text-embedding-ada-002",
            input=texts
        )

        # Verify correct number of embeddings
        assert len(response.data) == len(texts)

        # Verify each embedding
        for i, data in enumerate(response.data):
            assert data.index == i, "Embeddings returned out of order"
            assert len(data.embedding) == 1536

        print(f"\nBatch embeddings generated: {len(response.data)} embeddings")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-m", "live"])


================================================
File: .gitignore
================================================
.env
*.pyc

================================================
File: db/README.md
================================================
# Database Setup Guide

This directory contains the database schema and seed data for the Whatsapp Chatbot MVP.

## Prerequisites

1. Ensure Docker Compose is running and the PostgreSQL database container is up:
   ```bash
   docker-compose ps
   ```

2. Verify the database service is healthy:
   ```bash
   docker-compose logs db
   ```

## Database Connection Details

The PostgreSQL database is configured in `docker-compose.yml` with the following settings:
- **Database Name:** `windmill`
- **Username:** `postgres`
- **Password:** `changeme`
- **Host:** `db` (when connecting from within Docker network)
- **Port:** `5432`

## Running the Scripts

### Option 1: Using manage_db.sh (Recommended for business_logic_db)

If you're using the `business_logic_db` container, you can use the `manage_db.sh` script which automatically loads environment variables from your `.env` file.

#### Prerequisites

1. Ensure your `.env` file contains the required WhatsApp credentials:
   ```bash
   WHATSAPP_PHONE_NUMBER_ID=your_phone_number_id
   WHATSAPP_ACCESS_TOKEN=your_meta_access_token
   OWNER_EMAIL=your_email
   ```

2. Make sure `manage_db.sh` is executable:
   ```bash
   chmod +x db/manage_db.sh
   ```

3. Make sure your current working directory is set to /src/db/ prior to script execution, or it may fail

#### Usage

From the `db/` directory:

```bash
# Create schema
./manage_db.sh create

# Seed data (requires WHATSAPP_PHONE_NUMBER_ID and WHATSAPP_ACCESS_TOKEN in .env)
./manage_db.sh seed

# Reset everything (drop, create, seed)
./manage_db.sh reset

# Drop all tables (with confirmation)
./manage_db.sh drop
```

**Note:** The `seed` and `reset` commands will automatically substitute `${WHATSAPP_PHONE_NUMBER_ID}` and `${WHATSAPP_ACCESS_TOKEN}` placeholders in `seed.sql` with values from your `.env` file.

## Verifying the Setup

After running both scripts, verify the tables were created:

```bash
docker-compose exec db psql -U postgres -d windmill -c "\dt"
```

You should see tables like:
- `organizations`
- `users`
- `org_integrations`
- `chatbots`
- `chatbot_integrations`
- `knowledge_sources`
- `contacts`
- `messages`

To verify seed data was inserted:

```bash
docker-compose exec db psql -U postgres -d windmill -c "SELECT * FROM organizations;"
docker-compose exec db psql -U postgres -d windmill -c "SELECT * FROM chatbots;"
```

## Troubleshooting

### Database Container Not Running

If the database container is not running:

```bash
docker-compose up -d db
```

Wait for the health check to pass (usually 10-30 seconds).

### Permission Errors

If you encounter permission errors, ensure you're using the correct username (`postgres`) and that the database container has the proper volumes mounted.

### Connection Refused

If you get connection refused errors:
1. Check that the database container is healthy: `docker-compose ps`
2. Check database logs: `docker-compose logs db`
3. Ensure the database has finished initializing (wait for health check)

### Script Errors

If you encounter errors when running the scripts:
- Ensure `schema.sql` ran successfully before running `seed.sql`
- Check that the `pgcrypto` extension is available (it should be in PostgreSQL 16)
- Verify the SQL syntax is correct for your PostgreSQL version

## Next Steps

After setting up the database:

1. **If using `manage_db.sh`:** Ensure your `.env` file contains `WHATSAPP_PHONE_NUMBER_ID` and `WHATSAPP_ACCESS_TOKEN` before running the seed command.

2. **If using Docker exec directly:** You'll need to manually edit `seed.sql` to replace the `${WHATSAPP_PHONE_NUMBER_ID}` and `${WHATSAPP_ACCESS_TOKEN}` placeholders, or use `envsubst`:
   ```bash
   envsubst < db/seed.sql | docker-compose exec -T db psql -U postgres -d windmill
   ```

3. Configure your Windmill flows to use the database connection.

4. Test the webhook integration to ensure data flows correctly.



================================================
File: f/development/utils/check_knowledge_quota.script.lock
================================================
# py: 3.12
anyio==4.12.0
certifi==2025.11.12
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.11
psycopg2-binary==2.9.11
typing-extensions==4.15.0
wmill==1.601.1

================================================
File: tests/unit/test_step3_3_usage_logging.py
================================================
"""
Unit tests for Step 3.3: Usage Logging

Tests Step 3.3's ability to:
- Log token usage and message counts
- Calculate costs accurately
- Update usage_summary for quick quota checks
- Skip logging when previous steps failed
- Handle token estimation fallbacks
- Test different provider pricing
"""

import pytest
import sys
import os
from unittest.mock import Mock, patch, MagicMock

# Add parent directories to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../f/development'))

# Mock wmill module before importing step3_3
mock_wmill = Mock()
mock_wmill.get_resource.return_value = {
    "host": "localhost",
    "port": 5432,
    "user": "test_user",
    "password": "test_password",
    "dbname": "test_db"
}
sys.modules['wmill'] = mock_wmill

# Import the module under test
import importlib.util
spec = importlib.util.spec_from_file_location(
    "step3_3",
    os.path.join(os.path.dirname(__file__), '../../f/development/3_3_log_usage.py')
)
step3_3_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(step3_3_module)
step3_3_main = step3_3_module.main
_get_cost_per_1k_tokens = step3_3_module._get_cost_per_1k_tokens


class TestStep3_3UsageLogging:
    """Test Step 3.3's usage logging functionality"""

    @patch('psycopg2.connect')
    def test_successful_usage_logging(self, mock_connect):
        """Test successful logging of usage data"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Mock the RETURNING id from INSERT
        mock_cursor.fetchone.return_value = {"id": 12345}

        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {
                    "id": "chatbot-123",
                    "organization_id": "org-456",
                    "model_name": "gemini-3-flash-preview"
                },
                "user": {"id": "contact-789"}
            },
            llm_result={
                "reply_text": "Hello! How can I help you today?",
                "usage_info": {
                    "provider": "google",
                    "model": "gemini-3-flash-preview",
                    "tokens_input": 50,
                    "tokens_output": 30
                }
            },
            send_result={"success": True},
            webhook_event_id=1
        )

        # Assertions
        assert result["success"] is True
        assert result["usage_log_id"] == 12345
        assert result["tokens_used"] == 80  # 50 + 30
        assert result["message_count"] == 1
        assert result["estimated_cost"] > 0

        # Verify two SQL executes (INSERT usage_logs, UPDATE usage_summary)
        assert mock_cursor.execute.call_count == 2

        # Verify INSERT usage_logs call
        insert_call = mock_cursor.execute.call_args_list[0]
        assert "INSERT INTO usage_logs" in insert_call[0][0]
        assert insert_call[0][1] == (
            "org-456",
            "chatbot-123",
            "contact-789",
            1,  # webhook_event_id
            1,  # message_count
            50,  # tokens_input
            30,  # tokens_output
            80,  # tokens_total
            "gemini-3-flash-preview",
            "google",
            pytest.approx(0.00002, rel=1e-6),  # estimated_cost (80 tokens * 0.00025 / 1000 = 0.00002)
        )

        # Verify UPDATE usage_summary call
        update_call = mock_cursor.execute.call_args_list[1]
        assert "INSERT INTO usage_summary" in update_call[0][0]
        assert "ON CONFLICT (organization_id)" in update_call[0][0]
        assert "DO UPDATE SET" in update_call[0][0]

        # Verify commit
        assert mock_conn.commit.called

    @patch('psycopg2.connect')
    def test_token_estimation_fallback(self, mock_connect):
        """Test token estimation when LLM doesn't provide usage info"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor
        mock_cursor.fetchone.return_value = {"id": 100}

        # LLM result without usage_info
        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {
                    "id": "chatbot-123",
                    "organization_id": "org-456",
                    "model_name": "gemini-pro"
                },
                "user": {"id": "contact-789"}
            },
            llm_result={
                "reply_text": "This is a test response with some text.",  # ~40 chars = ~10 tokens
                "usage_info": {}  # No token counts
            },
            send_result={"success": True}
        )

        assert result["success"] is True
        assert result["tokens_used"] > 0  # Should have estimated tokens

    @patch('psycopg2.connect')
    def test_cost_calculation_openai(self, mock_connect):
        """Test cost calculation for OpenAI models"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor
        mock_cursor.fetchone.return_value = {"id": 100}

        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {
                    "id": "chatbot-123",
                    "organization_id": "org-456",
                    "model_name": "gpt-4o"
                },
                "user": {"id": "contact-789"}
            },
            llm_result={
                "usage_info": {
                    "provider": "openai",
                    "model": "gpt-4o",
                    "tokens_input": 1000,
                    "tokens_output": 500
                }
            },
            send_result={"success": True}
        )

        assert result["success"] is True
        # gpt-4o costs $0.005 per 1k tokens
        # 1500 tokens * 0.005 / 1000 = 0.0075
        assert result["estimated_cost"] == pytest.approx(0.0075, rel=1e-6)

    @patch('psycopg2.connect')
    def test_skip_when_step1_failed(self, mock_connect):
        """Test that usage is not logged when Step 1 failed"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_3_main(
            context_payload={
                "proceed": False,  # Step 1 failed
                "reason": "Chatbot not found"
            },
            llm_result={"usage_info": {}},
            send_result={"success": True}
        )

        assert result["success"] is False
        assert "Step 1 failed" in result["error"]
        assert not mock_cursor.execute.called

    @patch('psycopg2.connect')
    def test_skip_when_step2_failed(self, mock_connect):
        """Test that usage is not logged when Step 2 (LLM) failed"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {"id": "chatbot-123", "organization_id": "org-456"},
                "user": {"id": "contact-789"}
            },
            llm_result={
                "error": "LLM quota exceeded"  # Step 2 failed
            },
            send_result={"success": True}
        )

        assert result["success"] is False
        assert "Step 2 failed" in result["error"]
        assert not mock_cursor.execute.called

    @patch('psycopg2.connect')
    def test_skip_when_step3_failed(self, mock_connect):
        """Test that usage is not logged when Step 3 (send to WhatsApp) failed"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {"id": "chatbot-123", "organization_id": "org-456"},
                "user": {"id": "contact-789"}
            },
            llm_result={"usage_info": {}},
            send_result={
                "success": False,  # Message not delivered!
                "error": "WhatsApp API error"
            }
        )

        assert result["success"] is False
        assert "Step 3 failed" in result["error"]
        assert "message not delivered" in result["error"]
        assert not mock_cursor.execute.called

    @patch('psycopg2.connect')
    def test_database_error_rollback(self, mock_connect):
        """Test that database errors trigger rollback"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Simulate database error
        mock_cursor.execute.side_effect = Exception("Database constraint violation")

        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {"id": "chatbot-123", "organization_id": "org-456"},
                "user": {"id": "contact-789"}
            },
            llm_result={"usage_info": {}},
            send_result={"success": True}
        )

        assert result["success"] is False
        assert "Database constraint violation" in result["error"]
        
        # Verify rollback was called
        assert mock_conn.rollback.called

    @patch('psycopg2.connect')
    def test_usage_summary_upsert(self, mock_connect):
        """Test that usage_summary is updated via UPSERT pattern"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor
        mock_cursor.fetchone.return_value = {"id": 100}

        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {
                    "id": "chatbot-123",
                    "organization_id": "org-456",
                    "model_name": "gemini-pro"
                },
                "user": {"id": "contact-789"}
            },
            llm_result={
                "usage_info": {
                    "tokens_input": 100,
                    "tokens_output": 50
                }
            },
            send_result={"success": True}
        )

        assert result["success"] is True

        # Verify second SQL call updates usage_summary
        update_call = mock_cursor.execute.call_args_list[1]
        assert "ON CONFLICT (organization_id)" in update_call[0][0]
        assert "DO UPDATE SET" in update_call[0][0]
        assert "current_period_messages =" in update_call[0][0]
        assert "current_period_tokens =" in update_call[0][0]

    def test_cost_calculation_for_different_providers(self):
        """Test cost calculation helper for various providers/models"""
        # OpenAI GPT-4o
        cost = _get_cost_per_1k_tokens("openai", "gpt-4o")
        assert cost == 0.005

        # OpenAI GPT-4o-mini - Now correctly matches specific model first
        # (pricing dict is ordered from most specific to least specific)
        cost = _get_cost_per_1k_tokens("openai", "gpt-4o-mini")
        assert cost == 0.0002  # Correctly matches "gpt-4o-mini" first

        # Google Gemini Flash
        cost = _get_cost_per_1k_tokens("google", "gemini-3-flash-preview")
        assert cost == 0.00025

        # Anthropic Claude Sonnet
        cost = _get_cost_per_1k_tokens("anthropic", "claude-3-sonnet")
        assert cost == 0.003

        # Unknown provider - should return fallback
        cost = _get_cost_per_1k_tokens("unknown", "unknown-model")
        assert cost == 0.001  # Fallback rate

    @patch('psycopg2.connect')
    def test_webhook_event_id_tracking(self, mock_connect):
        """Test that webhook_event_id is properly tracked"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor
        mock_cursor.fetchone.return_value = {"id": 100}

        webhook_id = 99999

        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {"id": "chatbot-123", "organization_id": "org-456"},
                "user": {"id": "contact-789"}
            },
            llm_result={"usage_info": {"tokens_input": 10, "tokens_output": 10}},
            send_result={"success": True},
            webhook_event_id=webhook_id
        )

        assert result["success"] is True

        # Verify webhook_event_id was included in INSERT
        insert_call = mock_cursor.execute.call_args_list[0]
        assert webhook_id in insert_call[0][1]

    @patch('psycopg2.connect')
    def test_cleanup_on_error(self, mock_connect):
        """Test that database connections are properly closed on error"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Simulate error
        mock_cursor.execute.side_effect = Exception("Test error")

        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {"id": "chatbot-123", "organization_id": "org-456"},
                "user": {"id": "contact-789"}
            },
            llm_result={"usage_info": {}},
            send_result={"success": True}
        )

        assert result["success"] is False

        # Verify cleanup
        assert mock_cursor.close.called
        assert mock_conn.close.called

    @patch('psycopg2.connect')
    def test_returns_tokens_even_on_logging_failure(self, mock_connect):
        """Test that token count is still returned even if logging fails"""
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cursor

        # Simulate logging failure
        mock_cursor.execute.side_effect = Exception("Logging failed")

        result = step3_3_main(
            context_payload={
                "proceed": True,
                "chatbot": {"id": "chatbot-123", "organization_id": "org-456"},
                "user": {"id": "contact-789"}
            },
            llm_result={
                "usage_info": {
                    "tokens_input": 100,
                    "tokens_output": 50
                }
            },
            send_result={"success": True}
        )

        assert result["success"] is False
        # Tokens still reported despite logging failure
        assert result["tokens_used"] == 150


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


